---
title: Skagit Steelhead Fishery Management 2021
output:
  html_document:
    fig_caption: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r set_options, echo = FALSE, message = FALSE}
options(width = 100)
knitr::opts_chunk$set(message = FALSE)
set.seed(123)
```
![](https://user-images.githubusercontent.com/40445951/106297554-afe14000-6207-11eb-8b29-217650aed335.JPG)

## Overview
WDFW re-opened a popular sport fishery for winter steelhead on the Skagit River after a decade-long closure following the Endangered Species Act (ESA) listing of Puget Sound steelhead as Threatened in 2007. A [**Resource Management Plan**](https://github.com/tbuehrens/CreelAnalysis/blob/main/background/final_skagit_rmp_erd_04-10-2018.pdf) (RMP) that was jointly developed by WDFW and tribal co-managers, and adopted by NOAA Fisheries, provides the legal basis and rule set governing this fishery under the ESA. The plan ensures that impacts to wild steelhead populations from sport and tribal fisheries will not prevent the population from achieving conservation and recovery goals. The plan calls for careful monitoring of wild steelhead mortality resulting from fisheries in order to ensure that total mortality rate limits specified in the plan are not exceeded. Although current sport fishery regulations require release of wild steelhead, mortality does occur as a result of fish being caught and released, and these impacts must be quantified.

This page provides documentation of WDFW's efforts to plan and administer a fishery for Skagit steelhead that is consistent with the RMP, including:

  1) a comparison of the pre-season planned Skagit steelhead sport fishery-induced wild steelhead mortality with allowable exploitation rates designated in the Skagit steelhead RMP.
  
  2) in-season tracking of Skagit steelhead sport fishery fishery-induced wild steelhead mortality relative to allowable exploitation rates designated in the Skagit steelhead RMP.
  
Want to see the data and analysis for yourself? If you are interested in running the analysis yourself, jump to the ["Reproducing the Analysis"](#reproducing-the-analysis) section below, which overviews the GitHub code repository that contains the code necessary to reproduce the Skagit steelhead fishery creel analysis and fishery impact monitoring for 2021, and to reproduce this page.
```{r load_funcs, results = "hide",echo = FALSE, message = FALSE, warning = FALSE}
### Functions
#We also need a couple of helper functions which we will load from the functions folder, which we will load using the chunk of code with this heading in the RMD file.
wd_functions<-"functions"
sapply(FUN = source, paste(wd_functions, list.files(wd_functions), sep="/"))
```

```{r load_pkgs, message = FALSE, warning = FALSE, echo = FALSE,results = "hide"}
### Packages
#In addition to purrr, we also need a few packages that are not included with the base installation of R, so we begin by installing them (if necessary) and then loading them using using the chunk of code with this heading in the RMD file.
#===============================================
# Load packages, install and load if not already
#===============================================
packages_list<-c("timeDate",
      "plyr",
      "tidyverse",
      "rstan",
      "RColorBrewer",
      "readxl",
      "readr",
      "ggplot2",
      "tinytex",
      "here", 
      "lubridate", 
      "devtools",
      "xlsx",
      "cowplot",
      "ggpubr", 
      "chron",
      "suncalc", 
      "shinystan",
      "loo", 
      "data.table",
      "RColorBrewer",
      "reshape2",
      "MASS", 
      "kableExtra"
      )
install_or_load_pack(pack = packages_list)
```

```{r user_inputs,message=FALSE, warning=FALSE, echo = FALSE}
### User inputs
#Here we will specify user inputs like the file names of the data, the species, and stream name we would like to analyze data for, etc. using the chunk of code with this heading in the RMD file.
#======================================================
# Specify relative working directories for sub-folders
#======================================================
wd_LUTs <-"lookup tables"       # Location of look-up tables (maybe could be merged with data files??)
wd_data   <-"data"              # Location where data files are stored
wd_source_files<-"source files" # Location of source file (code working "behind the scenes")
wd_models  <-"models"           # Location of model files 
wd_outputs <-"results"          # Location of saved output (summary figures/tables and model results)

#======================================================
# Specify names of .csv data files
#======================================================
effort_file_name <-   "Effort_dat_2021_steelhead_04132021.csv"
interview_file_name <-"Interview_dat_2021_steelhead_cleaned_04132021.csv"
effort_xwalk_filename<-"02_Crosswalk_Table_for_Index_TieIn_Sections_2021-02-04.csv"
river_loc_filename<-"02_River.Locations_2019-01-07.csv"
creel_models_filename<-"02_Creel_Models_2021-01-20.csv"

#======================================================
# Denote data of interest (used to filter data below)
#======================================================
# Specify filter type(s) to extract data by (Enter "Y" or "N")
  by.Year<-      "N" # If "Y", will filter by full calendar year(s) (Jan. 1 - Dec. 31)
  by.YearGroup<- "N" # If "Y", will filter by a "Year Group", which go from May 1st Yr1 - April 30 Yr2
  by.Season<-    "N" # If "Y", will filter by "season", which is either Summer (May 1st - Oct 31st) or Winter (Nov. 1 - April 30)
  by.StreamName<-"Y" # If "Y", will filter by stream name
  by.Date<-      "N" # If "Y", will filter by a date range
  
# Specify date ranges for "Year Groups" and "Seasons"
  YearBegin<-  121 # day of year a "YearGroup" begins (FYI - 121 = May 1st in a non-leap year)
  summerBegin<-121 
  summerEnd<-  304 # FYI - 304 = Oct. 31st (in a non-leap year)
  winterBegin<-305 
  winterEnd<-  120 

# Specify filter unit(s)
  # YearGroup.of.Interest<- c("2017-2018") 
  # Season.of.Interest<-    c("Winter") 
  # Year.of.Interest<-      c("2017") 
  StreamName.of.Interest<-c("Skagit")
  # Begin.Date<-            c("2016-05-01") #Format must be "yyyy-mm-dd"
  # End.Date<-              c("2017-03-31") #Format must be "yyyy-mm-dd"
  
#======================================================
# Denote catch group of interest (species_origin_fate) 
#======================================================  
  catch.group.of.interest<-c("SH_W_R") 

#=========================
# Denote fishery closures
#=========================
# NOTE: if the fishery was ever closed during a season, meaning that no legal fishing should have occurred, we can set estimates of catch and effort to zero for those specific dates/sections using a two-step process:
# First, denote the total number of dates when the fishery was closed (the closure can be for the entire fishery -- all sections -- or a portion of the fishery)
  
  total.closed.dates<-30 # Total number of dates that at least one section of the river was closed 
  
# Second, if "total.closed.dates" >0, denote the specific date(s) and section that was/were closed use the following format 
    # the first column is the list of individual dates (by row) the fishery was closed date (format of date should be "yyyy-mm-dd") 
    # the number of additional columns equals the number of sections identified in the "final.effort.section.xwalk" 
    # the value entered below each section column should be either a 1 if the section was open or 0 if the section was closed
    # create a new line for each individual closure by date
                           #    Date   , Section-1, Section-2
    closed.Dates.Sections<-c("2021-02-03",     0,         0 
                            ,"2021-02-04",     0,         0
                            ,"2021-02-05",     0,         0
                            ,"2021-02-10",     0,         0 
                            ,"2021-02-11",     0,         0
                            ,"2021-02-12",     0,         0
                            ,"2021-02-17",     0,         0
                            ,"2021-02-18",     0,         0
                            ,"2021-02-19",     0,         0
                            ,"2021-02-24",     0,         0
                            ,"2021-02-25",     0,         0
                            ,"2021-02-26",     0,         0
                            ,"2021-03-03",     0,         0
                            ,"2021-03-04",     0,         0
                            ,"2021-03-05",     0,         0
                            ,"2021-03-10",     0,         0
                            ,"2021-03-11",     0,         0
                            ,"2021-03-12",     0,         0
                            ,"2021-03-17",     0,         0
                            ,"2021-03-18",     0,         0
                            ,"2021-03-19",     0,         0
                            ,"2021-03-24",     0,         0
                            ,"2021-03-25",     0,         0
                            ,"2021-03-26",     0,         0
                            ,"2021-03-31",     0,         0
                            ,"2021-04-01",     0,         0
                            ,"2021-04-02",     0,         0
                            ,"2021-04-07",     0,         0
                            ,"2021-04-08",     0,         0
                            ,"2021-04-09",     0,         0
                            ) 
```

```{r data_prep,message=FALSE, warning=FALSE,results = "hide",echo=FALSE}
### Data Preparation
#Here we will load and format the data for the analysis using the chunk of code with this heading in the RMD file.
#====================================================
# Load LUTs
  source(paste0(wd_source_files, "/Load_LUTs.R"))

# Load creel data and format
  source(paste0(wd_source_files, "/Import_Skagit_Creel_Data_and_Format.R"))

# Extract data of interest and format 
  ## add code that shows options for filtering data by date/year/season/location
  source(paste0(wd_source_files, "/05_Extract_Data_of_Interest_and_Calculate_Fields_2019-04-08.R"))  

#Run source summary file 
  ## add code that shows options for "catch groups"
  source(paste0(wd_source_files, "/06_Summarize_Effort_and_Catch_Data_for_TimeSeries_Model_2019-04-23.R"))   

##KB note: I will work on updating the code in the "05" and "06" file at some point soon; also, creating a "Import and format" file for data that is from our creel database
```


```{r run_analysis,results = "hide",echo=FALSE,warning=FALSE}

### Run Analysis
#Here we will run the in-season analysis of creel data to estimate CPUE, effort, and catch using the chunk of code with this heading in the RMD file.
#================================================================
#=======
#note for editing: any new priors need to go here, also in "prepare data" and in "summarize inputs"
#=======

# Denote whether you want to run a new model or load "saved" results from a previous model run
  model_source<-c("load_saved")  #enter either "run_new" or "load_saved"

# Assign a "Model_Run" number (if model_source == run_new, results will be saved to a new sub-folder; if model_source == load_saved, previous model results will be loaded)
  Model_Run<-4 #Enter numeric number (NOTE: be careful not to over-write previous models runs by entering a number that's already been used)

# Denote which creel model you want to run
  #creel_models[,1:3] #model summary table
  model_number<-c(4)
  
# Specify time period to stratify data by - day vs. week 
  model_period<-c("day") #enter "day" or "week"
  
# Specify parameter values for model priors
  value_cauchyDF_sigma_eps_C = 0.5#1 # the hyperhyper scale (degrees of freedom) parameter in the hyperprior distribution sigma_eps_C; default = 1  
  value_cauchyDF_sigma_eps_E = 0.5#1 # the hyperhyper scale (degrees of freedom) parameter in the hyperprior distribution sigma_eps_E; default = 1  
  value_cauchyDF_sigma_r_E = 0.5#1 # the hyperhyper scale (degrees of freedom) parameter in the hyperprior distribution sigma_r_E; default = 1  
  value_cauchyDF_sigma_r_C = 0.5#1 # the hyperhyper scale (degrees of freedom) parameter in the hyperprior distribution sigma_r_C; default = 1  
  value_normal_sigma_omega_C_0 = 1  #the SD hyperparameter in the prior distribution omega_C_0; normal sd (log-space); default = 1   
  value_normal_sigma_omega_E_0 =  3 # the SD hyperparameter in the prior distribution omega_E_0; normal sd (log-space);; default = 3  
  value_lognormal_sigma_b = 1 # the SD hyperparameter in the prior distribution b; default = 1  
  value_normal_sigma_B1 = 5 # the SD hyperparameter in the prior distribution B1; default = 5  
  value_normal_mu_mu_C = log(0.02) # the mean hyperparameter in the prior distribution mu_C; median (log-space); default = 0.02 (was originally  0.05) 
  value_normal_sigma_mu_C = 1.5 # the SD hyperparameter in the prior distribution mu_C; normal sd (log-space); default = 1.5 (was originally 5)
  value_normal_mu_mu_E = log(5) # the mean hyperparameter in the prior distribution mu_E; median effort (log-space); default = 15 
  value_normal_sigma_mu_E = 2  # the SD hyperparameter in the prior distribution mu_E; normal sd (log-space); default = 2 (was originally 5) 
  value_betashape_phi_E_scaled = 1 # the rate (alpha) and shape (beta) hyperparameters in phi_E_scaled; default = 1 (i.e., beta(1,1) which is uniform), alternative beta(2,2) 
  value_betashape_phi_C_scaled = 1 # the rate (alpha) and shape (beta) hyperparameters in phi_C_scaled; default = 1 (i.e., beta(1,1) which is uniform), alternative beta(2,2)
  value_cauchyDF_sigma_mu_C = 0.5#1      # the hyperhyper SD parameter in the hyperprior distribution sigma_mu_C
  value_cauchyDF_sigma_mu_E = 0.5#1      # the hyperhyper SD parameter in the hyperprior distribution sigma_mu_E

# Specific Stan model arguments
  n_chain<-4        # set the number of Markov chains. The default is 4.
  n_iter<-6000        # set the number of iterations for each chain (including warmup). The default is 2000.
  n_cores<-4         # set the number of cores to use when executing the chains in parallel. The defaults is 1. NOTE: Stan manual recommends setting the mc.cores option to be as many processors as the hardware and RAM allow (up to the number of chains).
  n_warmup<-3000   # set the length of warm-up (aka burn-in) iterations per chain.  The default is n_iter/2.  
  n_thin<-1          # set the thinning rate (aka, the period for saving samples). The default is 1, which is usually the recommended value.
  adapt_delta<-0.999  # set adapt delta, which is the target average proposal acceptance probability during Stan's adaptation period. Value between 0 and 1. The default is 0.8. Increasing it will force stan to take smaller steps (and thus run time will be longer). 
  max_treedepth<-10 # set the max tree depth; default is 8; NOTE: this sets the max depth of tree used by NUTS during each iteration; warnings about hitting the maximum treedepth is an efficiency concern

# Create sub-folders for output (if they don't already exist)
    source(paste0(wd_source_files, "/Create_output_subfolder.R"), print.eval = TRUE)

# Run source code to prepare data for model
    source(paste0(wd_source_files, "/Prepare_Data_For_Model.R "))
# Run source code to generate creel estimates
  source(paste0(wd_source_files, "/RunNew_or_LoadSaved_Creel_Model.R"))
  
# Generate summaries of model inputs and outputs
  if(model_source == "run_new"){  source(paste0(wd_source_files, "/Summarize_Model_Inputs_and_Outputs.R"))}
```


```{r summarize_results,message=FALSE, warning=FALSE,results = "hide",echo=FALSE}
### Summarize and Save Results
#Here we will summarize and save results of the in-season analysis of creel data to estimate CPUE, effort, and catch using the chunk of code with this heading in the RMD file.
#=============================================
#convergence diagnostics
  launch_diagnostics<-c("No") #Enter "Yes" to launch ShinyShin diagnostics
  if(launch_diagnostics=="Yes"){launch_shinystan(output$res_stan)} 

# generate plots and tables of creel estimates 
  source(file.path(wd_source_files,"Generate_Summaries_of_Creel_Estimates.R")) 
    
# KB note: update so table/plots of results are shown in PDF document
```

```{r, message=FALSE, warning=FALSE,results = "hide",echo=FALSE}
#we will quickly look at the estimated proportion of effort sampled vs estimated proportion of catch sampled
# dat<-extract(res_Stan)
# sample<-as.tibble(melt(apply(dat$E_Creel_array_gen,c(2:4),median))%>%
#   rename(section=Var1,day=Var2,gear=Var3,E=value)%>%
#   mutate(type="sample",E=as.numeric(as.character(E))))
# estimated<-as.tibble(melt(apply(dat$E,c(2:4),median)))%>%
#   rename(section=Var1,day=Var2,gear=Var3,E=value)%>%
#   mutate(type="estimate",E=as.numeric(as.character(E)))
# 
# dat<-bind_rows(sample,estimated)
# ggplot(dat,aes(x=as.numeric(day),y=E,color=type))+
#   geom_line()+
#   facet_wrap(~ as.factor(section) + as.factor(gear))
# 
# #compare psampled effort (observed) with psampled catch (estimated)
# psamp_E_obs<-melt(apply(res$E_Creel_array_gen,c(2:4),median)/apply(res$E,c(2:4),median))
# p_samp_C_est<-melt(apply(1-res$p_unsample_C,c(2:4),median))
# 
# plot(psamp_E_obs$value~p_samp_C_est$value)
# abline(a=0,b=1)
#catch sample rate is correlated with but unequal to effort sample rate!
```

## Pre-season Fishery Planning
This section displays the pre-season Skagit wild winter steelhead run-size forecast, the exploitation rate matrix prescribed by the Resource Management Plan, and uses the two to calculate the allowable harvest. It then compares the planned harvest (sport fishery catch * assumed catch and release mortality rate) with the allowable harvest to estimate the percent of the allowable harvest the sport fishery will use as well as the probability that the sport fishery will exceed various percentages of the allowable impacts.

First, the allowable harvest rates in the RMP are:
```{r message = FALSE, warning = FALSE,results = "asis",include=TRUE, echo=FALSE}
hcr<-read_csv(file.path("data","hcr.csv"))
hcr%>%mutate(MaxRunsize=format(MaxRunsize, scientific = FALSE))%>%
  rename(`Exploitation Rate` = ER, `Minimum Run Size` = MinRunsize,`Maximum Run Size` = MaxRunsize)%>%
  kbl(caption = "Table 1. Allowable Harvest Rates, Skagit RMP ",digits =3)%>%
  kable_classic(full_width = F, html_font = "Cambria")
```

The run size forecast is below, and is reprinted from the co-manager agreed-to [**pre-season forecast**](https://github.com/tbuehrens/Skagit-River-Steelhead-Forecast/blob/master/analysis/App_2_Model_fitting_forecast_2020_2021_step_head_fcst.pdf). A published scientific paper describing the methods may be found here: [**(preseason forecast publication)**](https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/1365-2664.13789). 

For more details on how this forecast was developed, see this [**GitHub repository**](https://github.com/casruff/Skagit-River-Steelhead-Forecast), which is collaboratively maintained by tribal Co-Managers & WDFW.

```{r include=TRUE, fig.align="center", fig.cap=c("Figure 1. Skagit Steelhead Run-size Forecast. The forecast is shown as a 'bell curve' (though not symmetric), where the height of the curve is proportional to the probability that the true value (catch or effort) is equal to a particular value on the x-axis. The 'best' value is the 50th percentile, shown as a dashed line."),echo=FALSE}
runsize<-read_csv("https://raw.github.com/tbuehrens/Skagit-River-Steelhead-Forecast/master/analysis/cache/ensemble_forecast_posterior.csv")
ggplot(data=runsize,aes(x=ensemble_forecast_posterior))+
  geom_density(fill="forest green")+
  geom_vline(data=runsize,mapping=aes(xintercept=median(ensemble_forecast_posterior)),linetype="dashed")+
  xlab("Forecasted Run-Size")+
  theme_bw()
runsize%>%
  summarise(`Run Size` = round(quantile(ensemble_forecast_posterior, c(0.025, 0.25, 0.5,0.75, 0.975))), q = c(0.025,0.25, 0.5,0.75, 0.975)*100)%>%
  dplyr::rename(Percentile=q)%>%
  kbl(caption = "Table 2. Runsize Forecast. The 'best' estimate is the median or 50th percentile. The table shows the forecast percentiles, which represent the probability that the true run-size will fall below a particular value. For example, there is a 50% probability the true run size will be below the 50th percentile of the forecast.",digits =3,format = "html", table.attr = "style='width:40%;'")%>%
  kable_classic(html_font = "Cambria")
```

### Fishery Impacts Relative to Resource Management Plan Limits: Preseason Plan
Below you will find a comparison of the the planned sport fishery catch, assuming 10 percent catch and release mortality, with the allowable harvest based on the run size forecast and the RMP mortality limits.

The allowable harvest, according to the RMP, is the number of fish that may be killed by the combined sport and tribal fisheries. However, as seen above, this number depends on the run-size, which fishery managers will not know until after the fishing season when they estimate escapement. However, based on the pre-season forecast run size, we can estimate what the true allowable harvest will be by combining the forecast run size, including its uncertainty, with the harvest rate matrix prescribed by the RMP. We can then plot the probability that the true allowable harvest will exceed a range of values. The true allowable harvest will be known with more certainty after the season when the actual run-size is known and the appropriate harvest rate from the matrix can be multiplied by the actual run size.

For example, in 2021 there is a greater than 90 percent chance that the true allowable harvest based on the true run size and the harvest matrix, will exceed 100 fish. There is a 50% chance that the allowable harvest will exceed 430 fish (10 percent of the best estimate of the run-size from the forecast), and a very small chance the allowable harvest will be greater than 2,000 fish:
```{r include=TRUE, fig.align="center", fig.cap=c("Figure 2. Probability that the allowable harvest exceeds a particular number of fish based on the RMP. Note the log10 scale on the x-axis."),echo=FALSE}
ER_func<-function(hcr,runsize){
  ER<-c(NULL)
  indexes<-c(NULL)
  for(i in 1:nrow(hcr)){
    runsize=round(runsize)
    min<-hcr$MinRunsize[i]
    max<-hcr$MaxRunsize[i]
    ind<-which(min <= runsize & runsize <= max)
    indexes<-c(indexes,ind)
    ER<-c(ER,rep(hcr$ER[i],length(ind)))
    ERdat<-data.frame(indexes,ER)
  }
 return(ERdat)
}
ER<-ER_func(hcr=hcr,runsize=runsize)
AH<-data.frame(ER$ER,runsize)
AH$AH<-AH$ER.ER*AH$ensemble_forecast_posterior
AH$Exceedence<-rev(sort(percent_rank(AH$AH)))
#hist(AH$AH,breaks=seq(0,max(AH$AH)*1.1,100))

breaks <- 10^(-10:10)
minor_breaks <- rep(1:9, 21)*(10^rep(-10:10, each=9))
ggplot(data=AH,aes(x=AH,y=Exceedence))+
  geom_line(size=1.25)+
  theme_bw()+
  scale_x_log10(breaks = breaks, minor_breaks = minor_breaks)+
  annotation_logticks(base = 10,sides = "b")+
  coord_equal() +
  ylab("Probability of Exceedance")+
  xlab("Allowable (State + Tribal) Co-manager Harvest (fish)")
```

Given the uncertainty in what the allowable harvest will turn out to be, managers must balance the risk of exceeding the allowable harvest with the cost of erring so heavily on the side of conservation (to ensure the allowable harvest is not exceeded) that no fishery is allowed at all. To assist with this task, managers can plan a fishery to harvest a particular number of fish (which itself may be estimated with uncertainty) and then calculate the probability that the allowable harvest will turn out to have been exceeded if the planned fishery is implemented.

For example, in 2021, a four-day a week fishery was planned with a projected total harvest of 76 fish (760 wild fish handled * assumed 10 percent catch and release mortality rate). If the actual catch matches the projection exactly, this would translate to 18 percent of the allowable (sport and tribal) harvest under the RMP if the true allowable harvest is 430 (which is the best estimate from the preseason forecast, and corresponds to the 50% exceedance probability). 

```{r include=TRUE, fig.align="center", fig.cap=c("Figure 3. Probability that the pre-season planned sport fishery harvest exceeds the allowable harvest (expressed as percentages) under the RMP. Vertical lines denote 50 percent of the allowable harvest, which is shared between the state and tribes, and 100 percent of the allowable harvest. The best estimate of the percent of the allowable harvest that will be used by the sport fishery is located on the graph where the probability of exceedance is 50 percent."),echo=FALSE}
preseason_planned_mortalities<-76
H_plan<-rlnorm(1000,log(preseason_planned_mortalities),0.1)
PAH_plan<-(sample(H_plan,10000,replace = T)/sample(AH$AH,10000,replace = T))*100
PAHdat_plan<-data.frame(PAH_plan,percent_rank(-PAH_plan))%>%rename(Exceedence="percent_rank..PAH_plan.")
ggplot(data=PAHdat_plan,aes(x=PAH_plan,y=Exceedence))+
  geom_line(size=1.25)+
  theme_bw()+
  ylab("Probability of Exceedance")+
  xlab("% of Allowable (Sport + Tribal) Co-Manager Harvest")+
  geom_vline(xintercept=50)+
  geom_vline(xintercept=100,col="red")
```
```{r include=TRUE, fig.align="center", fig.cap=c("Figure 4. Pre-season Planned Probability that the sport fishery harvest exceeds 50 and 100 percent of the allowable harvest under the RMP, as well as the best estimate of the percentage of allowable impacts the sport fishery will use"),echo=FALSE}
probs<-c(PAHdat_plan$PAH_plan[which(abs(PAHdat_plan$Exceedence - 0.5)==min(abs(PAHdat_plan$Exceedence - 0.5)))],
  PAHdat_plan$Exceedence[which(abs(PAHdat_plan$PAH - 50)==min(abs(PAHdat_plan$PAH - 50)))]*100,
  PAHdat_plan$Exceedence[which(abs(PAHdat_plan$PAH - 100)==min(abs(PAHdat_plan$PAH - 100)))]*100
  )
par(mar=c(5,15,5,5))
names(probs)<-c("best estimate of sport \nfishery impacts as \n % of total (sport + tribal) \n allowable","probability sport fishery \n impacts > 50% of total \n (sport + tribal) allowable","probability sport fishery \n impacts > 100% of total \n (sport + tribal) allowable")
barplot(probs,horiz = T,xlim=c(0,100),las=2,yaxs="i",col="forest green",main="Pre-season Plan"
        )
box()
```

## In-season Fishery Results

WDFW fishery managers use data from creel surveys to estimate the season total and daily catch and effort, and the daily catch per unit effort (CPUE). Results presented below include data collected through: **`r format(as.Date(all.Dates$Date[nrow(all.Dates)]), '%m/%d/%Y')`**

### Disclaimer
Contents on this page should be treated as preliminary and are subject to change. While every effort has been made to ensure the accuracy of data and modeling here, some numbers may change. Particularly in-season catch and effort estimates may change as additional data becomes available, and as data errors are corrected.

### Season Total Effort and Catch 

This table and the figures below it show estimates of the season total catch and effort. Estimates are shown as 'bell curves' (though not symmetric), where the height of the curve is proportional to the probability that the true value (catch or effort) is equal to a particular value on the x-axis. The "best" estimate is the 50th percentile (median) in the table, and is denoted by the middle dashed line in the graphs:
```{r run_resid_analysis, message = FALSE, warning = FALSE,results = "asis",include=TRUE, echo=FALSE}
results<-read_csv(file.path("results",catch.group.of.interest,paste0("Run_",Model_Run),"summarized_estimates",paste("Summary_Total_Catch_and_Effort",catch.group.of.interest,paste0("Run_",Model_Run),".csv",sep="_")))%>%
  dplyr::rename(Variable=X1)%>%
  dplyr::select(-CV)%>%
  #::rename(`CV (%)` = CV)%>%
  #mutate(`CV (%)`=`CV (%)`* 100)%>%
  kbl(caption = "Table 3. Total Catch (fish) and Effort (hours). The 'best' estimate is the median or 50th percentile. The table shows the percentiles of estimated catch and effort, which represent the probability that the true catch or effort is below a particular value. For example, there is a 50% probability the true catch is below the 50th percentile of the estimated catch in this table.",digits =1)%>%
  kable_classic(full_width = F, html_font = "Cambria")
print(results)
```


```{r, include=TRUE, fig.align="center", fig.cap=c("Figure 5. Season total catch (fish) and effort (angler hours). The middle dashed line shows the posterior median or 'best' estimate. Outer dashed lines show 95 percent credible intervals."),echo=FALSE,warning = FALSE}
season_results<-data.frame(res$C_sum,res$E_sum)%>%
  mutate(iter=row_number())%>%
  rename(`Season Total Catch`=res.C_sum,`Season Total Effort`=res.E_sum)%>%
  pivot_longer(names_to = "Parameter",values_to="value",cols=c(`Season Total Catch`,`Season Total Effort`))

#elimiminate very very extreme quantiles to pretty up plots
lims<-season_results%>%group_by(Parameter)%>%summarise(value = quantile(value, c(0, 0.99)), q = c(0,0.99))%>%pivot_wider(names_from = q,values_from=value)

season_results_trunc=season_results%>%left_join(lims,by="Parameter")%>%filter(value>as.numeric(`0`) & value < as.numeric(`0.99`))

ggplot(season_results_trunc,aes(x=value,fill=Parameter))+
  facet_wrap(~Parameter,ncol=1,  scales = 'free')+
  theme_bw()+
  geom_density()+
  ylab(NULL)+
  geom_vline(season_results%>%group_by(Parameter)%>%summarise(value = quantile(value, c(0.025, 0.5, 0.975)), q = c(0.025, 0.5, 0.975)),mapping=aes(xintercept=value,group=Parameter),linetype="dashed")+
  theme(legend.title = element_blank())

```

### Daily Catch, Effort, and Catch Per Unit Effort 
Below, you will find an estimate of the daily catch (fish), effort (hours), and catch per unit effort (fish/hr). The lines are the "best" estimates and the shading shows the 95 percent credible intervals (which are obtained from the statistical model used to estimate catch).
```{r, include=TRUE, fig.align="center", fig.cap=c("Figure 6. Daily catch. Lines are posterior medians or 'best' estimates, while shading shows 95% credible intervals."),echo=FALSE,warning = FALSE}
closed.Dates.DF<-closed.Dates.DF%>%pivot_longer(cols=c("Skagit","Sauk"))%>%rename(Section=name,closure=value)
Catch.summary<-Catch.summary%>%
  left_join(closed.Dates.DF,by=c("Date","Section"))%>%
  mutate(closure=ifelse(is.na(closure),"Open","Closed"))#%>%
  #mutate(Median=ifelse(closure=="Open",Median,NA),Mean=ifelse(closure=="Open",Mean,NA),l95=ifelse(closure=="Open",l95,NA),u95=ifelse(closure=="Open",u95,NA))
ggplot(Catch.summary,aes(x=as.Date(Date),y=Median,col=as.factor(Gear)))+
  facet_wrap(~Section,ncol=1)+
  geom_line(size=1.2)+
  ylab(paste0("Catch - ",catch.group.of.interest," (fish)"))+
  xlab("")+
  scale_x_date(date_labels = "%b-%d",date_breaks = "weeks")+
  geom_ribbon(aes(ymin=l95, ymax=u95,fill=as.factor(Gear)),alpha=0.2,col=NA)+
  #geom_rect(Catch.summary%>%filter(closure=="Closed"),mapping=aes(xmin = as.Date(Date)-0.5, xmax = as.Date(Date)+0.5, fill = as.factor(closure)),color=NA, ymin = -Inf, ymax = Inf, alpha = 1,fill="grey90")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90),legend.title = element_blank())
```

```{r, include=TRUE, fig.align="center", fig.cap=c("Figure 7. Daily effort. Lines are posterior medians or 'best' estimates, while shading shows 95% credible intervals."), echo=FALSE,warning=F}
Effort.summary<-Effort.summary%>%
  left_join(closed.Dates.DF,by=c("Date","Section"))%>%
  mutate(closure=ifelse(is.na(closure),"Open","Closed"))#%>%
  #mutate(Median=ifelse(closure=="Open",Median,NA),Mean=ifelse(closure=="Open",Mean,NA),l95=ifelse(closure=="Open",l95,NA),u95=ifelse(closure=="Open",u95,NA))
ggplot(Effort.summary,aes(x=as.Date(Date),y=Median,col=as.factor(Gear)))+
  facet_wrap(~Section,ncol=1)+
  geom_line(size=1.2)+
  ylab("Angling Effort (hrs)")+
  xlab("")+
  scale_x_date(date_labels = "%b-%d",date_breaks = "weeks")+
  geom_ribbon(aes(ymin=l95, ymax=u95,fill=as.factor(Gear)),alpha=0.2,col=NA)+
  #geom_rect(Effort.summary%>%filter(closure=="Closed"),mapping=aes(xmin = as.Date(Date)-0.5, xmax = as.Date(Date)+0.5, fill = as.factor(closure)),color=NA, ymin = -Inf, ymax = Inf, alpha = 0.2,fill="grey80")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90),legend.title = element_blank())
```

```{r, include=TRUE, fig.align="center", fig.cap=c("Figure 8. Daily CPUE. Lines are posterior medians or 'best' estimates, while shading shows 95% credible intervals."),echo=FALSE,warning=F}
CPUE.summary<-CPUE.summary%>%
  left_join(closed.Dates.DF,by=c("Date","Section"))%>%
  mutate(closure=ifelse(is.na(closure),"Open","Closed"))#%>%
  #mutate(Median=ifelse(closure=="Open",Median,NA),Mean=ifelse(closure=="Open",Mean,NA),l95=ifelse(closure=="Open",l95,NA),u95=ifelse(closure=="Open",u95,NA))
ggplot(CPUE.summary,aes(x=as.Date(Date),y=Median,col=as.factor(Gear)))+
  facet_wrap(~Section,ncol=1)+
  geom_line(size=1.2)+
  ylab("Catch Per Unit Effort (fish/hrs)")+
  xlab("")+
  scale_x_date(date_labels = "%b-%d",date_breaks = "weeks")+
  geom_ribbon(aes(ymin=l95, ymax=u95,fill=as.factor(Gear)),alpha=0.2,col=NA)+
  #geom_rect(CPUE.summary%>%filter(closure=="Closed"),mapping=aes(xmin = as.Date(Date)-0.5, xmax = as.Date(Date)+0.5, fill = as.factor(closure)),color=NA, ymin = -Inf, ymax = Inf, alpha = 0.2,fill="grey80")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90),legend.title = element_blank())
```

### In-season Impact Monitoring: Fishery Impacts Relative to Resource Management Plan Limits
Here, we compare the actual in-season estimated sport fishery catch, assuming 10% C&R mortality, with the allowable harvest estimated from the forecasted run size and the RMP mortality limits, to estimate the probability that the allowable harvest has been exceeded by the actual fishery (as opposed to the pre-season plan that we looked at previously). Early in the season when little catch has occurred, the probability that the allowable harvest has been exceeded is low, but increases as the season goes on. This analyis enables an in-season quantification of risk by managers, which may be helpful in decision-making:
```{r include=TRUE, fig.align="center", fig.cap=c("Figure 9. Probability that the actual in-season estimated sport fishery harvest exceeds the allowable harvest (expressed as percentages) under the RMP. Vertical lines denote 50 percent of the allowable harvest, which is shared between the state and tribes, and 100 percent of the allowable harvest. The best in-season estimate of the percent of the allowable harvest that has been used by the sport fishery is located on the graph where the probability of exceedance is 50 percent."),echo=FALSE,warning=F}
H<-res$C_sum*0.1
PAH<-(sample(H,10000,replace = T)/sample(AH$AH,10000,replace = T))*100
PAHdat<-data.frame(PAH,percent_rank(-PAH))%>%rename(Exceedence="percent_rank..PAH.")
ggplot(data=PAHdat,aes(x=PAH,y=Exceedence))+
  geom_line(size=1.25)+
  theme_bw()+
  ylab("Probability of Exceedance")+
  xlab("% of Allowable (Sport + Tribal) Co-Manager Harvest")+
  xlim(0,max(quantile(PAHdat$PAH,0.99),100))+
  geom_vline(xintercept=50)+
  geom_vline(xintercept=100,col="red")
```

```{r include=TRUE, fig.align="center", fig.cap=c("Figure 10. Probability that the sport fishery harvest exceeds 50 and 100 percent of the allowable harvest under the RMP, as well as the best estimate of the percentage of allowable impacts the sport fishery will use."),echo=FALSE}
probs<-c(PAHdat$PAH[which(abs(PAHdat$Exceedence - 0.5)==min(abs(PAHdat$Exceedence - 0.5)))],
  PAHdat$Exceedence[which(abs(PAHdat$PAH - 50)==min(abs(PAHdat$PAH - 50)))]*100,
  PAHdat$Exceedence[which(abs(PAHdat$PAH - 100)==min(abs(PAHdat$PAH - 100)))]*100
  )
par(mar=c(5,15,5,5))
names(probs)<-c("best estimate of sport \nfishery impacts as \n % of total (sport + tribal) \n allowable","probability sport fishery \n impacts > 50% of total \n (sport + tribal) allowable","probability sport fishery \n impacts > 100% of total \n (sport + tribal) allowable")
barplot(probs,horiz = T,xlim=c(0,100),las=2,yaxs="i",col="forest green", main="In-season Estimate"
        )
box()
```

<a name="reproducing-the-analysis"></a>

## Reproducing the Analysis
To run the analysis yourself, you can visit the [**code repository**](https://github.com/tbuehrens/CreelAnalysis/) and follow instructions in this [**R Markdown file**](https://github.com/tbuehrens/CreelAnalysis/blob/main/Skagit_Analysis_2021.Rmd). This file and accompanying repository contain the code necessary to reproduce the entire analysis found on this page as well as the page itself. All analyses require R software [**(install R)**](https://cran.r-project.org/) (v4.0+) for data retrieval, data processing, and summarizing model results, and Stan software [**(install Stan)**](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started/) parameter estimation. Additionally, for Stan to work, Rtools must also be installed and the C++ toolchain must be configured correctly: [**(install Rtools)**](https://cran.r-project.org/bin/windows/Rtools/).

***

Page Last Updated: `r format(Sys.time(), '%m/%d/%Y')`.

***