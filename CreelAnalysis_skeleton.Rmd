---
title: CreelAnalysis_skeleton
output:
  html_document:
    fig_caption: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

# setup 

```{r setup, echo = FALSE, message = FALSE}
options(width = 100)
knitr::opts_chunk$set(message = FALSE)
set.seed(123)

library(DBI); library(odbc); library(dbplyr)
library(tidyverse)

# # Load packages, install and load if not already
# packages_list<-c(
#   "DBI", "odbc", "dbplyr",
#   "tidyverse", "lubridate",
#   #"plyr", "reshape2", #really needed?
# 
#   "timeDate", #used for holidays, maybe other? check conflicts with lubridate?
#   "suncalc", #used for daylight/length getSunlightTimes()
#   "rstan", "shinystan", #will autoload dependencies
#   "loo", "MASS",
# 
#   #"devtools", "here", 
# 
#   "readxl"
#   ,
#   #"xlsx", #really needed?
#   
#   # "tinytex",
#   # "kableExtra",
#   # "cowplot",
#   # "ggpubr", 
#   # "RColorBrewer",
# 
#   # "chron",
#   # "data.table" #really needed?
#   
# )
# install_or_load_pack(pack = packages_list)
# install_or_load_pack <- function(pack){
#   create.pkg <- pack[!(pack %in% installed.packages()[, "Package"])]
#   if (length(create.pkg))
#     install.packages(create.pkg, dependencies = TRUE)
#   sapply(pack, require, character.only = TRUE)
# }

```

# data prep

```{r get_creel}
#quick demo, can/should/will redo this after talking to Kale and Danny
get_creel <- function(
  location, 
  date_start, 
  date_end, #default could be Sys.Date()?
  proj_name 
  ){
  date_start <- as.Date(date_start); date_end <- as.Date(date_end)

  event = 
    tbl(con_creel, dbplyr::in_schema("creel","vw_analysis_creel_event")) %>%
    filter(
      str_detect(water_body, location),
      str_detect(project_name, proj_name),
      between(event_date, date_start, date_end)
    ) %>% 
    dplyr::select(creel_event_id, water_body, event_date, tie_in_indicator)

  effort = 
    event %>%
    left_join(tbl(con_creel, dbplyr::in_schema("creel", "vw_analysis_effort_count")) %>% 
        dplyr::select(creel_event_id, location, count_sequence, start_time = effort_start_time, end_time = effort_end_time, count_type, count_quantity)
    , by = "creel_event_id", suffix = c("_event", "_effort")
    )

interview = event %>%
  dplyr::select(-tie_in_indicator) %>% 
  left_join(tbl(con_creel, dbplyr::in_schema("creel", "vw_analysis_interview")) %>% 
      dplyr::select(creel_event_id, interview_id, interview_number,	angler_type,	interview_location, angler_count,	total_group_count, trip_status, fishing_start_time,	fishing_end_time,	interview_time, boat_used, boat_type,	fish_from_boat,	vehicle_count,	trailer_count)
  , by = "creel_event_id", suffix = c("_event", "_interview"))

catch = interview %>%
  left_join(tbl(con_creel, dbplyr::in_schema("creel","vw_analysis_catch")) %>% 
      dplyr::select(creel_event_id, interview_id, catch_id,  species, run, life_stage, fin_mark, fate, fish_count)
  , by = c("creel_event_id", "interview_id"), suffix = c("_interview", "_catch")
  )

d <- list(event = event, effort = effort, interview = interview, catch = catch) %>% map(~collect(.x))

return(d)
}
```

```{r analysis_args}
#can imagine various ways to define this...
# KB: need to be intention about setting start and end dates as it dictates the time frame for which estimates are generated
analysis_args <- list(
  location = "Kalama River",
  date_start = "2017-03-01",   
  date_end = "2017-03-31",
  proj_name = "R5 Steelhead"
)
```

```{r get_creel_demo_object}
***REMOVED***
***REMOVED***

#presumably want to break this out differently at some point, but need to talk to Danny
***REMOVED***

#example single pull
(creel_data <- get_creel(
  location = analysis_args$location,
  date_start = analysis_args$date_start,
  date_end = analysis_args$date_end,
  proj_name = analysis_args$proj_name
  ))

DBI::dbDisconnect(con_creel)

```

```{r kale test arena}
# Load LUTS (these LUTs already exist in "lookup tables" sub-folder) 
    # River Locations
    # Sections x-walk
    # p_TI expansions (currently, model defines this as 1 but need to edit data summary code to use this LUT table)
    # Creel models

dir_input_files <- "input_files"
fn <- list(
  lut_effort_xwalk = "lut_02_Crosswalk_Table_for_Index_TieIn_Sections_2019-01-10.csv",
  lut_river_loc = "lut_02_River.Locations_2019-01-07.csv",
  lut_creel_models = "lut_02_Creel_Models_2021-01-20.csv"
  )
lut <- grep("^lut_", names(fn), value = T) %>% set_names() %>%
  map(~readr::read_csv(file.path(dir_input_files, fn[[.x]])))

# Cross-walk variables
  # "All_dates" master data_frame (defines all dates for fishery; date, daytype, day length); need to link this to River location (lat, long)
  # Currently, this is generated in the "06_Summarize_effort_and_catch_data...R" file
  
# Sections 
  # - Need to create LUT for final set of sections (current structure of model allows data to be collected at whatever spatial scale but typically sections are aggregated -- fewer sections that emcompass larger area; breakpoints could be jurisdiction and/or function of data collection logistics)
  # - Use section LUT to assign final section number to effort and interview data
  # Currently, this is completed in "05_Extract_Data_of_Interest_and_Calculations...R" file
  
# Gear/Angler types
  # - current model requires angler types to be boiled down into two groups (bank, boat)
  # - need users to categorize angler count/interview groupings as either bank or boat
  # - hypothetically, could have alternative model where only direct counts of anglers are used (i.e., doesn't use indirect counts of vehicles and trailers). here, estimates of catch could be generated for a larger number of angler groupings.
  # - Currently, this is completed in "06_Summarize_effort_and_catch_data...R" file
  
# Pair Tie-in (aka census) & index effort count
  # - Need to pair the index count that most closely aligns with the tie-in count for a particular date/section
  # - Use the matching pair to update the count_num of the tie-in count
  # - Currently, this is completed in "05_Extract_Data_of_Interest_and_Calculations...R" file (see "paired.surveys")
  
# catch group of interest
  # - need to combine multiple variables (species, origin, fate) to define a "catch group"

# "Calculations"
    # from interview data: Total_hours fished for an angler groups; (End/interview time - Start Time) X # of anglers in group
    # from interview data: Catch: summary of catch by "catch group" for each angler group (there may be multiple lines of catch if caught on different gears)
    # "corrected" end times (this is a Skagit thing but should think about some more; affects interviews that span multiple "final" sections)
    
# Summarize datasets  
    #  "summ_effort_counts" DF (effort counts summarized by survey type, day/period, gear, section, CountNum)	--- here just collapsing by gear/section


#something like this? will need to calc date info and/or time fields?
#not clear how vehicles/trailers (numeric with NAs) gets used to group/stratify?

all_holidays <- 2016:2024 %>%
  map(~c(
  timeDate::USNewYearsDay(.x), 
  timeDate::USMLKingsBirthday(.x),
  timeDate::USPresidentsDay(.x),
  timeDate::USMemorialDay(.x), 
  timeDate::USIndependenceDay(.x), 
  timeDate::USLaborDay(.x),
  timeDate::USVeteransDay(.x), 
  timeDate::USThanksgivingDay(.x), 
  timeDate::timeDate(
    as.character(
      timeDate::.nth.of.nday(.x, 11, 5, 4))), #Black Friday
  timeDate::USChristmasDay(.x)
  ) %>% as.character()) %>%
  unlist() %>% as.Date(format="%Y-%m-%d")

(d_days <- tibble(Date = seq(as.Date(analysis_args$date_start), as.Date(analysis_args$date_end), by = "day")) %>%
    mutate(
      Day_I = 1:nrow(.),
      Day = weekdays(Date),
      DayType = if_else(Day == "Saturday" | Day == "Sunday" | Day %in% all_holidays, "Weekend", "Weekday"),
      DayType_num = if_else(str_detect(DayType, "end"),1,0),
      DayL = suncalc::getSunlightTimes(
        date = Date,
        tz = "America/Los_Angeles",
        lat = lut$lut_river_loc$Lat,
        lon = lut$lut_river_loc$Long,
        keep=c("dawn", "dusk")
        ) %>%
        mutate(DayL = as.numeric(dusk - dawn)) %>%
        .$DayL
    ))

# START BACK HERE
 
count_types <- c("trailers", "vehicles", "boat_anglers", "bank_anglers")
#close to V_I, T_I, A_I below...?
creel_data$effort %>%
  group_by(
    tie_in_indicator,
    event_date,
    count_sequence,
    location #could/should collapse to "section"?
    ) %>%
  summarise(
    across(c(count_types), ~sum(., na.rm = T)),
    .groups = "drop"
    ) %>%
  left_join(d_days, by = c("event_date_effort" = "Date")) %>%
  #should be an external xwalk LUT, here just showing toy example
  left_join(
    creel_data$effort %>% distinct(location) %>% rownames_to_column("section"),
    by = "location"
  ) %>%
  filter(tie_in_indicator == 0) %>%
  select(tie_in_indicator, day_V = Day_I, countnum_V = count_sequence, section_V = section, V_I = vehicles)

  
    #  "summ_interview" DF (interviews summarized by individual angler group that include columns day/period, gear, section, total hours, catch for catch group of interest)
    #  "summ_interviewed_effort" DF (Total Hours fished for all angler groups interviewed by date, section, gear)
    #  "summ_interviewed_catch" DF (Total catch of catch-group-of-interest for all angler groups interviewed by date, section, gear)
  
# Random QA/QC
    # remove short trips (e.g., interviews from groups fishing less than 30 minutes)
    # remove interviews where start time was after end/int time
    # remove any Angler/Gear types entered as NA

# Filtering
    # subset catch for "catch_group_of_interest"
    # will want to build "catch_group_of_interest" in cases when multiple species/origins/fates are of interest

```


```{r standat_example}
# KB: general comment -- all of the indexing for the various effort counts could potentially be condensed with some "fill with NA" functions and filtering/re-sorting (fingers crossed) 
#DA: sure, this is almost certainly possible, but I'm still not quite clear exactly when/where/how we want to do it

standat <- list(
# Day attributes
	  D =            # int; number of fishing days;                                KB: pull from "master_date" DF
	, G =            # int; final number of unique gear/angler types               KB: pull from subsetted angler x-walk?
	, S =            # int; final number of river sections                         KB: pull from subsetted section x-walk?
	, H =            # int; max number of angler effort counts within a sample day KB: max(z$effort$count_sequence)
	, P_n =          # int; number of days/periods                                 KB: right now, model is set up to running as daily (P_n = D) or weekly (P_n = effectively D/7)
  , w=             # vec; index denoting daytype (day-type is offset in model)   KB: pull from "master_date" DF (1= weekend, 0 = weekday)
	, period =       # int; index denoting fishing day/period                      KB: calculate as 1:P_n (not sure why this isn't a vector)   
	, L =            # vec; daylength (model offset; assumption)                   KB: pull from "master_date" DF
	, O =            # mat; index denoting fishery status                          KB: user defined (1=open, 0 = closed; by period/date and section; 0 defined as 1E-6 for model)

# Vehicle index effort counts                                                    KB: index_vehicles<-"summ_effort_counts" %>% filter(gear == vehicles, survey == Creel)
	, V_n =          # int; total number of individual vehicle index effort counts KB: index_vehicles %>% nrow() 
	, day_V =        # int; index for day/period                                   KB: index_vehicles %>% pull(day/period)
	, section_V =    # int; index for section                                      KB: index_vehicles %>% pull(section)
	, countnum_V =   # int; index for count_num                                    KB: index_vehicles %>% pull(count_num)
	, V_I =          # int; observed # of vehicles                                 KB: index_vehicles %>% pull(Count)
  
# Trailer index effort counts                                                    KB: index_trailers<-"summ_effort_counts" %>% filter(gear == trailers, survey == Creel)
	, T_n =          # int; total number of boat trailer index effort counts       KB: index_trailers %>% nrow() 
	, day_T =        # int; index for day/period                                   KB: index_trailers %>% pull(day/period)
	, section_T =    # int; index for section                                      KB: index_trailers %>% pull(section)
	, countnum_T =   # int; index for count_num                                    KB: index_trailers %>% pull(count_num)
	, T_I =          # int; observed # of boat trailers                            KB: index_trailers %>% pull(Count)
  
# Angler index effort counts                                                     KB: index_anglers<-"summ_effort_counts" %>% filter(gear == angler_groups, survey == Creel); angler_groups=user defined?
	, A_n =           # int; total number of angler index effort counts            KB: index_anglers %>% nrow()       
	, day_A =         # int; index for day/period                                  KB: index_anglers %>% pull(day/period)  
	, gear_A =        # int; index denoting "gear/angler type"                     KB: index_anglers %>% pull(gear/angler) 
	, section_A =     # int; index for section                                     KB: index_anglers %>% pull(section)
	, countnum_A =    # int; index for count_num                                   KB: index_anglers %>% pull(count_num)
	, A_I =           # int; observed # of anglers                                 KB: index_anglers %>% pull(Count)
  
# Census (tie-in) effort counts                                                  KB: TI_anglers<-"summ_effort_counts" %>% filter(survey == "Tie In");
	, E_n =           # int; total number of angler tie-in effort counts           KB: TI_anglers %>% nrow() 
	, day_E =         # int; index denoting day/period                             KB: TI_anglers %>% pull(day/period) 
	, gear_E =        # int; index denoting "gear/angler type"                     KB: TI_anglers %>% pull(gear/angler)
	, section_E =     # int; index for section                                     KB: TI_anglers %>% pull(section)
	, countnum_E =    # int; index for count_num                                   KB: TI_anglers %>% pull(count_num)
	, E_s =           # int; observed # of anglers                                 KB: TI_anglers %>% pull(Count)
  
	# Proportion tie-in expansion
	, p_TI =          # mat; proportion of section covered by tie in counts        KB: user defined (see "Proportional_Expansions_for_Tie_In_Sections_Kalama_Example"; need to format)
  
	# interview data - CPUE                                                        KB: use "summ_interview" DF
	, IntC =          # int; total number of angler interviews with c & h data     KB: summ_interview %>% nrow()                                    	
	, day_IntC =      # int; index denoting day/period                             KB: summ_interview %>% pull(day/period)                      	
	, gear_IntC =     # int; index denoting "gear/angler type"                     KB: summ_interview %>% pull(gear/angler)      						
	, section_IntC =  # int; index for section                                     KB: summ_interview %>% pull(section)   						
	, c =             # int; total catch                                           KB: summ_interview %>% pull(Qty/Count)           						
	, h =             # vec; total hours fished                                    KB: summ_interview %>% pull(Total_Hours) 
  
	# interview data - Total Effort & Catch Creeled                                KB: use "summ_interviewed_effort"
	, IntCreel =      # int; total interviews by sub-groups	                       KB: summ_interviewed_effort %>% nrow()
	, day_Creel =     # int; index denoting day/period                             KB: summ_interviewed_effort %>% pull(day/period)
	, gear_Creel =    # int; index denoting "gear/angler type"                     KB: summ_interviewed_effort %>% pull(gear/angler)					
	, section_Creel = # int; index for section                                     KB: summ_interviewed_effort %>% pull(section)				
	, C_Creel =       # int; total catch                                           KB: summ_interviewed_effort %>% pull(Qty/Count)
	, E_Creel =       # vec; total hours fished                                    KB: summ_interviewed_effort %>% pull(Total_Hours)
  
	# interview data - angler expansion                                            KB: interview_sub<-df_interview %>% filter (V_A, T_A, and A_A != NA) ## these variables are used for indirect angler count expansions
	, IntA =          # int; total number of angler interviews where V_A, T_A, A_A were collected KB: interview_sub %>% nrow()
	, day_IntA =      # int; index denoting day/period                             KB: interview_sub %>% pull(day/period)                     	
	, gear_IntA =     # int; index denoting day/period                             KB: interview_sub %>% pull(gear/angler)        						
	, section_IntA =  # int; index denoting day/period                             KB: interview_sub %>% pull(section)   
	, V_A =           # int; total number of vehicles an angler group brought      KB: interview_sub %>% pull(vehicle_count)  	
	, T_A =           # int; total number of trailers an angler group brought      KB: interview_sub %>% pull(trailer_count)  			
	, A_A =           # int; total number of anglers in the groups interviewed     KB: interview_sub %>% pull(angler_count) ## NOTE: this was used above to calculated "Total_Hours" (fished)
  
	# hyper and hyperhyper parameters (KB: deleted - add back in later)

)

```

```{r user_inputs, message=FALSE, warning=FALSE, echo = FALSE}
# Denote catch group of interest (species_origin_fate) 
catch.group.of.interest<-c("SH_W_R")
```

## load

```{r input_files, message=FALSE, warning=FALSE, results = "hide", echo=FALSE}

dir_input_files <- "input_files"

# #copied contents of "lookup tables" and "data"
# list.files("lookup tables/", full.names = T) %>%
#   fs::file_copy(path = ., new_path = paste0("input_files/lut_",basename(.)))
# list.files("data/", full.names = T) %>%
#   fs::file_copy(path = ., new_path = paste0("input_files/data_",basename(.)))

#can transition to informatively named files
#such that objects are declared responsively/reactively
list.files(dir_input_files, pattern = "lut_")
list.files(dir_input_files, pattern = "data_")

fn <- list(
  lut_effort_xwalk = "lut_02_Crosswalk_Table_for_Index_TieIn_Sections_2019-01-10.csv", #"02_Crosswalk_Table_for_Index_TieIn_Sections_2021-02-04.csv"
  lut_river_loc = "lut_02_River.Locations_2019-01-07.csv",
  lut_creel_models = "lut_02_Creel_Models_2021-01-20.csv",
  
  data_effort = "data_03_Effort_dat - 2019_Skagit_creel_JSH_thru_4-30-19.csv",  #"Effort_dat_2021_steelhead_04132021.csv"
  data_group = "data_03_Interview-dat_2019-Skagit_JSH_thru_4-30-2019.csv" #"Interview_dat_2021_steelhead_cleaned_04132021.csv"
)

lut <- grep("^lut_", names(fn), value = T) %>% set_names() %>%
  map(~readr::read_csv(file.path(dir_input_files, fn[[.x]])))

```

```{r closure_dates, message=FALSE, warning=FALSE, echo = FALSE}
# Denote fishery closures
#total closed should be responsive rather than declared
#and date declaration can/should be much more concise
#for example something like this...
(closure_dates <- bind_rows(
  tibble(closure_begin = "2021-02-03", closure_end = "2021-02-05", section = "1,2"),
  tibble(closure_begin = "2021-02-10", closure_end = "2021-02-12", section = "1,2"),
  tibble(closure_begin = "2021-02-17", closure_end = "2021-02-19", section = "1,2"),
  tibble(closure_begin = "2021-02-24", closure_end = "2021-02-26", section = "1,2"),
  tibble(closure_begin = "2021-03-03", closure_end = "2021-03-05", section = "1,2"),
  tibble(closure_begin = "2021-03-10", closure_end = "2021-03-12", section = "1,2"),
  tibble(closure_begin = "2021-03-17", closure_end = "2021-03-19", section = "1,2"),
  tibble(closure_begin = "2021-03-24", closure_end = "2021-03-26", section = "1,2"),
  tibble(closure_begin = "2021-03-31", closure_end = "2021-04-02", section = "1,2"),
  tibble(closure_begin = "2021-04-07", closure_end = "2021-04-09", section = "1,2")
  ) %>%
  rowwise() %>%
  mutate(closure_date = paste(seq.Date(as.Date(closure_begin), as.Date(closure_end), by = "day"), collapse = ",")) %>%
  separate_rows(closure_date, sep = ",") %>%
  separate_rows(section, sep = ","))
#and total is
nrow(distinct(closure_dates, closure_date)) #or: length(unique(closure_dates$closure_date))
#then easy to..
closure_dates %>% select(closure_date, section) %>% mutate(closure_code = 0) %>% pivot_wider(names_from = section, values_from = closure_code)
#and if truly needed as long character vector: %>% rowwise() %>% mutate(x = list(c(closure_date, `1`, `2`))) %>% select(x) %>% unlist()

```

```{r all_holidays}
#declared as arbitrary fixed object, since no apparent reason for function
all_holidays <- 2016:2024 %>%
  map(~c(
  timeDate::USNewYearsDay(.x), 
  timeDate::USMLKingsBirthday(.x),
  timeDate::USPresidentsDay(.x),
  timeDate::USMemorialDay(.x), 
  timeDate::USIndependenceDay(.x), 
  timeDate::USLaborDay(.x),
  timeDate::USVeteransDay(.x), 
  timeDate::USThanksgivingDay(.x), 
  timeDate::timeDate(
    as.character(
      timeDate::.nth.of.nday(.x, 11, 5, 4))), #Black Friday
  timeDate::USChristmasDay(.x)
  ) %>% as.character()) %>% unlist() %>% as.Date(format="%Y-%m-%d")

```

```{r d_days}
#illustrating an example of building an "expanded dates lattice"
#to which any/all observations are attached
#also needs closure_dates joined?
date_start <- as.Date("2021-02-01")
date_end <- as.Date("2021-04-13")

(d_days <- tibble(Date = seq(min(date_start), max(date_end), by = "day")) %>%
    mutate(
      Day = weekdays(Date),
      DayType = if_else(Day == "Saturday" | Day == "Sunday" | Day %in% all_holidays, "Weekend", "Weekday"),
      DayType_num = if_else(str_detect(DayType, "end"),1,0),
      DayL = suncalc::getSunlightTimes(
        date = Date,
        tz = "America/Los_Angeles",
        lat = lut$lut_river_loc$Lat,
        lon = lut$lut_river_loc$Long,
        keep=c("dawn", "dusk")
        ) %>%
        mutate(DayL = as.numeric(dusk - dawn)) %>%
        .$DayL
    ))

#other possibly useful/needed fields that had been created with effort and group/interview data objects
    # Day = weekdays(Date),
    # DayType = if_else(Day == "Saturday" | Day == "Sunday" | Day %in% all_holidays, "Weekend", "Weekday"),
    # Month = format(Date,"%b"), Month.no = as.numeric(format(Date, "%m")),
    # Year = as.numeric(format(Date, "%Y")),
    # Weeknum = as.numeric(format(Date, "%V")),
    # j.date =  as.numeric(format(Date, "%j"))

```

```{r d_effort}
#port of first portion of: source(paste0(wd_source_files, "/Import_Skagit_Creel_Data_and_Format.R"))

#dropping apparently redundant class coercion of "count columns",
#if expecting malformatted data? could add back something like across(c(count_cols), as.numeric(as.character(.)))

#d_effort <- readr::read_csv(file.path(dir_input_files, fn$data_effort)) %>%
d_effort <- readr::read_csv("C:/Users/auerbdaa/Downloads/Effort_dat_2021_steelhead_04132021.csv") %>%
  filter(!is.na(StreamName)) %>%
  rowid_to_column(var = "Effort.Index") %>%
  rename(Vehicles = Car_Count, Trailers = Trailer_Count) %>%
  mutate(
    Date = as.Date(Survey_Date, format="%m/%d/%Y"),
    across(c(Effort_StartTime, Effort_EndTime), as.character),
    across(c(Effort_StartTime, Effort_EndTime), .fns = list(Dec = function(x) {round(as.numeric(chron::times(x))*24, 3)})),
    
    #should NA be coered to 0? 
    across(c(starts_with("Shore_"), starts_with("Boat_")), ~replace_na(., 0)),
    Boat = Shore_Boat_Motor + Shore_Drift_Raft + Boat_Boat_Motor + Boat_Drift_Raft,
    Bank = Shore_No_Craft + Shore_Pontoon
  ) %>%
  arrange(Date, Survey.Type, Effort_StartTime)

#Create a "HeaderID" column that denotes each unique survey date
d_effort <- d_effort %>% 
  left_join(
    d_effort %>% distinct(Date) %>% rowid_to_column(var = "HeaderID"),
    by = c("Date")
  ) %>%
  select(
    Effort.Index, HeaderID, 
    StreamName, Date, Surveyor,
    Survey.Type, SectionName, CountNum, Effort_StartTime, Effort_StartTime_Dec, Effort_EndTime, Effort_EndTime_Dec,
    Vehicles, Trailers, Boat, Bank
  )

# #depending on how this is used, could make readable alphanum (since not unary UID?)
# d_effort %>% unite(hid, c(Date, Survey.Type, Effort_StartTime)) %>% count(hid, HeaderID)

d_header <- d_effort %>%
  select(StreamName, Date, Survey.Type, Surveyor, HeaderID) %>%
  distinct(HeaderID, .keep_all = T)

```

```{r d_group}
#d_group <- readr::read_csv(file.path(dir_input_files, fn$data_group)) %>%
d_group <- readr::read_csv("C:/Users/auerbdaa/Downloads/Interview_dat_2021_steelhead_cleaned_04132021.csv") %>%
  filter(!is.na(StreamName)) %>%
  rowid_to_column(var = "Group.Index") %>%
  rename(Vehicles = Num_Cars, Trailers = Num_Trailers) %>%
  mutate(
    Date = as.Date(Survey_Date, format="%m/%d/%Y"),
    #DAA: 2019 data has Fishing_time_corrected but that's removed...?
    across(c(Fishing_Start_Time, Fishing_End_Time_Corrected, Interview_Start_Time), as.character),
    across(c(Fishing_Start_Time, Fishing_End_Time_Corrected, Interview_Start_Time), .fns = list(Dec = function(x) {round(as.numeric(chron::times(x))*24, 3)})),

    # Day = weekdays(Date),
    # DayType = if_else(Day == "Saturday" | Day == "Sunday" | Day %in% all_holidays, "Weekend", "Weekday"),
    # Month = format(Date,"%b"), Month.no = as.numeric(format(Date, "%m")),
    # Year = as.numeric(format(Date, "%Y")),
    # Weeknum = as.numeric(format(Date, "%V")),
    # j.date =  as.numeric(format(Date, "%j")),
    # YearGroup = if_else(
    #   as.numeric(format(Date, "%j")) >= YearBegin,
    #   paste(as.numeric(format(Date, "%Y")), as.numeric(format(Date, "%Y"))+1, sep="-"),
    #   paste(as.numeric(format(Date, "%Y"))-1, as.numeric(format(Date, "%Y")), sep="-")),
    # Season = if_else(as.numeric(format(Date, "%j")) >= summerBegin & as.numeric(format(Date, "%j"))<=summerEnd, "Summer", "Winter"),
    
    Trip.Status = tolower(CompleteTrip),
    Trip.Status = case_when(
        Trip.Status == "no" ~ "I",
        Trip.Status == "yes" ~ "C",
        Trip.Status == "unk" ~ "I"
      ), #marginally unstable but should return NA
    Total_Hours = NumAnglers * if_else(Trip.Status == "C",
                          Fishing_End_Time_Corrected_Dec - Fishing_Start_Time_Dec,
                          Interview_Start_Time_Dec - Fishing_Start_Time_Dec
                          ),
    Angler.Type = stringr::str_sub(toupper(AnglerCategory), 1, 1), #should take S/B/NA
    Count = replace_na(Qty, replace = 0)
  ) 
#add GroupNum, select cols
d_group <- d_group %>%
  left_join(
    d_group %>% distinct(Date, `Page #`, Row) %>% rowid_to_column(var = "GroupNum"),
    by = c("Date", "Page #", "Row")
  ) %>%
  select(
    Group.Index, StreamName, SectionName, Date,
    #Year, YearGroup, Season, Month, Month.no, Weeknum, j.date, Day, DayType,
    GroupNum, Angler.Type, NumAnglers, Vehicles, Trailers,
    Fishing_Start_Time, Fishing_Start_Time_Dec, 
    Interview_Start_Time, Interview_Start_Time_Dec, Percent_time_fished,
    Trip.Status, Species, Origin, Fate, Count, Total_Hours 
  )

```

```{r maybe_d_days_joins}
#fields missing/NA for days without data collection
d_days %>% left_join(d_effort, by = "Date")

setdiff(distinct(d_effort, Date), distinct(d_group, Date)); setdiff(distinct(d_group, Date), distinct(d_effort, Date))
#add days-fields to dates with data collection
#illustrating very silly example of EDA
left_join(d_group, d_days, by = "Date") %>% 
  filter(Species == "SH", Fate == "R") %>%
  group_by(Date) %>%
  summarise(
    date_cnt = sum(Count),
    date_hrs = sum(Total_Hours),
    date_cnt_per_hrs = date_cnt / date_hrs,
    date_total_cnt = DayL * date_cnt_per_hrs
    ) %>%
  ggplot(aes(Date, date_total_cnt)) + geom_point()

```

Alternative row subsets. Couple of easy options to streamline, depending on whether repeat re-runs with alternative subsets are realistically going to happen.

  1. just map all plausible splits
  2. pass all condition arg(s) to ensure explicit specification (NA or missing field problems?)

```{r filter_atl2}
#it is necessary to filter 3 objects separately or
#can/should/will they be joined to allow a single pass filter (that also ensures no mismatches)?
#or a single filter on a master object with control fields to which data are then joined?

filter_args <- list(year = 2019, stream = "Skagit", month = 2)

filter(
  d_header,
  Year == filter_args$year,
  StreamName == filter_args$stream,
  Month.no == 2
  )

```

## calc fields

Need to circle back to this. Looks easier/faster to just figure out a tidy route to what will eventually be needed than try to refactor linewise as-is.

```{r calc_fields, message=FALSE, warning=FALSE,results = "hide",echo=FALSE}
#Julian to Calendar Date Table
month.one<-as.data.frame(matrix(as.character(seq.Date(as.Date(paste(as.numeric(format(min(sub.header.dat$Date), "%Y")), format(min(sub.header.dat$Date), "%m"), "01", sep="-"))
                                                      , as.Date(paste(as.numeric(format(max(sub.header.dat$Date), "%Y")), format(max(sub.header.dat$Date)+30, "%m"), "01", sep="-")),
                                                      by="month"))))
names(month.one)<-c("Date")
month.one$j.date<-(as.numeric(format(as.Date(month.one$Date), "%j")))
month.one$month.day<-format(as.Date(month.one$Date), "%b-%d")
month.one$month<-format(as.Date(month.one$Date), "%b")

#Define "Section.Name" and "Section.Num" based on cross-walk table
#Effort data  
sub.effort.dat$final.Section.Name<-NA; sub.effort.dat$Section.Num<-NA; 
for(row in 1:nrow(sub.effort.dat)){
  sub.effort.dat$final.Section.Name[row]<-as.character(effort_xwalk$Section.Name[as.character(effort_xwalk$Section.Field) == as.character(sub.effort.dat$SectionName[row]) & as.character(effort_xwalk$YearGroup) == as.character(sub.effort.dat$YearGroup[row])])
  sub.effort.dat$Section.Num[row]<-effort_xwalk$Section.Summ[as.character(effort_xwalk$Section.Field) == as.character(sub.effort.dat$SectionName[row]) & as.character(effort_xwalk$YearGroup) == as.character(sub.effort.dat$YearGroup[row])]
}  

#Interview data
unique(sub.group.dat$SectionName); table(sub.group.dat$SectionName)
#NOTE: 14 groups had their fishing section denoted as "Both".  Since this is not an option in the lookup table and we want to define as either Skagit or Sauk, I arbitrarily defined the section for these groups as being Skagit
sub.group.dat[sub.group.dat$SectionName == "Both",]
sub.group.dat$SectionName[sub.group.dat$SectionName == "Both"]<-"Dalles bridge to Marblemount bridge"
sub.group.dat$final.Section.Name<-NA; sub.group.dat$Section.Num<-NA
for(row in 1:nrow(sub.group.dat)){
  sub.group.dat$final.Section.Name[row]<-as.character(effort_xwalk$Section.Name[as.character(effort_xwalk$Section.Field) == as.character(sub.group.dat$SectionName[row]) & as.character(effort_xwalk$YearGroup) == as.character(sub.group.dat$YearGroup[row])])
  sub.group.dat$Section.Num[row]<-effort_xwalk$Section.Summ[as.character(effort_xwalk$Section.Field) == as.character(sub.group.dat$SectionName[row]) & as.character(effort_xwalk$YearGroup) == as.character(sub.group.dat$YearGroup[row])]
}  

#Create index effort x-walk table (that will help define "outage" dates by section in "master" creel analysis file)
sub.river.effort.xwalk<-effort_xwalk[effort_xwalk$StreamName %in% unique(sub.effort.dat$StreamName), ]
final.effort.section.xwalk<-setNames(as.data.frame(matrix(NA, nrow=length(unique(sub.river.effort.xwalk$Section.Name)), ncol=2)), c("Section.Name", "Section.Num"))
for(section in 1:length(unique(sub.river.effort.xwalk$Section.Name))){
  final.effort.section.xwalk[section, "Section.Name"]<-as.character(unique(sub.river.effort.xwalk$Section.Name[sub.river.effort.xwalk$Section.Name == unique(sub.river.effort.xwalk$Section.Name)[section]]))
  final.effort.section.xwalk[section, "Section.Num"] <-unique(sub.river.effort.xwalk$Section.Summ[sub.river.effort.xwalk$Section.Name == unique(sub.river.effort.xwalk$Section.Name)[section]])
}
final.effort.section.xwalk

# #Calculate time to complete effort count by section
#     sub.effort.dat$Count.Time.Minutes<-NA
#     for(row in 1:nrow(sub.effort.dat)){as.numeric(ifelse(is.na(sub.effort.dat$Effort_EndTime_Dec[row])==FALSE & is.na(sub.effort.dat$Effort_StartTime_Dec[row])==FALSE ,sub.effort.dat$Count.Time.Minutes[row]<-round((sub.effort.dat$Effort_EndTime_Dec[row] - sub.effort.dat$Effort_StartTime_Dec[row])*60, 0), NA))}  

    #Identify dates when both index and tie-in surveys were completed on same day 
        index.dates<-unique(sub.effort.dat$Date[sub.effort.dat$Survey.Type == "Creel" ])
        tie.dates<-unique(sub.effort.dat$Date[sub.effort.dat$Survey.Type == "TieIn" ])
        paired.dates<-index.dates[index.dates %in% tie.dates]
        
    #Create summary of creel counts to identify which one pairs best with tie-in count
        paired.surveys<-setNames(as.data.frame(matrix(NA, nrow=0, ncol=8)), c("Date", "Count", "Index.Start", "Index.End", "TI.Start", "TI.End", "Start.Diff", "End.Diff"))
        for(date in 1:length(paired.dates)){
          sub.date<-sub.effort.dat[sub.effort.dat$Date == unique(paired.dates)[date],]
          counts<-unique(sub.date$CountNum)
          
          sub.paired.surveys<-setNames(as.data.frame(matrix(NA, nrow=max(sub.effort.dat$CountNum), ncol=8)), c("Date", "Count", "Index.Start", "Index.End", "TI.Start", "TI.End", "Start.Diff", "End.Diff"))
          
          tie.in.data<-sub.date[sub.date$Survey.Type == "TieIn", ]
          
            for(count in 1:length(counts)){
                sub.count<-sub.date[sub.date$CountNum == unique(sub.date$CountNum)[count] & sub.date$Survey.Type == "Creel",]
                
                sub.paired.surveys[count, "Date"]<-as.character(unique(paired.dates)[date])    
                sub.paired.surveys[count, "Count"]<-as.character(unique(sub.date$CountNum)[count])
                sub.paired.surveys[count, "Index.Start"]<-min(sub.count$Effort_StartTime_Dec, na.rm=TRUE)
                sub.paired.surveys[count, "Index.End"]<-max(sub.count$Effort_EndTime_Dec, na.rm=TRUE)
                sub.paired.surveys[count, "TI.Start"]<-min(tie.in.data$Effort_StartTime_Dec, na.rm=TRUE)
                sub.paired.surveys[count, "TI.End"]<-max(tie.in.data$Effort_EndTime_Dec, na.rm=TRUE)
                sub.paired.surveys[count, "Start.Diff"]<-abs(sub.paired.surveys$Index.Start[count] - sub.paired.surveys$TI.Start[count])
                sub.paired.surveys[count, "End.Diff"]<-abs(sub.paired.surveys$Index.End[count] - sub.paired.surveys$TI.End[count] )
            }
          paired.surveys<-rbind(paired.surveys, sub.paired.surveys)
        }  
        paired.surveys

    #Update "CountNum" for TieIn surveys based on summary in "paired.surveys" DF    
        sub.effort.dat$CountNum_New<-NA
        for(row in 1:nrow(sub.effort.dat)){
            ifelse(sub.effort.dat$Survey.Type[row] != "Creel" & as.character(sub.effort.dat$Date[row]) %in% as.character(paired.surveys$Date)
              , sub.effort.dat$CountNum_New[row]<-as.numeric(paired.surveys$Count[paired.surveys$Date == sub.effort.dat$Date[row] & paired.surveys$Start.Diff ==  min(paired.surveys$Start.Diff[paired.surveys$Date == sub.effort.dat$Date[row]])])
              , sub.effort.dat$CountNum_New[row]<-sub.effort.dat$CountNum[row])
        }
        unique(sub.effort.dat$Date[sub.effort.dat$CountNum != sub.effort.dat$CountNum_New])
        
    #Calculate "corrected" End time (if using "Percent_time_fished" field) - unique to Skagit River fishery
        if(any(colnames(sub.group.dat)=="Percent_time_fished")==TRUE){
          sub.group.dat$End.Time.Dec<-NA
          for(row in 1:nrow(sub.group.dat)){
            if(sub.group.dat$Percent_time_fished[row] == 1.0){
              sub.group.dat$End.Time.Dec[row]<-sub.group.dat$Interview.Time.Dec[row]
              
            }else{
              sub.group.dat$End.Time.Dec[row]<-sub.group.dat$Start.Time.Dec[row] + (sub.group.dat$Interview.Time.Dec[row] - sub.group.dat$Start.Time.Dec[row])*sub.group.dat$Percent_time_fished[row] 
            }
          }
        }

    #Calculate "Hours" fished by Group
        #unique(sub.group.dat$Trip.Status)
        if(any(colnames(sub.group.dat)=="Percent_time_fished")==TRUE){
          for(row in 1:nrow(sub.group.dat)){
          sub.group.dat$Hours[row]<-sub.group.dat$End.Time.Dec[row] - sub.group.dat$Start.Time.Dec[row]
          }
        }else{  
        
        for(row in 1:nrow(sub.group.dat)){
          ifelse(sub.group.dat$Trip.Status[row] == "C", sub.group.dat$Hours[row]<-(sub.group.dat$End.Time.Dec[row] - sub.group.dat$Start.Time.Dec[row]) , sub.group.dat$Hours[row]<-(sub.group.dat$Interview.Time.Dec[row] - sub.group.dat$Start.Time.Dec[row]) )
        }
        }
        
          
    #Calculate "Total_Hours" ("Hours" * "NumAnglers") by Group
        #unique(sub.group.dat$Trip.Status)
        for(row in 1:nrow(sub.group.dat)){
          sub.group.dat$Total_Hours[row]<-(sub.group.dat$Hours[row] * sub.group.dat$NumAnglers[row]) 
        }    

    #Combine "Species", "Origin", and "Fate" to create a unique "Catch.Group" index
      for(row in 1:nrow(sub.group.dat)){
        sub.group.dat$Catch.Group[row]<-paste(sub.group.dat$Species[row], sub.group.dat$Origin[row], sub.group.dat$Fate[row], sep = "_")
      }
      # for(row in 1:nrow(sub.gearfish.dat)){
      #   sub.gearfish.dat$Catch.Group[row]<-paste(sub.gearfish.dat$Species[row], sub.gearfish.dat$Origin[row], sub.gearfish.dat$Fate[row], sep = "_")
      # }

```

## stan prep

### 6A "all.Dates" x-walk table and create vector for open/closed fishery dates

```{r stan_6a, message=FALSE, warning=FALSE,results = "hide",echo=FALSE}
# source(paste0(wd_source_files, "/06_Summarize_Effort_and_Catch_Data_for_TimeSeries_Model_2019-04-23.R"))   

#Denote date range of dataset
  Date_Begin<-min(sub.effort.dat$Date) #c("2018-04-01") #Define date as the first day for which you want to make an estimate for the fishery (not necessarily first survey date)
  Date_End<-  max(sub.effort.dat$Date) #c("2018-04-30")    

#6A
  #Create cross-walk table "all.Dates" that defines the relative "day" and "week" number based on "Date_Begin" and "Date_End"
      all.Dates<-setNames(as.data.frame(cbind(seq(1, as.numeric(as.Date(Date_End)-as.Date(Date_Begin)+1), 1) , as.character(seq(as.Date(Date_Begin), as.Date(Date_End), 1)))), c("Day", "Date"))
      all.Dates$Year<-as.numeric(format(as.Date(all.Dates$Date), "%Y"))
      all.Dates$Month<-format(as.Date(all.Dates$Date), "%b")
      all.Dates$Month.no<-as.numeric(format(as.Date(all.Dates$Date), "%m"))
      all.Dates$DayofWeek<-weekdays(as.Date(all.Dates$Date)); 
      all.Dates$DayType<-ifelse(all.Dates$DayofWeek=="Saturday"|all.Dates$DayofWeek=="Sunday",1,0)
      all.Dates$DOY<-(as.numeric(format(as.Date(all.Dates$Date), "%j")))
      all.Dates$WeekNum_abs<-as.numeric(format(as.Date(all.Dates$Date), "%V"))
      all.Dates$Yr_Week<-paste(as.numeric(all.Dates$Year), as.numeric(all.Dates$WeekNum_abs), sep="_")
      head(all.Dates)
      
  ##Day Length
      #NOTE: I have specified day length using dawn & dusk, but this can be changed to several other metrics (e.g., sunrise & sunset)
      day.light<-getSunlightTimes(keep=c("dawn", "dusk"), date=seq(min(as.Date(all.Dates$Date)), max(as.Date(all.Dates$Date)),1), lat = river_loc$Lat[river_loc$River == StreamName.of.Interest], lon=river_loc$Long[river_loc$River == StreamName.of.Interest], tz = "America/Los_Angeles")
      day.light$DayL<-as.numeric(day.light[[5]]-day.light[[4]])    

  #Calculate relative day (DayNum_Rel) and week (WeekNum_Rel) columns for "Dates.of.Interest" DF
      #weeknums.abs<-unique(as.numeric(format(as.Date(Dates.of.Interest$Date), "%V")))
      weeknums.abs<-unique(all.Dates$Yr_Week)
      weeknums.rel<-seq(1, length(weeknums.abs),1)
      weeknum_xwalk<-setNames(as.data.frame(cbind(weeknums.abs, weeknums.rel)), c("Yr_Week", "weeknum"))
      for(row in 1:nrow(all.Dates)){all.Dates$Weeknum_Rel[row]<-as.numeric(weeknum_xwalk$weeknum[as.character(weeknum_xwalk$Yr_Week) == as.character(all.Dates$Yr_Week[row])])}
  
      all.Dates$Daynum_Rel<-all.Dates$Day
      # daynums.abs<-unique(all.Dates$Date)
      # daynum.rel<-seq(1, length(daynums.abs),1)
      # daynum_xwalk<-setNames(as.data.frame(cbind(daynums.abs, daynum.rel)), c("DOY", "daynum"))
      # for(row in 1:nrow(all.Dates)){all.Dates$Daynum_Rel[row]<-as.character(daynum_xwalk$daynum[as.character(daynum_xwalk$DOY) == as.character(all.Dates$Day)[row]])}
      head(all.Dates)
        
  #Using user defined "closed.Dates", create vector of data denoting whether the fishery was open (1) or closed (1E-9) by section 
          if(total.closed.dates==0){
              fishery.open.closed<-setNames(cbind(all.Dates$Date, as.data.frame(matrix(1, nrow=nrow(all.Dates), ncol=nrow(final.effort.section.xwalk)))), c("Date", as.character(final.effort.section.xwalk$Section.Name)))
          }else{
              fishery.open.closed<-setNames(cbind(all.Dates$Date, as.data.frame(matrix(1, nrow=nrow(all.Dates), ncol=nrow(final.effort.section.xwalk)))), c("Date", as.character(final.effort.section.xwalk$Section.Name)))
              closed.Dates.DF<-setNames(as.data.frame(matrix(closed.Dates.Sections, nrow=total.closed.dates, ncol=nrow(final.effort.section.xwalk)+1, byrow=TRUE)), c("Date", final.effort.section.xwalk$Section.Name))
              
              for(date in 1:nrow(all.Dates)){
                  if(all.Dates$Date[date] %in% closed.Dates.DF$Date){
                    for(section in 1:nrow(final.effort.section.xwalk)){
                        
                      fishery.open.closed[date, final.effort.section.xwalk$Section.Name[section]]<-as.numeric(as.character(closed.Dates.DF[as.Date(closed.Dates.DF$Date) == as.Date(all.Dates$Date)[date], final.effort.section.xwalk$Section.Name[section]]))
                    }
                  }
              }
          }
      fishery.open.closed
      fishery.open.closed[fishery.open.closed == 0] <- 1E-6
      fishery.open.closed$Date<-NULL
```

### 6B SUMMARIZE EFFORT DATA

```{r stan_6b, message=FALSE, warning=FALSE,results = "hide",echo=FALSE}
# Sub-set effort data based on defined "Date_Begin" and "Date_End"
    final_effort.dat<-sub.effort.dat[as.Date(sub.effort.dat$Date) %in% as.Date(all.Dates$Date),]
    length(unique(final_effort.dat$Date))

  # Sub-set Day Length data frame based on defined Date_Begin" and "Date_End"
    head(day.light)
    nrow(day.light) 
    DayL<-day.light[as.Date(day.light$date) %in% as.Date(all.Dates$Date),] #>= as.Date(Date_Begin) & as.Date(day.light$date) <= as.Date(Date_End), c("date", "DayL")]
    nrow(DayL)

  #Define aggregates of effort data
    creel.dates<-unique(final_effort.dat$Date)                    # Unique creel survey dates
    n_creel.dates<-length(creel.dates)                            # Number of unique creel survey dates
    
    survey.types<-unique(final_effort.dat$Survey.Type)            # Unique creel survey types (should be two - "Creel" & "TieIn")
    n_survey.types<-length(survey.types)                          # Number of unique creel survey types 
    
    counts<-as.vector(unique(final_effort.dat$CountNum[order(as.numeric(final_effort.dat$CountNum))])) # Unique counts (should be two - "1" & "2")
    n_counts<-length(counts)                                      # Number of unique (index) counts 
    
    gear.types<-c("Vehicles", "Trailers", "Bank", "Boat")         # Unique gear types (of interest)
    gear.short<-c("V", "T", "S", "B")                                       # Abbreviations for gear groups
    n_gear.types<-length(gear.types)                              # Number of unique gear types
    
    sections<-as.vector(unique(final_effort.dat$Section.Num[order(as.numeric(final_effort.dat$Section.Num))])) # Unique survey sections
    section.xwalk<-setNames(as.data.frame(matrix(c(unique(final_effort.dat$Section.Num), unique(final_effort.dat$final.Section.Name)),nrow=length(sections), ncol=2, byrow=FALSE)), c("Section_Num", "Section_Name"))
    n_sections<-length(sections)

  #Create summary effort data frame ("effort_samp_sum") to fill  
    effort_samp_sum<-setNames(as.data.frame(matrix(NA, nrow=0, ncol=8)), c("Date", "Survey", "Gear_Alpha", "Section", "CountNum", "CountTime", "CountTime_Dec", "Count"))
    
  #Loop through effort data (of interest) and fill  
    for(date in 1:n_creel.dates){
        sub_date<-final_effort.dat[final_effort.dat$Date == creel.dates[date], ]
        
        for(survey_type in 1:n_survey.types){
          sub_date.survey<-sub_date[sub_date$Survey.Type == survey.types[survey_type], ]
          
            for(section in 1:n_sections){
              sub_date.survey.section<-sub_date.survey[sub_date.survey$Section.Num == sections[section],]
              
              for(count in 1:n_counts){
                sub_date.survey.section.count<-sub_date.survey.section[sub_date.survey.section$CountNum_New == counts[count], ]
            
                sub.effort_samp_sum<-as.data.frame(matrix(NA, nrow=n_gear.types, ncol=8))
                names(sub.effort_samp_sum)<-c("Date", "Survey", "Gear_Alpha", "Section", "CountNum", "CountTime", "CountTime_Dec", "Count")
                
                for(gear in 1:n_gear.types){
                    sub.effort_samp_sum[gear, "Date"]<-as.character(creel.dates[date])
                    sub.effort_samp_sum[gear, "Survey"]<-as.character(survey.types[survey_type])
                    sub.effort_samp_sum[gear, "Gear_Alpha"]<-as.character(gear.types[gear])
                    sub.effort_samp_sum[gear, "Section"]<- as.character(sections[section])
                    sub.effort_samp_sum[gear, "CountNum"]<-counts[count]
                    sub.effort_samp_sum[gear, "CountTime"]<-ifelse(nrow(sub_date.survey.section.count)==0, NA, sub_date.survey.section.count$Effort_StartTime)
                    sub.effort_samp_sum[gear, "CountTime_Dec"]<-ifelse(nrow(sub_date.survey.section.count)==0, NA, sub_date.survey.section.count$Effort_StartTime_Dec)
                    sub.effort_samp_sum[gear, "Count"]<-ifelse(nrow(sub_date.survey.section.count)==0, NA, sum(sub_date.survey.section.count[, gear.types[gear]] ) )
                    
                }
                effort_samp_sum<-rbind(effort_samp_sum, sub.effort_samp_sum)
              }
            }
        }
    }
    effort_samp_sum[1:20,]

  #Add columns to "effort_samp_sum" that make variable values numeric 
    ##Day   
      effort_samp_sum$Day<-NA; for(row in 1:nrow(effort_samp_sum)){ effort_samp_sum$Day[row]<-as.numeric(as.character(all.Dates$Daynum_Rel[all.Dates$Date == effort_samp_sum$Date[row]]))}
        
    ##DayType    
      effort_samp_sum$DayType<-NA; for(row in 1:nrow(effort_samp_sum)){ effort_samp_sum$DayType[row]<-as.numeric(as.character(all.Dates$DayType[all.Dates$Date == effort_samp_sum$Date[row]]))}
        
    ##Weeknum
      effort_samp_sum$Week<-NA; for(row in 1:nrow(effort_samp_sum)){ effort_samp_sum$Week[row]<-as.numeric(as.character(all.Dates$Weeknum_Rel[all.Dates$Date == effort_samp_sum$Date[row]]))}
      n_week<-length(unique(effort_samp_sum$Week))  
        
    ##Gear
      gear.types; gear.short
      all.Gear<-setNames(as.data.frame(cbind(seq(1, length(gear.types),1), gear.types, gear.short)), c("Gear", "Gear_Alpha", "Gear_Short"))
      effort_samp_sum$Gear<-NA; for(row in 1:nrow(effort_samp_sum)){ effort_samp_sum$Gear[row]<-as.numeric(as.character(all.Gear$Gear[all.Gear$Gear_Alpha == effort_samp_sum$Gear_Alpha[row]]))}

    ##UniqueID (based on "Week"_"Day"_"CountNum")  
      effort_samp_sum$UniqueID<-NA; for(row in 1:nrow(effort_samp_sum)){effort_samp_sum$UniqueID[row]<-paste(effort_samp_sum$Week[row], effort_samp_sum$Day[row], effort_samp_sum$CountNum[row], sep = "_")}
      head(effort_samp_sum)
      
    ##CountNum by Week
        CountNum_byWeek_Xwalk<-setNames(as.data.frame(matrix(NA,nrow=0, ncol=2)), c("UniqueID", "CountNum_Week"))
        for(week in 1:length(unique(effort_samp_sum$Week))){
          sub.week<-effort_samp_sum[effort_samp_sum$Week == unique(effort_samp_sum$Week)[week],]
          
          sub.CountNum_byWeek_Xwalk<-setNames(as.data.frame(matrix(NA,nrow=length(unique(sub.week$UniqueID)), ncol=2)), c("UniqueID", "CountNum_Week"))
          
          for(ID in 1:length(unique(sub.week$UniqueID))){
            sub.CountNum_byWeek_Xwalk[ID, "UniqueID"]<-as.character(unique(sub.week$UniqueID)[ID])
            sub.CountNum_byWeek_Xwalk[ID, "CountNum_Week"]<-ID
                      }
          CountNum_byWeek_Xwalk<-rbind(CountNum_byWeek_Xwalk, sub.CountNum_byWeek_Xwalk)
        }
        head(CountNum_byWeek_Xwalk)
        
    effort_samp_sum$CountNum_Week<-NA; for(row in 1:nrow(effort_samp_sum)){effort_samp_sum$CountNum_Week[row]<- CountNum_byWeek_Xwalk$CountNum_Week[CountNum_byWeek_Xwalk$UniqueID == effort_samp_sum$UniqueID[row]]}
    
    # #Summarize data to make sure "CountNum_Week" summary worked
    #   effort_samp_sum %>%group_by(Week) %>%summarise(n_distinct(Date)) %>% print(n=50)
    #   effort_samp_sum %>%group_by(Week) %>%summarise(n_distinct(CountNum_Week))
    #   effort_samp_sum %>%group_by(Week) %>% distinct(Date)%>% print(n=60)

  #Index samples
    I_samp<-effort_samp_sum[effort_samp_sum$Survey == "Creel", c("Day", "DayType", "Week", "Gear", "Section", "CountNum", "CountNum_Week", "Count")]
    I_samp<-I_samp[!is.na(I_samp$Count),] #Drop rows where Count == "NA"
    I_samp$Section<-as.numeric(I_samp$Section)
    V_samp<-I_samp[I_samp$Gear==1,] # & !is.na(I_samp$Count)
    T_samp<-I_samp[I_samp$Gear==2,]
    A_samp<-I_samp[I_samp$Gear==3 | I_samp$Gear==4,]

  #Tie-In samples
    E_s_samp<-effort_samp_sum[effort_samp_sum$Survey == "TieIn", c("Day", "DayType", "Week", "Gear", "Section", "CountNum", "CountNum_Week", "Count")]
    E_s_samp<-E_s_samp[is.na(E_s_samp$Count)==FALSE,] #Drop rows where Count == "NA"
    E_s_samp$Section<-as.numeric(E_s_samp$Section)
    
  #Re-index gear-types for tie-in counts (bank(3) == 1; boat(4) ==2)  
    E_s_samp$Gear[E_s_samp$Gear == 3] <- 1; 
    E_s_samp$Gear[E_s_samp$Gear == 4] <- 2;
```

### 6C SUMMARIZE cATCH DATA

```{r ready_for_stan, message=FALSE, warning=FALSE,results = "hide",echo=FALSE}
head(sub.group.dat)
    #head(sub.gearfish.dat)
  
  # Sub-set group (interview) data based on defined "Date_Begin" and "Date_End"
    final_group.dat<-sub.group.dat[as.Date(sub.group.dat$Date) %in% as.Date(all.Dates$Date),]
    length(unique(final_group.dat$Date))    
  
  #Order data by Date and GroupNum
    final_group.dat<-final_group.dat[order(as.Date(final_group.dat$Date), final_group.dat$GroupNum),]
    #sub.gearfish.dat<-sub.gearfish.dat[order(as.Date(sub.gearfish.dat$Date), sub.gearfish.dat$GroupNum),]
    
  #Drop "final_group.dat" data if "Start.Time" is <NA>, "Interview.Time" is <NA> for groups with Trip.Status == I, or "End.Time" is <NA> for groups with Trip.Status == C
    drop.group.indices<-c(final_group.dat$GroupNum[is.na(final_group.dat$Start.Time)==TRUE] #Missing Start Time
      , final_group.dat$GroupNum[is.na(final_group.dat$Interview.Time)==TRUE & final_group.dat$Trip.Status=="I" ] #Missing Interview Time for Incomplete Trips
      , final_group.dat$GroupNum[is.na(final_group.dat$Start.Time)==FALSE & is.na(final_group.dat$End.Time.Dec)==TRUE & final_group.dat$Trip.Status=="C" ] #Missing Interview Time for Completed Trips
      )
    length(drop.group.indices)
    ifelse(length(drop.group.indices)==0, final_group.dat<-final_group.dat, final_group.dat<-final_group.dat[as.character(final_group.dat$GroupNum) %!in% as.character(drop.group.indices),])

  #Drop rows if hours fished by a Group is Negative 
    nrow(final_group.dat[final_group.dat$Hours<=0 | is.na(final_group.dat$Hours)==TRUE,])
    final_group.dat<-final_group.dat[final_group.dat$Hours>0 & is.na(final_group.dat$Hours)==FALSE,]
    
  #Assign values for "Count" - prior to eDate capture, "Count" was not a entered field and therefore all rows = NA
    final_group.dat$Count_New<-final_group.dat$Count
    
  #Summarize group level data by "Date" and "GroupNum" and "final.Section.Name" (NOTE: needed to add section for Skagit data because anglers could report fishing in multiple sections)
    for(row in 1:nrow(final_group.dat)){final_group.dat$temp.Group[row]<-paste(final_group.dat$Date[row], final_group.dat$GroupNum[row], final_group.dat$final.Section.Name[row], sep="_")}
    #uniq.Groups<-unique(final_group.dat$temp.Group)
    sub.unique.group.dat<-final_group.dat[!duplicated(final_group.dat$temp.Group),]
    
  #Identify catch groupings of interest
    unique(final_group.dat$Catch.Group)
    n_catch.groups<-length(catch.group.of.interest)
    
  #Summarize Catch of Interest by Angler Group
    catch.dat_byGroup<-setNames(as.data.frame(matrix(NA, nrow=length(unique(final_group.dat$temp.Group))*length(catch.group.of.interest), ncol=3)), c("temp.Group", "Catch.Group", "Count"))
    for(group in 1:length(unique(final_group.dat$temp.Group))){
        sub.Group.Index<-final_group.dat[final_group.dat$temp.Group == unique(final_group.dat$temp.Group)[group], ]
      
        for(spp.group in 1:length(catch.group.of.interest)){
          
          catch.dat_byGroup[(group-1) * n_catch.groups  + spp.group, "temp.Group"]<-unique(final_group.dat$temp.Group)[group]
          catch.dat_byGroup[(group-1) * n_catch.groups  + spp.group, "Catch.Group"]<-catch.group.of.interest[spp.group]
          catch.dat_byGroup[(group-1) * n_catch.groups  + spp.group, "Count"]<-sum(sub.Group.Index$Count_New[sub.Group.Index$Catch.Group==catch.group.of.interest[spp.group]])
        }
    }
    head(catch.dat_byGroup)
    tail(catch.dat_byGroup)
    aggregate(catch.dat_byGroup$Count ~ catch.dat_byGroup$Catch.Group, FUN=sum)
    
  #Summarize "catch.dat_byUniqueGroup"
   catch.dat_byUniqueGroup<- catch.dat_byGroup %>% spread(Catch.Group, Count) 
   head(catch.dat_byUniqueGroup)
   length(unique(catch.dat_byUniqueGroup$temp.Group)); nrow(catch.dat_byUniqueGroup)
   
  #Create new datasets by joining "sub.unique.group.dat" and "catch.dat_byUniqueGroup" using "temp.Group"
    group.fish.dat<-left_join(sub.unique.group.dat, catch.dat_byUniqueGroup, by="temp.Group")
    length(unique(group.fish.dat$Group.Index)); nrow(group.fish.dat)
    head(group.fish.dat) 
    
  # #Assign 0s (catch) for rows with values == NA (NOTE: this happens if a group was interviewed but no gear/fish entry was created either in database and/or on back of datasheet) 
  #   group.fish.dat[, catch.group.of.interest][is.na(group.fish.dat[, catch.group.of.interest])]<-0

  #Sub-set data of interest
    Group_Catch<-group.fish.dat[, c("Date", "Weeknum", "DayType", "Angler.Type", "temp.Group",  "Section.Num", "Hours", "Trip.Status", "NumAnglers", "Vehicles", "Trailers", "Total_Hours", catch.group.of.interest)]
    head(Group_Catch)

  #Drop group interviews if "Hours" less than threshold AND "Trip.Status" == I
    drop.short.trips<-"Y" #Enter "Y" (yes) or "N" (no)
    min.fish.threshold<-0.5 #If "Y", enter as minimum trip length threshold for groups that are still fishing (i.e., Trip.Status == "I")
    if(drop.short.trips == "Y"){
        Group_Catch<-Group_Catch[Group_Catch$Trip.Status == "C" |  (Group_Catch$Trip.Status == "I" & Group_Catch$Hours >= min.fish.threshold),]
    }
    
  #Drop data if "Angler.Type" is equal to "NA"
    Group_Catch<-Group_Catch[is.na(Group_Catch$Angler.Type)==FALSE,]
    
  #Sub-set catch data based on defined Date_Begin" and "Date_End"
    final_Group_Catch.dat<-Group_Catch[as.Date(Group_Catch$Date) >= as.Date(Date_Begin) & as.Date(Group_Catch$Date) <= as.Date(Date_End), ]
    length(unique(final_Group_Catch.dat$Date)) 
    length(unique(final_effort.dat$Date))
    
  #Add columns to "final_Group_Catch.dat" that make variable values numeric 
    ##Day
        final_Group_Catch.dat$Day<-NA; for(row in 1:nrow(final_Group_Catch.dat)){ final_Group_Catch.dat$Day[row]<-as.numeric(as.character(all.Dates$Daynum_Rel[as.Date(all.Dates$Date) == as.Date(final_Group_Catch.dat$Date[row])]))}
    ##DayType    
        final_Group_Catch.dat$DayType<-NA; for(row in 1:nrow(final_Group_Catch.dat)){ final_Group_Catch.dat$DayType[row]<-as.numeric(as.character(all.Dates$DayType[as.Date(all.Dates$Date) == as.Date(final_Group_Catch.dat$Date[row])]))}
    ##Gear
        final_Group_Catch.dat$Gear<-NA; for(row in 1:nrow(final_Group_Catch.dat)){ final_Group_Catch.dat$Gear[row]<-as.numeric(as.character(all.Gear$Gear[all.Gear$Gear_Short == final_Group_Catch.dat$Angler.Type[row]]))}
    ##Group
      all.Groups<-setNames(as.data.frame(cbind(seq(1, nrow(final_Group_Catch.dat), 1) , final_Group_Catch.dat$temp.Group)), c("Group", "temp.Group"))
      final_Group_Catch.dat$Angler<-NA; for(row in 1:nrow(final_Group_Catch.dat)){ final_Group_Catch.dat$Angler[row]<-as.numeric(as.character(all.Groups$Group[as.character(all.Groups$temp.Group) == as.character(final_Group_Catch.dat$temp.Group[row])]))}
          #NOTE: if you get a warning here there may be two groups with the same "ID" which would result from a data entry error
      
    ##Weeknum
      final_Group_Catch.dat$Week<-NA; for(row in 1:nrow(final_Group_Catch.dat)){ final_Group_Catch.dat$Week[row]<-as.numeric(as.character(all.Dates$Weeknum_Rel[as.Date(all.Dates$Date) == as.Date(final_Group_Catch.dat$Date[row])]))}

  #Catch samples (to estimate CPUE)
    c_samp<-final_Group_Catch.dat[is.na(final_Group_Catch.dat$Hours)==FALSE, c("Day", "DayType", "Week", "Gear", "Angler", "Section.Num", "Total_Hours", catch.group.of.interest)]
    head(c_samp)
    nrow(c_samp)
    
  #Angler samples (to estimate boats & trailers per angler)
    a_samp<-final_Group_Catch.dat[is.na(final_Group_Catch.dat$NumAnglers)==FALSE & is.na(final_Group_Catch.dat$Vehicles)==FALSE & is.na(final_Group_Catch.dat$Trailers)==FALSE, c("Day", "DayType", "Week", "Gear", "Angler", "Section.Num", "NumAnglers", "Vehicles", "Trailers")]
    head(a_samp)
    nrow(a_samp)  
    
  #Re-index gear-types for interview anglers (bank(3) == 1; boat(4) ==2)  
    c_samp$Gear[c_samp$Gear == 3] <- 1; c_samp$Gear[c_samp$Gear == 4] <- 2;
    a_samp$Gear[a_samp$Gear == 3] <- 1; a_samp$Gear[a_samp$Gear == 4] <- 2;
    gear.xwalk<-setNames(as.data.frame(matrix(c(c(1,2), as.character(all.Gear$Gear_Alpha[c(3:4)])),nrow=2, ncol=2, byrow=FALSE)), c("Gear_Num", "Gear_Name"))

  #total hours creel and total catch sample a per day/gear/section to compare with effort
    C_sample<-c_samp%>%
      group_by(Day,Gear,Section.Num)%>%
      summarise(Total_Hours=sum(Total_Hours),Total_Catch=sum((!!as.name(catch.group.of.interest))))%>%
      filter(Total_Hours>0)
    #sapply(as.data.frame(lapply(JS_stats_ALL, "[[", "ni")), function(x) as.numeric(as.character(x)))
```

### 6E make stan data

```{r data_to_stan}
#getting close to ready to build quickly from initial data load-in chunks

standat <- list(
  D = nrow(d_days)
  #something along the lines of...? But not this!
  foo = d_group %>% 
    filter(Species == "SH", Origin == "W", Fate == "R") %>%
    group_by(GroupNum, ) %>%
    summarise(tot_hr = sum(Total_Hours), .groups = "drop")

)

standat <- list(
  D = nrow(all.Dates),                                               # total number of days in the fishery we are estimating catch for
  w = all.Dates$DayType,                                             # an indexing column with length equal to total number of days in the fishery
  L = DayL$DayL,                                                     # decimal daylength in hrs for each day in 1 : D
  # # #can/should build here off common object
  # # str(all.Dates)   #all.Dates; df of date-related fields as chr & num
  # # #only using DayType (0|1 ~ wkday|wkend/holiday)
  # # str(DayL) #df dusk-dawn
  # # #could work from something like this, maybe needs bind of closure_dates/fishery.open.closed
  # tibble(Date = seq(min(d_header$Date), max(d_header$Date), by = "day")) %>%
  #   mutate(
  #     Day = weekdays(Date),
  #     DayType = if_else(Day == "Saturday" | Day == "Sunday" | Day %in% all_holidays, "Weekend", "Weekday"),
  #     DayType_num = if_else(str_detect(DayType, "end"),1,0),
  #     DayL = suncalc::getSunlightTimes(
  #       date = Date,
  #       tz = "America/Los_Angeles",
  #       lat = lut$lut_river_loc$Lat,
  #       lon = lut$lut_river_loc$Long,
  #       keep=c("dawn", "dusk")
  #       ) %>%
  #       mutate(DayL = as.numeric(dusk - dawn)) %>%
  #       .$DayL
  #   )
  O = as.matrix(fishery.open.closed),                                # matrix denoting whether the fishery was open (1) or closed (1E-9) for each individual "D" (row) and section 1:S (column)
  # str(fishery.open.closed)

  G = length(unique(I_samp$Gear)),                                   # total number of unique gear types (#previously G=n_gear.types)
  #see I_samp below

  S = n_sections,                                                    # total number of census/tie in Sections
  # str(n_sections) #scalar int
  # #appears based on lut effort section number
  # lut$lut_effort_xwalk %>% distinct(Section.Name, Section.Summ)
  # #currently re-derived from joined version on final effort data
  # #so various options depending on how it gets used in modeling
  # left_join(d_effort, lut$lut_effort_xwalk, by = c("SectionName" = "Section.Field")) %>% select(contains("ection")) %>% distinct(Section.Name, Section.Summ)
  
  H = max(as.numeric(V_samp$CountNum), as.numeric(T_samp$CountNum)), # max. number of index effort counts completed on a single survey day
  #see V_samp, T_samp below

# str(A_samp) #empty? CountNum
# C_sample #grouped tibble, Total_Catch, Total_Hours
# str(c_samp) #1828, SH_W_R, Total_Hours 
# str(a_samp) #1828, NumAnglers, Vehicles, Trailers
# str(E_s_samp) # also empty?
# str(I_samp) #484, Gear
# str(T_samp) #242, CountNum (used), but also Count
# str(V_samp) #242, CountNum

  #Angler counts
  A_n = nrow(A_samp),
  A_I = A_samp$Count,
  day_A = A_samp$Day,
  gear_A = A_samp$Gear,
  section_A = A_samp$Section,
  countnum_A = as.numeric(A_samp$CountNum),
  
  #interview data total creeled effort and catch
  IntCreel = nrow(C_sample),
  day_Creel = C_sample$Day,
  gear_Creel = C_sample$Gear,
  section_Creel = C_sample$Section.Num,
  C_Creel = C_sample$Total_Catch,
  E_Creel = C_sample$Total_Hours,

  #interview data - CPUE
  IntC = nrow(c_samp),              # total number of interviews conducted across all surveys dates where CPUE data (c & h) were collected 
  gear_IntC = c_samp$Gear,          # an indexing column with length equal to count of angler interviews with a 	1 or 2 depending on whether the interview was a boat or bank fisherman
  day_IntC = c_samp$Day,            # an indexing column with length equal to count angler interviews with the day of the fishery listed starting at 1 and going to the last day n.
  section_IntC = c_samp$Section.Num, #Section where angler was fishing (1 = Skagit, 2 = Sauk)
  c = c_samp[, catch.group.of.interest[1]], # total catch of [spp.of.interest] for an individual group #c_samp$SH_NOR_R          
  h = abs(c_samp$Total_Hours),        # number of hours an individual angler/group spent fishing

  #interview data - angler expansions
  IntA = nrow(c_samp),              # total number of interviews across all surveys dates where angler expansion data (V_A, T_A, A_A) were collected 
  gear_IntA = c_samp$Gear,          # an indexing column with length equal to count of angler interviews with a 	1 or 2 depending on whether the interview was a boat or bank fisherman
  day_IntA = c_samp$Day,            # an indexing column with length equal to count angler interviews with the day of the fishery listed starting at 1 and going to the last day n.
  section_IntA = c_samp$Section.Num, #Section where angler was fishing (1 = Skagit, 2 = Sauk)
  V_A = a_samp$Vehicles,        # Total number of vehicles an individual angler group brought to the river
  T_A = a_samp$Trailers,        # Total number of Trailers an individual angler group brought to the river
  A_A = a_samp$NumAnglers,      #Total number of anglers in each group interviewed
  
  #tie ins
  E_n = nrow(E_s_samp),          # the total number of census effort counts 
  E_s = E_s_samp$Count,          # the effort count in a tie-in section
  day_E = E_s_samp$Day,          # an indexing column with length equal to count of Census effort counts with the day of the fishery listed starting at 1 and going to the last day n.
  gear_E = E_s_samp$Gear,        # an indexing column with length equal to count of Census effort counts with a 1 or 2 depending on whether the interview was a boat or bank 	fisherman
  section_E = E_s_samp$Section,  # an indexing column with length equal to count of Census effort counts 	with the section number starting at 1 
  countnum_E = as.numeric(E_s_samp$CountNum),
  
  #Trailers
  T_n = nrow(T_samp),            # the total number of index effort counts  (n = index survey dates X sections X # of index count for each day)
  T_I = T_samp$Count,            # sum of the index effort counts (of vehicles & trailers) within a section during an effort count
  day_T = T_samp$Day,            # an indexing column with length equal to count of Index effort counts with the day of the fishery listed starting at 1 and going to the last day n
  gear_T = T_samp$Gear,          # an indexing column with length equal to count of Index effort counts with 	a 1 or 2 depending on whether the interview was a boat or bank fisherman
  section_T = T_samp$Section,    # an indexing column with length equal to count of Index effort counts with the section number starting at 1
  countnum_T = as.numeric(T_samp$CountNum),
  
  #Vehicles
  V_n = nrow(V_samp),            # the total number of index effort counts  (n = index survey dates X sections X counts X angler types)
  V_I = V_samp$Count,            # sum of the index effort vehicle count within a section during an effort count
  day_V = V_samp$Day,            # an indexing column with length equal to count of Index effort counts with the day of the fishery listed starting at 1 and going to the last day n
  gear_V = V_samp$Gear,          # an indexing column with length equal to count of Index effort counts with 	a 1 or 2 depending on whether the interview was a boat or bank fisherman
  section_V = V_samp$Section,    # an indexing column with length equal to count of Index effort counts with the section number starting at 1
  countnum_V = as.numeric(V_samp$CountNum),

)
#standat$p_TE<-matrix(1,nrow=standat$G,ncol=standat$S)   # proportion of section covered by tie in counts: Option 1:usually 1.0, Option 2: miles of tie-in divided by total miles in section, Option 3:Best guess or something else.

```

```{r stan_6e_orig, message=FALSE, warning=FALSE,results = "hide",echo=FALSE}
standat<-list(
  D=nrow(all.Dates),                                               # total number of days in the fishery we are estimating catch for
  G=length(unique(I_samp$Gear)),                                   # total number of unique gear types (#previously G=n_gear.types)
  S=n_sections,                                                    # total number of census/tie in Sections
  H=max(as.numeric(V_samp$CountNum), as.numeric(T_samp$CountNum)), # max. number of index effort counts completed on a single survey day

  #day attributes
  w=all.Dates$DayType,                                             # an indexing column with length equal to total number of days in the fishery we are estimating catch for with a value listed as either 1 or 2 depending on whether the daytype was a weekday or weekend, respectively
  L=DayL$DayL,                                                     # decimal daylength in hrs for each day in 1 : D
  O=as.matrix(fishery.open.closed),                                # matrix denoting whether the fishery was open (1) or closed (1E-9) for each individual "D" (row) and section 1:S (column)

  #Vehicles
  V_n=nrow(V_samp),            # the total number of index effort counts  (n = index survey dates X sections X counts X angler types)
  V_I=V_samp$Count,            # sum of the index effort vehicle count within a section during an effort count
  day_V=V_samp$Day,            # an indexing column with length equal to count of Index effort counts with the day of the fishery listed starting at 1 and going to the last day n
  gear_V=V_samp$Gear,          # an indexing column with length equal to count of Index effort counts with 	a 1 or 2 depending on whether the interview was a boat or bank fisherman
  section_V=V_samp$Section,    # an indexing column with length equal to count of Index effort counts with the section number starting at 1
  countnum_V=as.numeric(V_samp$CountNum),

  #Trailers
  T_n=nrow(T_samp),            # the total number of index effort counts  (n = index survey dates X sections X # of index count for each day)
  T_I=T_samp$Count,            # sum of the index effort counts (of vehicles & trailers) within a section during an effort count
  day_T=T_samp$Day,            # an indexing column with length equal to count of Index effort counts with the day of the fishery listed starting at 1 and going to the last day n
  gear_T=T_samp$Gear,          # an indexing column with length equal to count of Index effort counts with 	a 1 or 2 depending on whether the interview was a boat or bank fisherman
  section_T=T_samp$Section,    # an indexing column with length equal to count of Index effort counts with the section number starting at 1
  countnum_T=as.numeric(T_samp$CountNum),
  
  #Angler counts
  A_n=nrow(A_samp),
  A_I=A_samp$Count,
  day_A=A_samp$Day,
  gear_A=A_samp$Gear,
  section_A=A_samp$Section,
  countnum_A=as.numeric(A_samp$CountNum),
  
  #tie ins
  E_n=nrow(E_s_samp),          # the total number of census effort counts 
  E_s=E_s_samp$Count,          # the effort count in a tie-in section
  day_E=E_s_samp$Day,          # an indexing column with length equal to count of Census effort counts with the day of the fishery listed starting at 1 and going to the last day n.
  gear_E=E_s_samp$Gear,        # an indexing column with length equal to count of Census effort counts with a 1 or 2 depending on whether the interview was a boat or bank 	fisherman
  section_E=E_s_samp$Section,  # an indexing column with length equal to count of Census effort counts 	with the section number starting at 1 
  countnum_E=as.numeric(E_s_samp$CountNum),

  #interview data - CPUE
  IntC=nrow(c_samp),              # total number of interviews conducted across all surveys dates where CPUE data (c & h) were collected 
  gear_IntC=c_samp$Gear,          # an indexing column with length equal to count of angler interviews with a 	1 or 2 depending on whether the interview was a boat or bank fisherman
  day_IntC=c_samp$Day,            # an indexing column with length equal to count angler interviews with the day of the fishery listed starting at 1 and going to the last day n.
  section_IntC = c_samp$Section.Num, #Section where angler was fishing (1 = Skagit, 2 = Sauk)
  c=c_samp[, catch.group.of.interest[1]], # total catch of [spp.of.interest] for an individual group #c_samp$SH_NOR_R          
  h=abs(c_samp$Total_Hours),        # number of hours an individual angler/group spent fishing
  
  #interview data total creeled effort and catch
  IntCreel = nrow(C_sample),
  day_Creel = C_sample$Day,
  gear_Creel = C_sample$Gear,
  section_Creel = C_sample$Section.Num,
  C_Creel = C_sample$Total_Catch,
  E_Creel = C_sample$Total_Hours,
  
  #interview data - angler expansions
  IntA=nrow(c_samp),              # total number of interviews across all surveys dates where angler expansion data (V_A, T_A, A_A) were collected 
  gear_IntA=c_samp$Gear,          # an indexing column with length equal to count of angler interviews with a 	1 or 2 depending on whether the interview was a boat or bank fisherman
  day_IntA=c_samp$Day,            # an indexing column with length equal to count angler interviews with the day of the fishery listed starting at 1 and going to the last day n.
  section_IntA = c_samp$Section.Num, #Section where angler was fishing (1 = Skagit, 2 = Sauk)
  V_A= a_samp$Vehicles,        # Total number of vehicles an individual angler group brought to the river
  T_A= a_samp$Trailers,        # Total number of Trailers an individual angler group brought to the river
  A_A= a_samp$NumAnglers       #Total number of anglers in each group interviewed
)
#standat$p_TE<-matrix(1,nrow=standat$G,ncol=standat$S)   # proportion of section covered by tie in counts: Option 1:usually 1.0, Option 2: miles of tie-in divided by total miles in section, Option 3:Best guess or something else.

```

# analysis

```{r analysis_params, results = "hide",echo=FALSE,warning=FALSE}
wd_models  <-"models"           # Location of model files 
wd_outputs <-"results"          # Location of saved output (summary figures/tables and model results)

# Denote whether you want to run a new model or load "saved" results from a previous model run
  model_source<-c("load_saved")  #enter either "run_new" or "load_saved"

# Assign a "Model_Run" number (if model_source == run_new, results will be saved to a new sub-folder; if model_source == load_saved, previous model results will be loaded)
  Model_Run<-4 #Enter numeric number (NOTE: be careful not to over-write previous models runs by entering a number that's already been used)

# Denote which creel model you want to run
  #creel_models[,1:3] #model summary table
  model_number<-c(4)
  
# Specify time period to stratify data by - day vs. week 
  model_period<-c("day") #enter "day" or "week"
  
# Specify parameter values for model priors
  value_cauchyDF_sigma_eps_C = 0.5#1 # the hyperhyper scale (degrees of freedom) parameter in the hyperprior distribution sigma_eps_C; default = 1  
  value_cauchyDF_sigma_eps_E = 0.5#1 # the hyperhyper scale (degrees of freedom) parameter in the hyperprior distribution sigma_eps_E; default = 1  
  value_cauchyDF_sigma_r_E = 0.5#1 # the hyperhyper scale (degrees of freedom) parameter in the hyperprior distribution sigma_r_E; default = 1  
  value_cauchyDF_sigma_r_C = 0.5#1 # the hyperhyper scale (degrees of freedom) parameter in the hyperprior distribution sigma_r_C; default = 1  
  value_normal_sigma_omega_C_0 = 1  #the SD hyperparameter in the prior distribution omega_C_0; normal sd (log-space); default = 1   
  value_normal_sigma_omega_E_0 =  3 # the SD hyperparameter in the prior distribution omega_E_0; normal sd (log-space);; default = 3  
  value_lognormal_sigma_b = 1 # the SD hyperparameter in the prior distribution b; default = 1  
  value_normal_sigma_B1 = 5 # the SD hyperparameter in the prior distribution B1; default = 5  
  value_normal_mu_mu_C = log(0.02) # the mean hyperparameter in the prior distribution mu_C; median (log-space); default = 0.02 (was originally  0.05) 
  value_normal_sigma_mu_C = 1.5 # the SD hyperparameter in the prior distribution mu_C; normal sd (log-space); default = 1.5 (was originally 5)
  value_normal_mu_mu_E = log(5) # the mean hyperparameter in the prior distribution mu_E; median effort (log-space); default = 15 
  value_normal_sigma_mu_E = 2  # the SD hyperparameter in the prior distribution mu_E; normal sd (log-space); default = 2 (was originally 5) 
  value_betashape_phi_E_scaled = 1 # the rate (alpha) and shape (beta) hyperparameters in phi_E_scaled; default = 1 (i.e., beta(1,1) which is uniform), alternative beta(2,2) 
  value_betashape_phi_C_scaled = 1 # the rate (alpha) and shape (beta) hyperparameters in phi_C_scaled; default = 1 (i.e., beta(1,1) which is uniform), alternative beta(2,2)
  value_cauchyDF_sigma_mu_C = 0.5#1      # the hyperhyper SD parameter in the hyperprior distribution sigma_mu_C
  value_cauchyDF_sigma_mu_E = 0.5#1      # the hyperhyper SD parameter in the hyperprior distribution sigma_mu_E

# Specific Stan model arguments
  n_chain<-4        # set the number of Markov chains. The default is 4.
  n_iter<-6000        # set the number of iterations for each chain (including warmup). The default is 2000.
  n_cores<-4         # set the number of cores to use when executing the chains in parallel. The defaults is 1. NOTE: Stan manual recommends setting the mc.cores option to be as many processors as the hardware and RAM allow (up to the number of chains).
  n_warmup<-3000   # set the length of warm-up (aka burn-in) iterations per chain.  The default is n_iter/2.  
  n_thin<-1          # set the thinning rate (aka, the period for saving samples). The default is 1, which is usually the recommended value.
  adapt_delta<-0.999  # set adapt delta, which is the target average proposal acceptance probability during Stan's adaptation period. Value between 0 and 1. The default is 0.8. Increasing it will force stan to take smaller steps (and thus run time will be longer). 
  max_treedepth<-10 # set the max tree depth; default is 8; NOTE: this sets the max depth of tree used by NUTS during each iteration; warnings about hitting the maximum treedepth is an efficiency concern
```

```{r analysis_subfolders, results = "hide",echo=FALSE,warning=FALSE}
# # Create sub-folders for output (if they don't already exist)
#     source(paste0(wd_source_files, "/Create_output_subfolder.R"), print.eval = TRUE)

# outputs
  ifelse(!dir.exists(wd_outputs), {dir.create(wd_outputs); "Output sub-folder created"},"Output sub-folder exists already")
    
# catch group  
  filepath_catchgroup<-paste(wd_outputs, catch.group.of.interest, sep="/")
  ifelse(!dir.exists(filepath_catchgroup), {dir.create(filepath_catchgroup); "Catch group sub-folder created"},"Catch group sub-folder exists already")

# model run  
  filepath_Run<-paste(filepath_catchgroup, paste0("Run_", Model_Run), sep="/")
  ifelse(!dir.exists(filepath_Run), {dir.create(filepath_Run); "Model run sub-folder created"},"Model run sub-folder exists already")

# model information and outputs    
  filepath_modeloutputs<-paste(filepath_Run, "model_outputs", sep="/")
  ifelse(!dir.exists(filepath_modeloutputs), {dir.create(filepath_modeloutputs); "Model info sub-folder created"},"Model info sub-folder exists already")

# summarized estimates  
  filepath_modelestimates<-paste(filepath_Run, "summarized_estimates", sep="/")
  ifelse(!dir.exists(filepath_modelestimates), {dir.create(filepath_modelestimates); "Model estimates sub-folder created"},"Model info sub-folder exists already")
```

```{r analysis_fina_prep, results = "hide",echo=FALSE,warning=FALSE}
# # Run source code to prepare data for model
#     source(paste0(wd_source_files, "/Prepare_Data_For_Model.R "))

#Last minute data fixes
  standat$V_A[standat$V_A>standat$A_A]<-standat$A_A[standat$V_A>standat$A_A] #Number of vehicles for a group can't be greater than number of anglers in the group
      
# Create two new "standat" variables depending on specified time (strata) period (e.g., day vs. week) 
  if(model_period=="day"){
    standat$P_n<-nrow(all.Dates) # For now: enter "D" for day or length(unique(week)) for week
    standat$period<-c(1:nrow(all.Dates)) #week          # For now: enter "1:D" for day or "week" for week
  }else{
    if(model_period=="week"){
      standat$P_n<-length(unique(all.Dates$Weeknum_Rel)) 
      standat$period<-all.Dates$Weeknum_Rel
    }
  }
  
# proportion tie-in expansion
  p_TI<-matrix(rep(1, standat$G * standat$S), nrow=standat$G, ncol=standat$S)
  standat$p_TI<-p_TI

# Add priors to "standat"
    standat$value_cauchyDF_sigma_eps_C<-value_cauchyDF_sigma_eps_C   
    standat$value_cauchyDF_sigma_eps_E<-value_cauchyDF_sigma_eps_E       
    standat$value_cauchyDF_sigma_r_E<-value_cauchyDF_sigma_r_E     
    standat$value_cauchyDF_sigma_r_C<-value_cauchyDF_sigma_r_C    
    standat$value_normal_sigma_omega_C_0<-value_normal_sigma_omega_C_0         
    standat$value_normal_sigma_omega_E_0<-value_normal_sigma_omega_E_0
    standat$value_normal_sigma_B1<-value_normal_sigma_B1      
    standat$value_lognormal_sigma_b<-value_lognormal_sigma_b                    
    standat$value_normal_mu_mu_C<-value_normal_mu_mu_C   
    standat$value_normal_sigma_mu_C<-value_normal_sigma_mu_C   
    standat$value_normal_mu_mu_E<-value_normal_mu_mu_E   
    standat$value_normal_sigma_mu_E<-value_normal_sigma_mu_E   
    standat$value_betashape_phi_E_scaled<-value_betashape_phi_E_scaled 
    standat$value_betashape_phi_C_scaled<-value_betashape_phi_C_scaled 
```

```{r analysis_load_or_run, results = "hide",echo=FALSE,warning=FALSE}
# # Run source code to generate creel estimates
#   source(paste0(wd_source_files, "/RunNew_or_LoadSaved_Creel_Model.R"))

# This file either (1) loads models results (via an .rds file), or (2) runs a new model
# Based on "model_number" chosen, create object of the "model.name"
    model.name<-as.character(creel_models$Model_Name[creel_models$Model_number == model_number])

# Load saved model results or...
if(model_source== "load_saved"){
  res_Stan <- readRDS(paste(filepath_modeloutputs, "results_res_Stan.rds", sep="/")) #Load RDS (stan model output)
#...Run new model
}else{

#Compile Model
  start.time<-Sys.time(); print(start.time)
  message(paste("Compiling stan model"))
  model.file.name<-as.character(creel_models$Model_file_name[creel_models$Model_number == model_number])
  model<-stan_model(paste(wd_models, model.file.name, sep="/")) 
  
#Run model in stan using NUTS/HMC
  message(paste("Running stan model - track progress via Viewer Pane"))
  res_Stan<-
    sampling(
      object=model
      ,data=standat
      , chains = n_chain
      , cores=n_cores
      , iter=n_iter
      , thin=n_thin
      , init="0"
      , warmup=n_warmup
      , include=T
      , control=list(adapt_delta=adapt_delta , max_treedepth=max_treedepth)
    )
  end.time<-Sys.time()
  model_duration<-print(paste("Elapsed Time = ",end.time-start.time,sep=""))
  print(elasped_time_by_chain<-get_elapsed_time(res_Stan))
  approx.model.runtime.minutes<-max(elasped_time_by_chain[,2])/60+max(elasped_time_by_chain[,1])/60
  
#Save Model warnings
  print(warnings())
  model_warnings<-warnings() #KB note: this doesn't appear to be working (not sure why)
}
#Extract posterior draws    
  res<-extract(res_Stan) 
 
```

```{r analysis_results, results = "hide",echo=FALSE,warning=FALSE}
# # Generate summaries of model inputs and outputs
#   if(model_source == "run_new"){  source(paste0(wd_source_files, "/Summarize_Model_Inputs_and_Outputs.R"))}

# This code generates summaries files regarding the inputs and outputs for a new model run 

# # Model warnings #KB note: this doesn't appear to be working (not sure why)
#     if(length(model_warnings)>0){saved_model_warning<-model_warnings}else{saved_model_warning<-c("there were no model warnings")}
#     writeLines(capture.output(saved_model_warning), paste0(filepath_modeloutputs, "/info_Model_warnings_", Sys.Date(), ".txt"))

# R session Info
  writeLines(capture.output(sessionInfo()), paste0(filepath_modeloutputs, "/info_sessionInfo_", Sys.Date(), ".txt"))
    
# Model summary
  mod.sum.elements<-
      c("Model Name"                   , as.character(creel_models$Model_Name[creel_models$Model_number == model_number])
      , "Model File"                   , model.file.name
      , "Model Period"                 , model_period
      , "Date Begin"                   , as.character(Date_Begin)
      , "Date End"                     , as.character(Date_End)
      , "Catch Group"                  , catch.group.of.interest
      , ""                             , c("")
      , "Chains"                       , n_chain
      , "Iterations"                   , n_iter
      , "Warmup"                       , n_warmup
      , "Thin Rate"                    , n_thin
      , "Adapt_Delta"                  , adapt_delta
      , "Max Tree Depth"               , max_treedepth
      , "Cores"                        , n_cores
      , ""                             , c("")
      , "value_cauchyDF_sigma_eps_C"   , value_cauchyDF_sigma_eps_C
      , "value_cauchyDF_sigma_eps_E"   , value_cauchyDF_sigma_eps_E
      , "value_cauchyDF_sigma_r_E"     , value_cauchyDF_sigma_r_E
      , "value_cauchyDF_sigma_r_C"     , value_cauchyDF_sigma_r_C
      , "value_normal_sigma_omega_C_0" , value_normal_sigma_omega_C_0 
      , "value_normal_sigma_omega_E_0" , value_normal_sigma_omega_E_0
      , "value_lognormal_sigma_b"      , value_lognormal_sigma_b
      , "value_normal_sigma_B1"        , value_normal_sigma_B1
      , "value_normal_mu_mu_C"         , round(value_normal_mu_mu_C, 3)
      , "value_normal_sigma_mu_C"      , round(value_normal_sigma_mu_C, 3) 
      , "value_normal_mu_mu_E"         , round(value_normal_mu_mu_E, 3) 
      , "value_normal_sigma_mu_E"      , round(value_normal_sigma_mu_E, 3)
      , "value_betashape_phi_E_scaled" , value_betashape_phi_E_scaled 
      , "value_betashape_phi_C_scaled" , value_betashape_phi_C_scaled 
  )
  mod.sum<-setNames(as.data.frame(matrix(mod.sum.elements, nrow=length(mod.sum.elements)/2, ncol=2, byrow=TRUE)), c("Argument", "Sim_Input"))
  writeLines(capture.output(mod.sum), paste0(filepath_modeloutputs, "/info_Model_setup_", Sys.Date(), ".txt"))

# Model run-time
  run.time<-c("Approx. Run Time", round(approx.model.runtime.minutes,2))
  mod_run.time<-setNames(as.data.frame(matrix(run.time, nrow=length(run.time)/2, ncol=2, byrow=TRUE)), c("Argument", "Time_minutes"))
  writeLines(capture.output(mod_run.time), paste0(filepath_modeloutputs, "/info_Model_Run_Time_", Sys.Date(), ".txt"))

# Save a file of the summary model results
  s.stan<-summary(res_Stan)
  write.csv(s.stan$summary, paste0(filepath_modeloutputs, "/results_res_Stan_summary_", Sys.Date(), ".csv"))
  
# Save "res_stan" object as .rds file   
  saveRDS(res_Stan, file=paste0(filepath_modeloutputs, "/results_res_Stan.rds"))

# Calculate loo-IC
  loo_IC<-loo(res$log_lik) # see: https://rdrr.io/cran/loo/man/loo.html and https://cran.r-project.org/web/packages/loo/loo.pdf 
  writeLines(capture.output(loo_IC), paste0(filepath_modeloutputs, "/results_model_looIC_", Sys.Date(), ".txt"))  

```


```{r summarize_results,message=FALSE, warning=FALSE,results = "hide",echo=FALSE}
### Summarize and Save Results
#Here we will summarize and save results of the in-season analysis of creel data to estimate CPUE, effort, and catch using the chunk of code with this heading in the RMD file.
#=============================================
#convergence diagnostics
  launch_diagnostics<-c("No") #Enter "Yes" to launch ShinyShin diagnostics
  if(launch_diagnostics=="Yes"){launch_shinystan(output$res_stan)} 

# generate plots and tables of creel estimates 
  source(file.path(wd_source_files,"Generate_Summaries_of_Creel_Estimates.R")) 
    
# KB note: update so table/plots of results are shown in PDF document
```

***

Page Last Updated: `r format(Sys.time(), '%m/%d/%Y')`.

***