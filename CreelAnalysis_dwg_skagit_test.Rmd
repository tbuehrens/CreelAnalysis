---
title: CreelAnalysis from DWG
params:
  proj_name: "Skagit"
  water_body: "Skagit River,Sauk River"
  date_start: "2021-02-01"   
  date_end: "2021-04-13"
  year_group: "2020-2021"
  catch_group: "Steelhead_W_R"
output:
  html_document:
    fig_caption: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
    code_folding: hide
---

# setup 

```{r setup, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(message = FALSE)

#library(DBI); library(odbc); library(dbplyr)
library(tidyverse)
library(rstan)

#base endpoints
dwg_base <- list(
  event = "https://data.wa.gov/resource/ui95-axtn.csv",
  effort = "https://data.wa.gov/resource/h9a6-g38s.csv",
  interview = "https://data.wa.gov/resource/rpax-ahqm.csv",
  catch = "https://data.wa.gov/resource/6y4e-8ftk.csv",
  gear = "https://data.wa.gov/resource/d2ks-afhz.csv"
)

dwg_sent <- list() #will hold full API strings built on above endpoints with params
creel <- list() #will hold resulting data objects

dates_holidays_2015_2030 <- read_lines("input_files/dates_holidays_2015_2030.txt") |> 
  as.Date(format="%Y-%m-%d")

lut <- map(
  list(
    river_loc = "input_files/lut_River.Locations_2019-01-07.csv",
    creel_models = "input_files/lut_Creel_Models_2021-01-20.csv",
    sections = "input_files/lut_water_body_location.csv",
    census_expansion = "input_files/lut_Proportional_Expansions_for_Tie_In_Sections_Skagit_Steelhead_2021_Example.csv" # EB Added proportional TI expansion to list of LUTs
  ),
  ~readr::read_csv(file.path(.x))
)


#tie_in_indicator: 0 is index/creel, 1 is tie-in/census

# Proportional tie in expansion table 
# value of 1 for p_TI means that the entire river section is surveyed during census counts?
# What is "Indirect_TI_Expan" and how it is calculated?

```

```{r build_mostly_static_inputs}
#### can/should continue to rethink the best way to get these in
#
## holiday dates
# write_lines(
#   2015:2030 %>%
#     map(~c(
#       timeDate::USNewYearsDay(.x), 
#       timeDate::USMLKingsBirthday(.x),
#       timeDate::USPresidentsDay(.x),
#       timeDate::USMemorialDay(.x), 
#       timeDate::USIndependenceDay(.x), 
#       timeDate::USLaborDay(.x),
#       timeDate::USVeteransDay(.x), 
#       timeDate::USThanksgivingDay(.x), 
#       timeDate::timeDate(as.character(timeDate::.nth.of.nday(.x, 11, 5, 4))), #Black Friday
#       timeDate::USChristmasDay(.x)
#     ) |> as.character()
#     ) |>  unlist(),
#   "input_files/dates_holidays_2015_2030.txt")
#
##starting point for section numbering, tie-in designation, prop expansion
##requires behind-VPN direct connection to DB
# lut_water_body_location <- full_join(
#   tbl(con_creel, dbplyr::in_schema("creel", "water_body_lut")) |> collect() |> select(water_body_id:water_body_desc),
#   tbl(con_creel, dbplyr::in_schema("creel", "location_lut")) |> collect() |> select(location_id:lower_rm),
#   by = c("water_body_id")
#   ) |>
#   collect()
# 
# write_csv(lut_water_body_location, "input_files/lut_water_body_location.csv")
```

# data

The data used are from the `r params$proj_name` project on the `r params$location` between `r params$date_start` and `r params$date_end`.

Further development may include interactive control parameter specification via the GUI: [https://bookdown.org/yihui/rmarkdown/params-knit.html#the-interactive-user-interface]

There is also the option to step through multiple pre-defined control parameters:
[https://bookdown.org/yihui/rmarkdown-cookbook/parameterized-reports.html]

## creel events

First, get the creel events of interest by building the Socrata API url string and grabbing the data

```{r get_event}
dwg_sent$event <- URLencode(
  paste0(dwg_base$event,
         "?$where=project_name in('", params$proj_name, "')",
         " AND water_body in('", str_replace(params$water_body, ",", "','"), "')",
         " AND event_date between '", params$date_start,
         "T00:00:00' and '", params$date_end,
         "T00:00:00'&$limit=100000"
  )
)

creel$event <- read_csv(dwg_sent$event) |> 
  dplyr::select(creel_event_id, water_body, event_date, tie_in_indicator)
```

## effort counts

Then, get the associated effort and interview data.

```{r get_effort}
dwg_sent$effort <- URLencode(
  paste0(dwg_base$effort,
         "?$where=creel_event_id in('",
         paste(creel$event$creel_event_id, collapse = "','"),
         "')&$limit=100000"
  )
)

creel$effort <- read_csv(dwg_sent$effort)
```

## interviews

```{r get_interview}
dwg_sent$interview <- URLencode(
  paste0(dwg_base$interview,
         "?$where=creel_event_id in('",
         paste(creel$event$creel_event_id, collapse = "','"),
         "')&$limit=100000"
  )
)

creel$interview <- read_csv(dwg_sent$interview) |> 
  rename(location = interview_location)
```

## catch data

And finally, the catch data associated with the interviews.

```{r get_catch}
dwg_sent$catch <- URLencode(
  paste0(dwg_base$catch,
         "?$where=creel_event_id in('",
         paste(creel$event$creel_event_id, collapse = "','"),
         "')&$limit=100000"
  )
)

# catch with redundant post-join columns removed 
# filtering catch to specific catch group parameter
creel$catch <- read_csv(dwg_sent$catch) |> 
  dplyr::select(interview_id, catch_id, species, run, life_stage, fin_mark, fate, fish_count) 

```

## sections

Aggregations of `location` units that may or may not correspond to `water_body`/`water_body_desc`...

```{r add_sections}
#x <- read_csv(URLencode(paste0(dwg_base$effort, "?$limit=1000000")))
#x |> count(project_name, water_body) |> print(n=50)

# #all the units in the database associated with the YAML water_body controls
# lut$sections |> 
#   #count(water_body_code, water_body_desc) |> print(n=100)
#   filter(water_body_desc %in% unlist(str_split(params$water_body, pattern = ","))) |> print(n=100)

# #all the units in the pulled data
# creel$event |> count(water_body)
# creel$effort |> count(water_body, location) |> print(n=50)
# creel$interview |> count(water_body, location) |> print(n=50)

section_codes <- tibble(
  water_body_desc = c("Sauk River", "Skagit River"),
  section = c(2,1)
)

sections <- full_join(
  distinct(creel$effort, water_body, location),
  distinct(creel$interview, water_body, location),
  by = c("water_body", "location")
) |> 
  left_join(
    lut$sections |> select(water_body_desc, location = location_code, upper_rm, lower_rm)
    ,
    by = c("location")
  ) |> 
  #count(water_body, water_body_desc)
  left_join(section_codes, by = "water_body_desc")

# #confirming that this codes locations to identical levels as prior csv approach 
# left_join(
#   sections,
#   read_csv("input_files/lut_02_Crosswalk_Table_for_Index_TieIn_Sections_2021-02-04_MatchedDB.csv"),
#   by = c("location" = "Section.Field")
#   ) |> 
#   select(-c(upper_rm:lower_rm, StreamName:Survey.Type)) |> 
#   mutate(d = section - Section.Summ) |> 
#   print(n=40)

#add to the primary objects
creel$effort <- creel$effort |> 
  select(-created_datetime, -modified_datetime) |> 
  left_join(
    sections |> select(water_body, location, water_body_desc, section),
    by = c("water_body", "location")
    )

creel$interview <- creel$interview |> 
  select(-created_datetime, -modified_datetime,
         -state_residence, -zip_code) |> 
  left_join(
    sections |> select(water_body, location, water_body_desc, section),
    by = c("water_body", "location")
  )

```

## dates

First, build an "expanded dates lattice" to which any/all observations are attached, ensuring complete cases.

Then join per-section closures by date.

```{r d_days}
#creel$event |> distinct(event_date)
(
  d_days <- tibble(
    event_date = seq(
      as.Date(params$date_start, "%Y-%m-%d"),
      as.Date(params$date_end, "%Y-%m-%d"),
      by = "day")
    ) |> 
    mutate(
      Day = weekdays(event_date),
      DayType = if_else(Day == "Saturday" | Day == "Sunday" | Day %in% dates_holidays_2015_2030, "Weekend", "Weekday"),
      DayType_num = if_else(str_detect(DayType, "end"),1,0),
      DayL = suncalc::getSunlightTimes(
        date = event_date,
        tz = "America/Los_Angeles",
        lat = lut$river_loc$Lat,
        lon = lut$river_loc$Long,
        keep=c("dawn", "dusk")
      ) |> 
        mutate(DayL = as.numeric(dusk - dawn)) |>
        pluck("DayL"),
      Week = as.numeric(format(event_date, "%V"))
    ) |> 
    rowid_to_column(var = "day_index") |> 
    #now add closures...
    left_join(
      bind_rows(
        tibble(section = "1,2", closure_begin = "2021-02-03", closure_end = "2021-02-05"),
        tibble(section = "1,2", closure_begin = "2021-02-10", closure_end = "2021-02-12"),
        tibble(section = "1,2", closure_begin = "2021-02-17", closure_end = "2021-02-19"),
        tibble(section = "1,2", closure_begin = "2021-02-24", closure_end = "2021-02-26"),
        tibble(section = "1,2", closure_begin = "2021-03-03", closure_end = "2021-03-05"),
        tibble(section = "1,2", closure_begin = "2021-03-10", closure_end = "2021-03-12"),
        tibble(section = "1,2", closure_begin = "2021-03-17", closure_end = "2021-03-19"),
        tibble(section = "1,2", closure_begin = "2021-03-24", closure_end = "2021-03-26"),
        tibble(section = "1,2", closure_begin = "2021-03-31", closure_end = "2021-04-02"),
        tibble(section = "1,2", closure_begin = "2021-04-07", closure_end = "2021-04-09"),
        ) |> 
        rowwise() |>
        mutate(closure_date = paste(seq.Date(as.Date(closure_begin), as.Date(closure_end), by = "day"), collapse = ",")) |> 
        separate_rows(closure_date, sep = ",") |> 
        select(event_date = closure_date, section) |> 
        mutate(
          event_date = as.Date(event_date),
          closure_code = FALSE # TB - The 1e-06 is needed to keep the model from crashing ¦log-normal parameters cant be 0
        ) |> 
        separate_rows(section, sep = ",") |> 
        pivot_wider(names_from = section, names_prefix = "open_section_", values_from = closure_code)
      ,
      by ="event_date"
      ) |> 
    mutate(across(starts_with("open_section_"), ~replace_na(., TRUE)))
)

```

# Some sort of pre-analysis QAQC?

```{r, eval=FALSE}
identical(nrow(event), nrow(distinct(event, creel_event_id))) #example, likely unnecessary given database structure

#effort$creel_event_id matches event$creel_event_id
distinct(effort, creel_event_id)
setdiff(event$creel_event_id, effort$creel_event_id)
setdiff(effort$creel_event_id, event$creel_event_id)

#but interview$creel_event_id is nested subset of event$creel_event_id
distinct(interview, creel_event_id)
setdiff(event$creel_event_id, interview$creel_event_id)
setdiff(interview$creel_event_id, event$creel_event_id)

#and catch$creel_event_id is nested subset of interview$creel_event_id
setdiff(interview$creel_event_id, catch$creel_event_id)
setdiff(catch$creel_event_id, interview$creel_event_id)

#so catch$creel_event_id is even smaller subset of event$creel_event_id
distinct(catch, creel_event_id)
setdiff(event$creel_event_id, catch$creel_event_id)
setdiff(catch$creel_event_id, event$creel_event_id)

#similarly catch$interview_id is a nested subset of interview$interview_id
setdiff(interview$interview_id, catch$interview_id)
setdiff(catch$interview_id, interview$interview_id)

#event |> head() |> gt::gt()

```



# wrangle for `stan()`

## data from `effort`

### census

```{r effort_census}

#DA: this is reasonable, and/but I'd like to think about whether/how to do this somewhat differently... 
#regardless, need to make a call on how to handle ties in min-time diff
paired_counts <- left_join(
  creel$effort |> filter(tie_in_indicator == 1) |> distinct(event_date, section, location, tie_in_indicator, effort_start_time, count_sequence)
  ,
  creel$effort |> filter(tie_in_indicator == 0) |> distinct(event_date, section, location, tie_in_indicator, effort_start_time, count_sequence)
  ,
  by = c("event_date", "section")
  ,
  suffix = c("_cen", "_ind")
  ) |> 
  group_by(event_date, section, location_cen) |> 
  slice_min(abs(effort_start_time_cen - effort_start_time_ind), n = 1) |> 
  count(event_date, section, location_cen)


# # EB based on original code effort_index and effort_census counts paired based on minimum difference between start time of counts, the count sequence from the effort_index count that best matches the effort_census count is then assigned to the effort_census count, so that they match based on event_date and count_sequence. 
# # identify effort_index counts that are closest temporal match to effort_census counts 
# indexcounts <- effort |> filter(tie_in_indicator == 0) |> 
#     left_join(lut_section, by = c("water_body", "location"))
# censuscounts <- effort |> filter(tie_in_indicator == 1) |> 
#     left_join(lut_section, by = c("water_body", "location"))
# 
# paired_counts <- inner_join(indexcounts, censuscounts, by = c("event_date", "section")) |>  # join back together to pair index and census counts by event_date and section
#   mutate(dateDiff = abs(effort_start_time.x - effort_start_time.y)) %>% # calculate time difference between start time of census and index surveys 
#   group_by(event_date, section, location.y) |> 
#   filter(dateDiff == min(dateDiff)) |> 
#   arrange(event_date) |> 
#   distinct(event_date, location.y, section, count_sequence.x, count_sequence.y, effort_start_time.x, effort_start_time.y, dateDiff) |>
#   select(event_date, location.y, section, count_sequence.x, count_sequence.y, effort_start_time.x, effort_start_time.y, dateDiff) |>  # preserve more columns here to look at the times that are matched 
#   select(event_date, location = location.y, section, count_sequence = count_sequence.x) # select what we'll need for a clean join to the census count data

## DA: NOT YET REWORKED BUT SHOULD BE QUICK
  
# census (tie in) effort counts 
effort_census <- effort |> 
  filter(
    tie_in_indicator == 1,
    !count_type == "Boats" # this will be a problem for creels utilizing boat counts
    ) |> 
  select(water_body, event_date, location, tie_in_indicator, count_type, count_quantity) |> # remove count_sequence (all census count_sequence == 1) - will add closest temporal match of count_sequence from the paired counts object 
  left_join(lut_section, by = c("water_body", "location")) |>
  left_join(d_days, by = "event_date") |>
  left_join(paired_counts) |> 
  drop_na(count_sequence) |>  # missing data stems from census count sections where we don't conduct effort count. Circle back to this..
  mutate(
    #angler_type = if_else(count_type == "Boat Anglers", "Boat", "Bank")
    #angler_type = str_sub(count_type, 1, 4),
    angler_type = word(count_type, 1), #stringr function to grab word instead of hard code length of first word due to the interchangeable use of "bank" and "shore" among datasets - can then make final encoding of boat / bank an if_else based on presence of "boat", which seems consistent to thus far
    section = dense_rank(section) # re-index sections for stan_dat list
  ) |>
  group_by(event_date, day_index, section, count_sequence, angler_type) |> # aggregate census counts to bank / boat categories
  summarize(count_quantity = sum(count_quantity)) |> 
  arrange(event_date, section, count_sequence) |> 
  ungroup()


```

### index

```{r effort_index}

# effort_index contains index counts of vehicles, trailers, anglers, and boats  

# DW - 23 effort count stops return a different Count Number / Count Sequence between the static original values and the current dynamic values returned by the database. Most of these are due to the first count not being done, and the raw csv staring at CountNum = 2. I retained the original value in the comment field of the effort_event table. 

# EB - added in 'no count' events so that Database derived count sequence matches that of the original data. Only found these on 2/8 for 14 rows (effort counts of cars or trailers)

effort_index <- effort |> 
  filter(tie_in_indicator == 0) |> 
  select(water_body, event_date, location, tie_in_indicator, count_sequence, count_type, count_quantity) |> 
  ##pending resolution of location name strings in database 
  # left_join(
  #   lut$lut_effort_xwalk |> 
  #     filter(YearGroup == params$year_group)
  #   , by = c("water_body", "location")) |> 
  left_join(lut_section, by = c("water_body", "location")) |>
  left_join(d_days, by = "event_date") |> 
  group_by(section, event_date, day_index, Week, count_sequence, count_type) |> 
  summarise(count_quantity_sum = sum(count_quantity), .groups = "drop") |> 
  pivot_wider(names_from = count_type, values_from = count_quantity_sum) |> 
  mutate(across(-c(section:count_sequence), ~replace_na(., 0)),
            section = dense_rank(section)) |> # re-index sections for stan_dat list 
  arrange(event_date, section, count_sequence)

table(effort_index$count_sequence)
```


## data from `interview`

Only "index" for interview, by definition.

```{r}

catch_cg <- catch |> # catch table filtered to catch group (cg) parameter 
  mutate(
    origin = case_when(
      fin_mark == "UM" ~ "W",
      fin_mark == "AD" ~ "H",
      fin_mark == "UNK" ~ "U"),
    origin = replace_na(origin, "U"),
    fate = str_sub(fate, 1,1), #same operation as strsub()
    fate = replace_na(fate, "U"),
    species_origin_fate = paste(species, origin, fate, sep = "_")) |> 
  filter(
    species_origin_fate == params$catch_group
  ) |> 
  select(
    interview_id, catch_id, species_origin_fate, fish_count
  )

interview_index <- interview |> 
  left_join(catch_cg, by = "interview_id") |>
  left_join(lut_section, by = "location") |> 
  left_join(d_days, by = "event_date") |> 
  mutate(
    angler_type = if_else(boat_used == "No", "Bank", "Boat"),
    trip_guided = case_when(
      trip_guided == "Guided" ~ "Yes",
      trip_guided == "Non-guided" ~ "No",
      trip_guided == "Unknown" ~ "Unknown",
      is.na(trip_guided) ~ "Unknown"),
    trip_status = replace_na(trip_status, "Unknown"),
    fishing_end_time = if_else(is.na(fishing_end_time), interview_time, fishing_end_time),
    angler_hours = round(as.numeric(fishing_end_time - fishing_start_time) / 3600, 5),
    angler_hours_total = angler_count * angler_hours,
    # origin = case_when(
    #   fin_mark == "UM" ~ "W",
    #   fin_mark == "AD" ~ "H",
    #   fin_mark == "UNK" ~ "U"),
    # origin = replace_na(origin, "U"),
    # fate = str_sub(fate, 1,1), #same operation as strsub()
    # fate = case_when(
    #   fate == "Kept" ~ "K",
    #   fate == "Released" ~ "R",
    #   fate == "Unknown" ~ "U"),
    # fate = replace_na(fate, "U"),
    # species_origin_fate = paste(species, origin, fate, sep = "_"),
    # fish_count = replace_na(fish_count, 0), # Could also determine which fields are appropriate to use replace_na on and perform an across( ) + replace_na to be more succinct
    vehicle_count = replace_na(vehicle_count, 0),
    trailer_count = replace_na(trailer_count, 0),
    fish_count = replace_na(fish_count, 0), # coerce NA's from table join to 0's
    species_origin_fate = params$catch_group # make catch group field uniform 
  ) |>
  arrange(event_date, interview_number) |> 
  filter(!angler_hours <= 0.5) |>  # EB remove short trips
  mutate(section = dense_rank(section)) # re-index sections for stan_dat list


# EB: need to make current code covers these filters from original code 
  #Drop "final_group.dat" data if "Start.Time" is <NA>, "Interview.Time" is <NA> for groups with Trip.Status == I, or "End.Time" is <NA> for groups with Trip.Status == C

  #Drop rows if hours fished by a Group is Negative 

# summarizing catch by unique interview (interview_id) in this chunk 
interview_index_TF <- interview_index |> 
    group_by(interview_id, event_date, interview_time, day_index, section, angler_count, angler_type, angler_hours, angler_hours_total,vehicle_count, trailer_count) |>
  summarize(fish_count = sum(fish_count)) |> 
  ungroup() |>
  arrange(event_date, section, interview_time)

# EB: equivalent to gear.xwalk table in original code. Keep going back on forth on what terminology makes most sense between "gear_type" or "angler_type"
lut_angler_type <- interview_index_TF |>
  select(angler_type) |> 
  distinct() |> 
  rowid_to_column(var = "angler_type_num")
  

```

### Daily interview data from `interview`
```{r}

#total hours creel and total catch sample a per day/angler type/section to compare with effort

interview_index_TF_DailySum <- interview_index_TF |>
  group_by(event_date,day_index,section,angler_type) |>
  summarise(
        angler_hours_total_dailysum = sum(angler_hours_total),
  catch_dailysum = sum(fish_count)) |> 
  ungroup() |> 
  filter(angler_hours_total_dailysum > 0)

```

### Interview data filtered to only events with vehicle or trailer counts from `interview`
```{r}

# interview data - angler expansion  KB: interview_sub<-df_interview %>% filter (V_A, T_A, and A_A != NA) ## these variables are used for indirect angler count expansions
# TF = target fish group
# AE = angler expansion 
interview_index_TF_AE <- interview_index_TF |>  
	filter(across(c(vehicle_count, trailer_count), ~ !is.na(.)))

```

## census count proportional expansion table
```{r}
# pull subset of sections actually surveyed from effort_index table
# use this to grab corresponding proportional tie expansion values per section

sections_sub <- effort_index |> 
  select(section) |> 
  mutate(section = dense_rank(section)) |>  # re-index sections for stan_dat list
  distinct()

census_expansion_sub <- left_join(sections_sub, lut$lut_census_expansion, by = "section")

 census_expansion_sub_bank <- census_expansion_sub |> 
	  filter(angler_type == "Bank") |> 
	  select(
	    section,
	    p_TI) |>
	  pivot_wider(names_from = section, values_from = p_TI)

  census_expansion_sub_boat <- census_expansion_sub |> 
	  filter(angler_type == "Boat") |> 
	  select(
	    section,
	    p_TI) |>
	  pivot_wider(names_from = section, values_from = p_TI)
 
census_expansion_sub <- bind_rows(census_expansion_sub_bank, census_expansion_sub_boat) # row 1 = bank anglers; row 2 = boat anglers 
```
## stan_dat

```{r}


stan_dat <- list(
  # Day attributes
  
  # int; number of fishing days; KB: pull from "master_date" DF
	  D = nrow(d_days) 

	  # int; final number of unique gear/angler types  KB: pull from subsetted angler x-walk?
	, G = as.integer(length(unique(interview_index$angler_type))) 
	
	# int; final number of river sections    KB: pull from subsetted section x-walk?
	, S =  as.integer(length(unique(effort_index$section)))    
	  
	# int; max number of angler effort counts within a sample day KB: max(z$effort$count_sequence)
	, H = max(effort_index$count_sequence)            
	  
	  # int; number of days/periods KB: right now, model is set up to running as daily (P_n = D) or weekly (P_n = effectively D/7)
	, P_n = nrow(d_days) 

	# vec; index denoting daytype (day-type is offset in model)   KB: pull from "master_date" DF (1= weekend, 0 = weekday)
  , w = d_days$DayType_num

	# int; index denoting fishing day/period      KB: calculate as 1:P_n (not sure why this isn't a vector)
	, period = 1:nrow(d_days)         
	  
	  # vec; daylength (model offset; assumption)       KB: pull from "master_date" DF
	, L = d_days$DayL
	
	# mat; index denoting fishery status                          KB: user defined (1=open, 0 = closed; by period/date and section; 0 defined as 1E-6 for model)
	, O = d_days |> 
	  select(starts_with("section_")) |>
	  as.matrix()
	  
	  
  # Vehicle index effort counts KB: index_vehicles="summ_effort_counts" %>% filter(gear == vehicles, survey == Creel) 
	  
  # int; total number of individual vehicle index effort counts KB: index_vehicles %>% nrow() 
	, V_n = effort_index |> 
	    tally() |> 
	    as.integer()

		  # int; observed # of vehicles  KB: index_vehicles %>% pull(Count)
	, V_I = effort_index |>
	  pull(`Vehicle Only`)
	
	  # int; index for day/period KB: index_vehicles %>% pull(day/period)
	, day_V = effort_index |>
	  pull(day_index)
	
	, gear_V = effort_index |> 
    mutate(gear_V = 1) |> 
    pull(gear_V)
  
	  # int; index for section     KB: index_vehicles %>% pull(section)
	, section_V = effort_index |>
	  pull(section) 
	  
	  # int; index for count_num    KB: index_vehicles %>% pull(count_num)
	, countnum_V = effort_index |>    
	  pull(count_sequence)

	  

	
	
	# Trailer index effort counts  KB: index_trailers="summ_effort_counts" %>% filter(gear == trailers, survey == Creel)
	
	# int; total number of boat trailer index effort counts       KB: index_trailers %>% nrow() 
	, T_n = effort_index |> 
	    tally() |> 
	    as.integer()
	
	  # int; observed # of boat trailers      KB: index_trailers %>% pull(Count)
	, T_I = effort_index |>
	  pull(`Trailers Only`)
	
  # int; index for day/period         KB: index_trailers %>% pull(day/period)
	, day_T = effort_index |>
	  pull(day_index) 
	
	, gear_T = effort_index |> 
    mutate(gear_T = 2) |> 
    pull(gear_T)
	
  # int; index for section          KB: index_trailers %>% pull(section)
	, section_T = effort_index |>
	   pull(section)

  # int; index for count_num        KB: index_trailers %>% pull(count_num)
	, countnum_T = effort_index |>    
	  pull(count_sequence)


	       
	
  # Angler index effort counts  KB: index_anglers="summ_effort_counts"     %>% filter(gear == angler_groups, survey == Creel); angler_groups=user defined?
	# int; total number of angler index effort counts            KB: index_anglers %>% nrow() 
	
	# Need to circle back on how to deal with this when applicable, set all values to 0 as a placeholder, matching example standat_2021-05-28.txt
	, A_n = 0
	  
	  # effort_index |>
	  # tally() |> 
	  # as.integer()
	
		  # int; observed # of anglers                                 KB: index_anglers %>% pull(Count)
	, A_I = numeric(0) #EB placeholder
	
	
	# int; index for day/period                                  KB: index_anglers %>% pull(day/period) 
	, day_A = numeric(0)
	  
	  # effort_index |>
	  # pull(day_index)
	
	 # int; index denoting "gear/angler type"                     KB: index_anglers %>% pull(gear/angler) 
	, gear_A = numeric(0) #EB placeholder 
	
	
	# int; index for section                                     KB: index_anglers %>% pull(section)
	, section_A = numeric(0)
	  
	  # effort_index |>
	  # pull(section)
	  
	  # int; index for count_num                                   KB: index_anglers %>% pull(count_num)
	, countnum_A = numeric(0)
	  
	  # effort_index |> 
	  # pull(count_sequence)

  
  
  # Census (tie-in) effort counts      KB: TI_anglers="summ_effort_counts" %>% filter(survey == "Tie In");

	 # int; total number of angler tie-in effort counts       KB: TI_anglers %>% nrow() 
	, E_n = effort_census |> 
	    tally() |> 
	    as.integer()          
  
   # int; index denoting day/period                         KB: TI_anglers %>% pull(day/period) 
	, day_E = effort_census |> 
    pull(day_index)
	
  # int; index denoting "gear/angler type"                  KB: TI_anglers %>% pull(gear/angler)
	# EB: depending on the variety of angler types (eg multiple types of boats) this may not be flexible enough for other datasets 
  , gear_E = effort_census |> 
	  mutate(angler_type = if_else(angler_type == "Boat", 2, 1)) |> 
    pull(angler_type)        
	
  # int; index for section                                  KB: TI_anglers %>% pull(section)
  , section_E = effort_census |> 
    pull(section)
	
	# EB count_sequence here needs to match that of the closest effort_index count (if my understanding is correct)
  # int; index for count_num                                KB: TI_anglers %>% pull(count_num)
  , countnum_E = effort_census |> 
    pull(count_sequence)   
	
  # int; observed # of anglers                              KB: TI_anglers %>% pull(Count)
  , E_s = effort_census |> 
    pull(count_quantity)           
  

	# Proportion tie-in expansion mat; proportion of section covered by tie in counts KB: user defined (see "Proportional_Expansions_for_Tie_In_Sections_Kalama_Example"; need to format)

	, p_TI = census_expansion_sub |> 
	  as.matrix()
	    
	 # EB: Need to decide at what step in the workflow to filter catch data to species_origin_fate (or other strings including life stage / run). Right now TF object filters interview and catch data to the fish group of interest 
	  
	
	
	# interview data - CPUE 
	  
	# int; total number of angler interviews with c & h data   KB: use "summ_interview" DF
	, IntC = interview_index_TF |> 
	    tally() |> 
	    as.integer()  
	
	# int; index denoting day/period                             KB: summ_interview %>% pull(day/period)
	, day_IntC = interview_index_TF |> 
    pull(day_index)
	
	# int; index denoting "gear/angler type"                     KB: summ_interview %>% pull(gear/angler) 
	
	# EB: depending on the variety of angler types (eg multiple types of boats) this may not be flexible enough for other datasets 
	, gear_IntC = interview_index_TF |>
	  mutate(angler_type = if_else(angler_type == "Boat", 2, 1)) |> 
	  pull(angler_type)
	  
	  # int; index for section                                     KB: summ_interview %>% pull(section)  
	, section_IntC  = interview_index_TF |> 
	  pull(section) 
	
	  # int; total catch                                           KB: summ_interview %>% pull(Qty/Count)  
	, c = interview_index_TF |> 
	  pull(fish_count)          
	  
	# vec; total hours fished   KB: summ_interview %>% pull(Total_Hours) 
  # EB Does Total_Hours refer to fishing time (angler hours) multiplied by the total number of anglers in an                    interviewed party (group_angler_hours)? 
	, h =  interview_index_TF |> 
	  pull(angler_hours_total)         
	  

	 
	# interview data - Total Effort & Catch Creeled              KB: use "summ_interviewed_effort"
	
	# int; total interviews by sub-groups	                       KB: summ_interviewed_effort %>% nrow()
	, IntCreel = interview_index_TF_DailySum |>
	    tally() |> 
	    as.integer()        
	
	# int; index denoting day/period                             KB: summ_interviewed_effort %>% pull(day/period)
	, day_Creel = interview_index_TF_DailySum |>
	    pull(day_index)   
	
	# int; index denoting "gear/angler type"                     KB: summ_interviewed_effort %>% pull(gear/angler)	
		# EB: depending on the variety of angler types (eg multiple types of boats) this may not be flexible enough for other datasets 
	, gear_Creel = interview_index_TF_DailySum |>
	  mutate(angler_type = if_else(angler_type == "Boat", 2, 1)) |> 
	    pull(angler_type)    
	  
	  # int; index for section                                     KB: summ_interviewed_effort %>% pull(section)
	, section_Creel = interview_index_TF_DailySum |>
	    pull(section) 		
	  
	  # int; total catch                                           KB: summ_interviewed_effort %>% pull(Qty/Count)
	, C_Creel = interview_index_TF_DailySum |>
	    pull(catch_dailysum)       
	  
	   # vec; total hours fished                                    KB: summ_interviewed_effort %>% pull(Total_Hours)
	, E_Creel = interview_index_TF_DailySum |>
	    pull(angler_hours_total_dailysum)       
  
	  
	# interview data - angler expansion  KB: interview_sub=df_interview %>% filter (V_A, T_A, and A_A != NA) ## these variables are used for indirect angler count expansions
	
	# int; total number of angler interviews where V_A, T_A, A_A were collected KB: interview_sub %>% nrow()
	, IntA = interview_index_TF_AE |> 
	  tally() |> 
	  as.integer()
	
	  # int; index denoting day/period                             KB: interview_sub %>% pull(day/period)                     	
	, day_IntA = interview_index_TF_AE |> 
	  pull(day_index)
	  
	  # int; index denoting day/period                             KB: interview_sub %>% pull(gear/angler)  
		# EB: depending on the variety of angler types (eg multiple types of boats) this may not be flexible enough for other datasets 
	, gear_IntA = interview_index_TF_AE |>
	  mutate(angler_type = if_else(angler_type == "Boat", 2, 1)) |> 
	  pull(angler_type)     
	  
	  # int; index denoting day/period                             KB: interview_sub %>% pull(section)   
	, section_IntA = interview_index_TF_AE |> 
	  pull(section)  
	  
	  # int; total number of vehicles an angler group brought      KB: interview_sub %>% pull(vehicle_count) 
	, V_A = interview_index_TF_AE |> 
	  pull(vehicle_count)            
	  
	  # int; total number of trailers an angler group brought      KB: interview_sub %>% pull(trailer_count)  
	  
	, T_A = interview_index_TF_AE |> 
	  pull(trailer_count) 
	  
	  # int; total number of anglers in the groups interviewed     KB: interview_sub %>% pull(angler_count) ## NOTE: this was used above to calculated "Total_Hours" (fished)
	  
	, A_A = interview_index_TF_AE |> 
	  pull(angler_count)           
	  
)

capture.output(stan_dat, file = "results/stan_dat_2021-12-03.txt")


```


# fit

```{r}
creel_models <- lut$lut_creel_models
model_period <- "day"
wd_models  <-"models"           # Location of model files 
wd_outputs <-"results"          # Location of saved output (summary figures/tables and model results)

# Denote whether you want to run a new model or load "saved" results from a previous model run
  model_source<-c("run_new")  #enter either "run_new" or "load_saved"

# Assign a "Model_Run" number (if model_source == run_new, results will be saved to a new sub-folder; if model_source == load_saved, previous model results will be loaded)
  Model_Run<-4 #Enter numeric number (NOTE: be careful not to over-write previous models runs by entering a number that's already been used)

# Denote which creel model you want to run
  #creel_models[,1:3] #model summary table
  model_number<-c(4)
  
# Specify time period to stratify data by - day vs. week 
  model_period<-c("day") #enter "day" or "week"
  
# Specify parameter values for model priors
  value_cauchyDF_sigma_eps_C = 0.5#1 # the hyperhyper scale (degrees of freedom) parameter in the hyperprior distribution sigma_eps_C; default = 1  
  value_cauchyDF_sigma_eps_E = 0.5#1 # the hyperhyper scale (degrees of freedom) parameter in the hyperprior distribution sigma_eps_E; default = 1  
  value_cauchyDF_sigma_r_E = 0.5#1 # the hyperhyper scale (degrees of freedom) parameter in the hyperprior distribution sigma_r_E; default = 1  
  value_cauchyDF_sigma_r_C = 0.5 #1 # the hyperhyper scale (degrees of freedom) parameter in the hyperprior distribution sigma_r_C; default = 1 
  

  
  
  value_normal_sigma_omega_C_0 = 1  #the SD hyperparameter in the prior distribution omega_C_0; normal sd (log-space); default = 1   
  value_normal_sigma_omega_E_0 =  3 # the SD hyperparameter in the prior distribution omega_E_0; normal sd (log-space);; default = 3  
  value_lognormal_sigma_b = 1 # the SD hyperparameter in the prior distribution b; default = 1  
  value_normal_sigma_B1 = 5 # the SD hyperparameter in the prior distribution B1; default = 5  
  value_normal_mu_mu_C = log(0.02) # the mean hyperparameter in the prior distribution mu_C; median (log-space); default = 0.02 (was originally  0.05) 
  value_normal_sigma_mu_C = 1.5 # the SD hyperparameter in the prior distribution mu_C; normal sd (log-space); default = 1.5 (was originally 5)
  value_normal_mu_mu_E = log(5) # the mean hyperparameter in the prior distribution mu_E; median effort (log-space); default = 15 
  value_normal_sigma_mu_E = 2  # the SD hyperparameter in the prior distribution mu_E; normal sd (log-space); default = 2 (was originally 5) 
  value_betashape_phi_E_scaled = 1 # the rate (alpha) and shape (beta) hyperparameters in phi_E_scaled; default = 1 (i.e., beta(1,1) which is uniform), alternative beta(2,2) 
  value_betashape_phi_C_scaled = 1 # the rate (alpha) and shape (beta) hyperparameters in phi_C_scaled; default = 1 (i.e., beta(1,1) which is uniform), alternative beta(2,2)
  value_cauchyDF_sigma_mu_C = 0.5#1      # the hyperhyper SD parameter in the hyperprior distribution sigma_mu_C
  value_cauchyDF_sigma_mu_E = 0.5#1      # the hyperhyper SD parameter in the hyperprior distribution sigma_mu_E

# Specific Stan model arguments
  n_chain<-4        # set the number of Markov chains. The default is 4.
  n_iter<-6000        # set the number of iterations for each chain (including warmup). The default is 2000.
  n_cores<-4         # set the number of cores to use when executing the chains in parallel. The defaults is 1. NOTE: Stan manual recommends setting the mc.cores option to be as many processors as the hardware and RAM allow (up to the number of chains).
  n_warmup<-3000   # set the length of warm-up (aka burn-in) iterations per chain.  The default is n_iter/2.  
  n_thin<-1          # set the thinning rate (aka, the period for saving samples). The default is 1, which is usually the recommended value.
  adapt_delta<-0.999  # set adapt delta, which is the target average proposal acceptance probability during Stan's adaptation period. Value between 0 and 1. The default is 0.8. Increasing it will force stan to take smaller steps (and thus run time will be longer). 
  max_treedepth<-10 # set the max tree depth; default is 8; NOTE: this sets the max depth of tree used by NUTS during each iteration; warnings about hitting the maximum treedepth is an efficiency concern
  
```  
  
```{r}
  #---------------------------------------------------------------------------------------- -    
# This file creates sub-folders to save model information, outputs, and summarized results
#---------------------------------------------------------------------------------------- - 
# outputs
  ifelse(!dir.exists(wd_outputs), {dir.create(wd_outputs); "Output sub-folder created"},"Output sub-folder exists already")
    
# catch group  
  filepath_catchgroup<-paste(wd_outputs, params$catch_group, sep="/")
  ifelse(!dir.exists(filepath_catchgroup), {dir.create(filepath_catchgroup); "Catch group sub-folder created"},"Catch group sub-folder exists already")

# model run  
  filepath_Run<-paste(filepath_catchgroup, paste0("Run_", Model_Run), sep="/")
  ifelse(!dir.exists(filepath_Run), {dir.create(filepath_Run); "Model run sub-folder created"},"Model run sub-folder exists already")

# model information and outputs    
  filepath_modeloutputs<-paste(filepath_Run, "model_outputs", sep="/")
  ifelse(!dir.exists(filepath_modeloutputs), {dir.create(filepath_modeloutputs); "Model info sub-folder created"},"Model info sub-folder exists already")

# summarized estimates  
  filepath_modelestimates<-paste(filepath_Run, "summarized_estimates", sep="/")
  ifelse(!dir.exists(filepath_modelestimates), {dir.create(filepath_modelestimates); "Model estimates sub-folder created"},"Model info sub-folder exists already")

  
```  
  

```{r}  
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# This file prepares final dataset for creel (stan) model
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Last minute data fixes
  stan_dat$V_A[stan_dat$V_A>stan_dat$A_A]<-stan_dat$A_A[stan_dat$V_A>stan_dat$A_A] #Number of vehicles for a group can't be greater than number of anglers in the group
  
# Create two new "stan_dat" variables depending on specified time (strata) period (e.g., day vs. week) 
  if(model_period=="day"){
    stan_dat$P_n<-nrow(d_days) # For now: enter "D" for day or length(unique(week)) for week
    stan_dat$period<-c(1:nrow(d_days)) #week          # For now: enter "1:D" for day or "week" for week
  }else{
    if(model_period=="week"){
      stan_dat$P_n<-length(unique(d_days$Week)) 
      stan_dat$period<-d_days$Week
    }
  }
  
# proportion tie-in expansion
  # p_TI<-matrix(rep(1, stan_dat$G * stan_dat$S), nrow=stan_dat$G, ncol=stan_dat$S)
  # stan_dat$p_TI<-p_TI

# Add priors to "stan_dat"
    stan_dat$value_cauchyDF_sigma_eps_C<-value_cauchyDF_sigma_eps_C   
    stan_dat$value_cauchyDF_sigma_eps_E<-value_cauchyDF_sigma_eps_E       
    stan_dat$value_cauchyDF_sigma_r_E<-value_cauchyDF_sigma_r_E     
    stan_dat$value_cauchyDF_sigma_r_C<-value_cauchyDF_sigma_r_C    
    
    stan_dat$value_cauchyDF_sigma_mu_C <- value_cauchyDF_sigma_mu_C
    stan_dat$value_cauchyDF_sigma_mu_E <- value_cauchyDF_sigma_mu_E
    
    stan_dat$value_normal_sigma_omega_C_0<-value_normal_sigma_omega_C_0         
    stan_dat$value_normal_sigma_omega_E_0<-value_normal_sigma_omega_E_0
    stan_dat$value_normal_sigma_B1<-value_normal_sigma_B1      
    stan_dat$value_lognormal_sigma_b<-value_lognormal_sigma_b                    
    stan_dat$value_normal_mu_mu_C<-value_normal_mu_mu_C   
    stan_dat$value_normal_sigma_mu_C<-value_normal_sigma_mu_C   
    stan_dat$value_normal_mu_mu_E<-value_normal_mu_mu_E   
    stan_dat$value_normal_sigma_mu_E<-value_normal_sigma_mu_E   
    stan_dat$value_betashape_phi_E_scaled<-value_betashape_phi_E_scaled 
    stan_dat$value_betashape_phi_C_scaled<-value_betashape_phi_C_scaled 
    
```

```{r}
  
# Based on "model_number" chosen, create object of the "model.name"
    model.name<-as.character(creel_models$Model_Name[creel_models$Model_number == model_number])

# Load saved model results or...
if(model_source== "load_saved"){
  res_Stan <- readRDS(paste(filepath_modeloutputs, "results_res_Stan.rds", sep="/")) #Load RDS (stan model output)
#...Run new model
}else{

#Compile Model
  start.time<-Sys.time(); print(start.time)
  message(paste("Compiling stan model"))
  model.file.name<-as.character(creel_models$Model_file_name[creel_models$Model_number == model_number])
  model<-stan_model(paste(wd_models, model.file.name, sep="/")) 


#Run model in stan using NUTS/HMC
  message(paste("Running stan model - track progress via Viewer Pane"))
  res_Stan<-
    sampling(
      object=model
      ,data=stan_dat
      , chains = n_chain
      , cores=n_cores
      , iter=n_iter
      , thin=n_thin
      , init="0"
      , warmup=n_warmup
      , include=T
      , control=list(adapt_delta=adapt_delta , max_treedepth=max_treedepth)
    )
  end.time<-Sys.time()
  model_duration<-print(paste("Elapsed Time = ",end.time-start.time,sep=""))
  print(elasped_time_by_chain<-get_elapsed_time(res_Stan))
  approx.model.runtime.minutes<-max(elasped_time_by_chain[,2])/60+max(elasped_time_by_chain[,1])/60
  
#Save Model warnings
  print(warnings())
  model_warnings<-warnings() #KB note: this doesn't appear to be working (not sure why)
}
#Extract posterior draws    
  res<-extract(res_Stan) 

```


```{r}
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# This code generates summaries files regarding the inputs and outputs for a new model run 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# # Model warnings #KB note: this doesn't appear to be working (not sure why)
#     if(length(model_warnings)>0){saved_model_warning<-model_warnings}else{saved_model_warning<-c("there were no model warnings")}
#     writeLines(capture.output(saved_model_warning), paste0(filepath_modeloutputs, "/info_Model_warnings_", Sys.Date(), ".txt"))

# R session Info
  writeLines(capture.output(sessionInfo()), paste0(filepath_modeloutputs, "/info_sessionInfo_", Sys.Date(), ".txt"))
    
# Model summary
  mod.sum.elements<-
      c("Model Name"                   , as.character(creel_models$Model_Name[creel_models$Model_number == model_number])
      , "Model File"                   , model.file.name
      , "Model Period"                 , model_period
      , "Date Begin"                   , as.character(Date_Begin)
      , "Date End"                     , as.character(Date_End)
      , "Catch Group"                  , catch.group.of.interest
      , ""                             , c("")
      , "Chains"                       , n_chain
      , "Iterations"                   , n_iter
      , "Warmup"                       , n_warmup
      , "Thin Rate"                    , n_thin
      , "Adapt_Delta"                  , adapt_delta
      , "Max Tree Depth"               , max_treedepth
      , "Cores"                        , n_cores
      , ""                             , c("")
      , "value_cauchyDF_sigma_eps_C"   , value_cauchyDF_sigma_eps_C
      , "value_cauchyDF_sigma_eps_E"   , value_cauchyDF_sigma_eps_E
      , "value_cauchyDF_sigma_r_E"     , value_cauchyDF_sigma_r_E
      , "value_cauchyDF_sigma_r_C"     , value_cauchyDF_sigma_r_C
      , "value_normal_sigma_omega_C_0" , value_normal_sigma_omega_C_0 
      , "value_normal_sigma_omega_E_0" , value_normal_sigma_omega_E_0
      , "value_lognormal_sigma_b"      , value_lognormal_sigma_b
      , "value_normal_sigma_B1"        , value_normal_sigma_B1
      , "value_normal_mu_mu_C"         , round(value_normal_mu_mu_C, 3)
      , "value_normal_sigma_mu_C"      , round(value_normal_sigma_mu_C, 3) 
      , "value_normal_mu_mu_E"         , round(value_normal_mu_mu_E, 3) 
      , "value_normal_sigma_mu_E"      , round(value_normal_sigma_mu_E, 3)
      , "value_betashape_phi_E_scaled" , value_betashape_phi_E_scaled 
      , "value_betashape_phi_C_scaled" , value_betashape_phi_C_scaled 
  )
  mod.sum<-setNames(as.data.frame(matrix(mod.sum.elements, nrow=length(mod.sum.elements)/2, ncol=2, byrow=TRUE)), c("Argument", "Sim_Input"))
  writeLines(capture.output(mod.sum), paste0(filepath_modeloutputs, "/info_Model_setup_", Sys.Date(), ".txt"))

# Model run-time
  run.time<-c("Approx. Run Time", round(approx.model.runtime.minutes,2))
  mod_run.time<-setNames(as.data.frame(matrix(run.time, nrow=length(run.time)/2, ncol=2, byrow=TRUE)), c("Argument", "Time_minutes"))
  writeLines(capture.output(mod_run.time), paste0(filepath_modeloutputs, "/info_Model_Run_Time_", Sys.Date(), ".txt"))

# Save a file of the summary model results
  s.stan<-summary(res_Stan)
  write.csv(s.stan$summary, paste0(filepath_modeloutputs, "/results_res_Stan_summary_", Sys.Date(), ".csv"))
  
# Save "res_stan" object as .rds file   
  saveRDS(res_Stan, file=paste0(filepath_modeloutputs, "/results_res_Stan.rds"))

# Calculate loo-IC
  loo_IC<-loo(res$log_lik) # see: https://rdrr.io/cran/loo/man/loo.html and https://cran.r-project.org/web/packages/loo/loo.pdf 
  writeLines(capture.output(loo_IC), paste0(filepath_modeloutputs, "/results_model_looIC_", Sys.Date(), ".txt"))  

```


```{r summarize_results,message=FALSE, warning=FALSE,results = "hide",echo=FALSE}
### Summarize and Save Results
#Here we will summarize and save results of the in-season analysis of creel data to estimate CPUE, effort, and catch using the chunk of code with this heading in the RMD file.
#=============================================
#convergence diagnostics
  launch_diagnostics<-c("No") #Enter "Yes" to launch ShinyShin diagnostics
  if(launch_diagnostics=="Yes"){launch_shinystan(output$res_stan)} 

# generate plots and tables of creel estimates 
  source(file.path(wd_source_files,"Generate_Summaries_of_Creel_Estimates.R")) 
    
# KB note: update so table/plots of results are shown in PDF document
```


```{r}

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# This file summarizes creel model estimates by creating summary plots and tables
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#--------------------------------------------------------------------------------------------------------------------- -
# SUMMARIZE SEASON TOTAL EFFORT AND CATCH                                                                ----
#--------------------------------------------------------------------------------------------------------------------- - 
  #calculate season total effort
    #Total_season_catch<-c(setNames(mean(res$C_sum), "Mean"), quantile(res$C_sum,c(0.025,0.25,0.5,0.75,0.975)), setNames(sd(res$C_sum)/mean(res$C_sum), "CV"))
    Total_season_catch<-c(setNames(round(mean(c(apply(res$C_sum, c(1), sum))),0), "Mean"), round(quantile(apply(res$C_sum, c(1), sum),c(0.025,0.25,0.5,0.75,0.975)),0), setNames(round(sd(c(apply(res$C_sum, c(1), sum)))/mean(c(apply(res$C_sum, c(1), sum))),3), "CV"))
  
  #calculate season total catch
    Total_season_effort<-c(setNames(round(mean(res$E_sum),0), "Mean"), round(quantile(res$E_sum,c(0.025,0.25,0.5,0.75,0.975)),0), setNames(round(sd(res$E_sum)/mean(res$E_sum),3), "CV"))
    
  #Export .csv of total catch and effort
    catch.effort.totals<-rbind(Total_season_catch, Total_season_effort)
    write.csv(catch.effort.totals, paste(filepath_modelestimates, paste("Summary_Total_Catch_and_Effort", catch.group.of.interest, paste0("Run_", Model_Run), ".csv", sep="_"), sep="/"), row.names = T)

#--------------------------------------------------------------------------------------------------------------------- -
# SUMMARIZE TOTAL EFFORT DATA - by date, section, and gear-type (all Skagit models estimate effort by section and gear)                                                                                 ----
#--------------------------------------------------------------------------------------------------------------------- -   
  #Extract effort data and name dimensions (so that they are meaningful)
      effort.results<-res$E; dim(effort.results)
      dimnames(effort.results)<-list(seq(1:nrow(effort.results)), section.xwalk$Section_Name, paste("day",seq(1:standat$D), sep="_"), gear.xwalk$Gear_Name )
      # effort.results[1:2, , 1:2,]
      # E.mean<-setNames(as.data.frame(matrix(matrix(apply(effort.results,c(2:4),mean),nrow=dim(effort.results)[2]*dim(effort.results)[3],byrow=T),nrow=dim(effort.results)[3]))
      #                 , CJ(dimnames(effort.results)[[2]], dimnames(effort.results)[[4]], sorted = FALSE)[, paste(V1, V2, sep =" - ")])
  
  #create data frame to fill with summarized effort data
      Effort.summary<-setNames(as.data.frame(matrix(NA, nrow=0, ncol=7)), c("Day", "Section", "Gear", "Mean", "Median", "l95", "u95"))
    
  #loop through array and summarize by day, section, and gear type 
      for(period in 1:dim(effort.results)[3]){
          sub.period<-effort.results[1:nrow(effort.results), 1:dim(effort.results)[2], period, 1:dim(effort.results)[4]]
          sub.period[1:5,,]
          
          for(gear in 1:dim(sub.period)[2]){
            gear.xwalk
            sub.gear<-sub.period[1:nrow(effort.results), 1:dim(sub.period)[2], gear]
            sub.gear[1:5, ]
            
            sub.Effort.summary<-setNames(as.data.frame(matrix(NA, nrow=dim(sub.period)[2], ncol=7)), c("Day", "Section", "Gear", "Mean", "Median", "l95", "u95"))
            
            for(section in 1:dim(sub.period)[2]){
              sub.section<-sub.gear[, section]
              section.xwalk
              
              sub.Effort.summary[section, "Day"]<-as.numeric(period)
              sub.Effort.summary[section, "Section"]<-as.character(section.xwalk$Section_Name[section.xwalk$Section_Num == section])
              sub.Effort.summary[section, "Gear"]<-as.character(gear.xwalk$Gear_Name[gear.xwalk$Gear_Num == gear])
              sub.Effort.summary[section, "Mean"]<-mean(sub.section)
              sub.Effort.summary[section, "Median"]<-quantile(sub.section, 0.5)
              sub.Effort.summary[section, "l95"]<-quantile(sub.section, 0.025)
              sub.Effort.summary[section, "u95"]<-quantile(sub.section, 0.975)
            }
           Effort.summary<-rbind(Effort.summary, sub.Effort.summary) 
          }
      }
      all.Dates$Day<-as.numeric(as.character(all.Dates$Day)) #make sure "all.Dates$Day" is a numeric variable 
      
  #Join "Effort.summary" with "all.Dates" date and daytype information   
     Effort.summary<-left_join(Effort.summary, all.Dates[, c("Day", "Date", "DayType")], by="Day") 

  # write "Effort.summary" to a .csv file   
      write.csv(Effort.summary, paste(filepath_modelestimates, paste("Summary_Effort (total hours per Period) by Section and Gear", catch.group.of.interest, paste0("Run_", Model_Run), ".csv", sep="_"), sep="/"), row.names = F)

#--------------------------------------------------------------------------------------------------------------------- -
# SUMMARIZE CATCH AND CPUE DATA                                                                                 ----
#--------------------------------------------------------------------------------------------------------------------- -   
  #=============================================================================================================== =
  # CATCH SUMMARY
  #=============================================================================================================== =
    #Extract catch data and name dimensions (so that they are meaningful)
        catch.results<-res$C; dim(catch.results)
        dimnames(catch.results)<-list(seq(1:nrow(catch.results)), section.xwalk$Section_Name, paste("day",seq(1:standat$D), sep="_"), gear.xwalk$Gear_Name )
        # C.mean<-setNames(as.data.frame(matrix(matrix(apply(catch.results,c(2:4),mean),nrow=dim(catch.results)[2]*dim(catch.results)[3],byrow=T),nrow=dim(catch.results)[3]))
        #             , CJ(dimnames(catch.results)[[2]], dimnames(catch.results)[[4]], sorted = FALSE)[, paste(V1, V2, sep =" - ")])
    
    #create data frame to fill with summarized catch data
        Catch.summary<-setNames(as.data.frame(matrix(NA, nrow=0, ncol=7)), c("Day", "Section", "Gear", "Mean", "Median", "l95", "u95"))
      
    #loop through array and summarize by day, gear type, and section
        for(period in 1:dim(catch.results)[3]){
            sub.period<-catch.results[1:nrow(catch.results), 1:dim(catch.results)[2], period, 1:dim(catch.results)[4]]
            sub.period[1:5,,]
            
            for(gear in 1:dim(sub.period)[2]){
              
              gear.xwalk
              sub.gear<-sub.period[1:nrow(catch.results), 1:dim(sub.period)[2], gear]
              sub.gear[1:5, ]
              
              sub.Catch.summary<-setNames(as.data.frame(matrix(NA, nrow=dim(sub.period)[2], ncol=7)), c("Day", "Section", "Gear", "Mean", "Median", "l95", "u95"))
              
              for(section in 1:dim(sub.period)[2]){
                sub.section<-sub.gear[, section]
                section.xwalk
                
                sub.Catch.summary[section, "Day"]<-as.numeric(period)
                sub.Catch.summary[section, "Section"]<-as.character(section.xwalk$Section_Name[section.xwalk$Section_Num == section])
                sub.Catch.summary[section, "Gear"]<-as.character(gear.xwalk$Gear_Name[gear.xwalk$Gear_Num == gear])
                sub.Catch.summary[section, "Mean"]<-mean(sub.section)
                sub.Catch.summary[section, "Median"]<-quantile(sub.section, 0.5)
                sub.Catch.summary[section, "l95"]<-quantile(sub.section, 0.025)
                sub.Catch.summary[section, "u95"]<-quantile(sub.section, 0.975)
              }
             Catch.summary<-rbind(Catch.summary, sub.Catch.summary) 
            }
        }
        
    # Join "Catch.summary" with "all.Dates" date and daytype information   
       Catch.summary<-left_join(Catch.summary, all.Dates[, c("Day", "Date", "DayType")], by="Day") 
       
    # write "Catch.summary" to a .csv file   
       write.csv(Catch.summary, paste(filepath_modelestimates, paste("Summary_Catch (total fish per Period) by Gear and Section", catch.group.of.interest, paste0("Run_", Model_Run), ".csv", sep="_"), sep="/"), row.names = F)
       
  #=============================================================================================================== =
  # CPUE SUMMARY
  #=============================================================================================================== =
    #Extract catch data and name dimensions (so that they are meaningful)
        cpue.results<-res$lambda_C_S; dim(cpue.results)
        dimnames(cpue.results)<-list(seq(1:nrow(cpue.results)), section.xwalk$Section_Name, paste("day",seq(1:standat$D), sep="_"), gear.xwalk$Gear_Name )
        # C.mean<-setNames(as.data.frame(matrix(matrix(apply(cpue.results,c(2:4),mean),nrow=dim(cpue.results)[2]*dim(cpue.results)[3],byrow=T),nrow=dim(cpue.results)[3]))
        #             , CJ(dimnames(cpue.results)[[2]], dimnames(cpue.results)[[4]], sorted = FALSE)[, paste(V1, V2, sep =" - ")])
    
    #create data frame to fill with summarized catch data
        CPUE.summary<-setNames(as.data.frame(matrix(NA, nrow=0, ncol=7)), c("Day", "Section", "Gear", "Mean", "Median", "l95", "u95"))
      
    #loop through array and summarize by day, gear type, and section
        for(period in 1:dim(cpue.results)[3]){
            sub.period<-cpue.results[1:nrow(cpue.results), 1:dim(cpue.results)[2], period, 1:dim(cpue.results)[4]]
            sub.period[1:5,,]
            
            for(gear in 1:dim(sub.period)[2]){
              
              gear.xwalk
              sub.gear<-sub.period[1:nrow(cpue.results), 1:dim(sub.period)[2], gear]
              sub.gear[1:5, ]
              
              sub.CPUE.summary<-setNames(as.data.frame(matrix(NA, nrow=dim(sub.period)[2], ncol=7)), c("Day", "Section", "Gear", "Mean", "Median", "l95", "u95"))
              
              for(section in 1:dim(sub.period)[2]){
                sub.section<-sub.gear[, section]
                section.xwalk
                
                sub.CPUE.summary[section, "Day"]<-as.numeric(period)
                sub.CPUE.summary[section, "Section"]<-as.character(section.xwalk$Section_Name[section.xwalk$Section_Num == section])
                sub.CPUE.summary[section, "Gear"]<-as.character(gear.xwalk$Gear_Name[gear.xwalk$Gear_Num == gear])
                sub.CPUE.summary[section, "Mean"]<-round(mean(sub.section),3)
                sub.CPUE.summary[section, "Median"]<-round(quantile(sub.section, 0.5),3)
                sub.CPUE.summary[section, "l95"]<-round(quantile(sub.section, 0.025),3)
                sub.CPUE.summary[section, "u95"]<-round(quantile(sub.section, 0.975),3)
              }
             CPUE.summary<-rbind(CPUE.summary, sub.CPUE.summary) 
            }
        }
        
    # Join "CPUE.summary" with "all.Dates" date and daytype information   
       CPUE.summary<-left_join(CPUE.summary, all.Dates[, c("Day", "Date", "DayType")], by="Day") 
       
    # write "CPUE.summary" to a .csv file   
       write.csv(CPUE.summary, paste(filepath_modelestimates, paste("Summary_CPUE (fish per hr per Period) by Gear and Section", catch.group.of.interest, paste0("Run_", Model_Run), ".csv", sep="_"), sep="/"), row.names = F)   


```

# results

```{r}

# GENERATE PLOTS OF EFFORT AND CATCH SUMMARIES                                                                      ----
#--------------------------------------------------------------------------------------------------------------------- -        
      
  #plot timeseries and total effort and catch
    plot.width<-8.5
    plot.height<-11
    
  #Set up plotting arguments
    eff.pch=21
    eff.cex=2
    gear.cols<-c(colorRampPalette(brewer.pal(max(3,as.numeric(length(unique(Effort.summary$Gear)))),"Blues"))(length(unique(Effort.summary$Gear))))
    section.cols<-c(colorRampPalette(brewer.pal(max(3,as.numeric(length(unique(Effort.summary$Section)))),"Greens"))(length(unique(Effort.summary$Section))))

  #Self-made jitter sequence for x-axis
    jitter.right<-seq(0,1,0.25)
    temp.jitter<-c()
    for(value in 1:length(jitter.right)){
      temp.jitter<-c(temp.jitter, c(jitter.right[value], jitter.right[value]*-1))
    }
    jitter.seq<-as.numeric(unique(temp.jitter))

#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++    
#START OF PDF FUNCTION!!!!! 
#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++    
  pdf(paste(filepath_modelestimates, paste("BSS creel model summary plots", catch.group.of.interest, paste0("Run_", Model_Run),".pdf", sep="_"), sep="/"), width=plot.width, height=plot.height)
  mpg_axis<-0

```


```{r}
# Plot #1 - PDF of total seasonal catch and total seasonal effort
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~
      # set par arguments  
        #windows(width=8.5, height=11)
        par(mfcol=c(2,1), family='sans',  xaxs="i", yaxs="i", cex.axis=1, cex=1, mgp=c(2, 0.75, mpg_axis), mar=c(3,3,1,1), oma=c(4,3,1.5,1)) 
    
      #total seasonal catch    
        plot(density(apply(res$C_sum, c(1), sum)), col="blue", xlab="Total Catch (fish)", ylab="Probability Density", main="", bty="n", xaxs="i",yaxs="i")
        #abline(v=mean(c(apply(res$C_sum, c(1), sum))),lwd=2, lty=2, col="red") #add abline for mean
        abline(v=median(c(apply(res$C_sum, c(1), sum))),lwd=2, lty=2,  col="red") #add abline for median
        abline(v=quantile(apply(res$C_sum, c(1), sum), 0.025),lwd=2, lty=2,  col="black"); abline(v=quantile(apply(res$C_sum, c(1), sum), 0.975),lwd=2, lty=2,  col="black")  #add ablines for 95%
        legend("topright", c( "median", "95%CI"), lty=2, col=c("red", "black"), bty="n", lwd=2)
      
      #total seasonal effort  
        plot(density(res$E_sum),col="blue",xlab="Total Effort (hrs)", ylab="Probability Density", main="", bty="n", xaxs="i",yaxs="i")
        #abline(v=mean(res$E_sum),lwd=2, lty=2, col="red") #add abline for mean
        abline(v=quantile(res$E_sum, 0.5),lwd=2, lty=2,  col="red") #add abline for median
        abline(v=quantile(res$E_sum, 0.025),lwd=2, lty=2,  col="black"); abline(v=quantile(res$E_sum, 0.975),lwd=2, lty=2,  col="black")  #add ablines for 95%
        

```


```{r}
# Plot #2 - Plot Total Daily Effort (hrs) by Section and Gear Type
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~
      # windows(height=11, width=8.5)
        
      # set par arguments    
          par(mfcol=c(length(unique(Effort.summary$Section)),1), family='sans', xaxs="i", yaxs="i",cex.axis=1, cex=1, mgp=c(2,0.75, mpg_axis),mar=c(2,2,1,1), oma=c(3,3,1.5,1)) 
      
      # Loop through "Effort.summary" and plot
          for(section in 1:length(unique(Effort.summary$Section))){
              sub.section<-Effort.summary[Effort.summary$Section == unique(Effort.summary$Section)[section],]
              
              plot(NA, bty="n", ylim=c(-10, round_any(max(Effort.summary$u95, na.rm=T),200,f=ceiling)), xlim=c(0, max(Effort.summary$Day, na.rm=T)+1)) #, main=paste("Section - ", unique(Effort.summary$Section)[section], sep=""))
              
              for(day in 1:length(unique(sub.section$Day))){
                  sub.day<-sub.section[sub.section$Day == unique(sub.section$Day)[day],]
                  if(sub.day$DayType[1]==2){
                      abline(v=unique(sub.section$Day)[day], col="gray", lty=2, lwd=2)
              }}
                  for(gear in 1:length(unique(sub.section$Gear))){
                    sub.gear<-sub.section[sub.section$Gear == unique(sub.section$Gear)[gear],]
                    x.axis<-as.numeric(as.character(sub.gear$Day))+jitter.seq[gear]
                    
                    arrows(x0=(as.numeric(as.character(sub.gear$Day))+jitter.seq[gear]), y0=as.numeric(sub.gear$l95)
                         , x1=(as.numeric(as.character(sub.gear$Day))+jitter.seq[gear]), y1=as.numeric(sub.gear$u95)
                         , length=0.05, angle=90, code=3, lwd=2)
                    points(sub.gear$Median ~ x.axis, bg=gear.cols[gear], pch=eff.pch, cex=eff.cex, col="black")
        
                  }
              for(day in 1:length(unique(sub.section$Day))){
                  sub.day<-sub.section[sub.section$Day == unique(sub.section$Day)[day],]
                  if(sub.day$Mean[1] == 0){
                    x.line<-sub.day$Day+jitter.seq[1:length(unique(sub.section$Gear))]
                    lines(x=c(min(x.line), max(x.line)), y=c(0,0), col='gray', lend=2, lwd=30)
              }
              legend("topleft", legend = unique(Effort.summary$Section)[section], bty="n", bg="white", box.col = "white")  
              }
          }
          legend("topright", legend = unique(Effort.summary$Gear), bty="n", cex=1, pch=eff.pch, col="black", pt.bg = gear.cols, pt.cex=eff.cex)
          title(xlab = "Day", ylab = "Total Daily Effort (hrs)", outer=T, line=1)

```


```{r}
 # Plot #3 - Plot Total Daily Catch (fish) by Section and Gear Type
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~
      #windows(height=11, width=8.5)
      par(mfcol=c(length(unique(Catch.summary$Section)),1), family='sans', xaxs="i", yaxs="i",cex.axis=1, cex=1, mgp=c(2,0.75, mpg_axis),mar=c(2,2,1,1), oma=c(3,3,1.5,1)) 
  

      for(section in 1:length(unique(Catch.summary$Section))){
          sub.section<-Catch.summary[Catch.summary$Section == unique(Catch.summary$Section)[section],]
          
          plot(NA, bty="n", ylim=c(-1, round_any(max(Catch.summary$u95, na.rm=T),10,f=ceiling)), xlim=c(0, max(Catch.summary$Day, na.rm=T)+1)) #, main=paste("Section - ", unique(Catch.summary$Section)[section], sep=""))
          
          for(day in 1:length(unique(sub.section$Day))){
              sub.day<-sub.section[sub.section$Day == unique(sub.section$Day)[day],]
              if(sub.day$DayType[1]==2){
                  abline(v=unique(sub.section$Day)[day], col="gray", lty=2, lwd=2)
          }}
              for(gear in 1:length(unique(sub.section$Gear))){
                sub.gear<-sub.section[sub.section$Gear == unique(sub.section$Gear)[gear],]
                x.axis<-as.numeric(as.character(sub.gear$Day))+jitter.seq[gear]
                
                arrows(x0=(as.numeric(as.character(sub.gear$Day))+jitter.seq[gear]), y0=as.numeric(sub.gear$l95)
                     , x1=(as.numeric(as.character(sub.gear$Day))+jitter.seq[gear]), y1=as.numeric(sub.gear$u95)
                     , length=0.05, angle=90, code=3, lwd=2)
                points(sub.gear$Median ~ x.axis, bg=gear.cols[gear], pch=eff.pch, cex=eff.cex, col="black")
    
              }
          for(day in 1:length(unique(sub.section$Day))){
              sub.day<-sub.section[sub.section$Day == unique(sub.section$Day)[day],]
              if(sub.day$Mean[1] == 0){
                x.line<-sub.day$Day+jitter.seq[1:length(unique(sub.section$Gear))]
                lines(x=c(min(x.line), max(x.line)), y=c(0,0), col='gray', lend=2, lwd=30)
          }
          legend("topleft", legend = unique(Catch.summary$Section)[section], bty="n", bg="white", box.col = "white")  
          }
      }
      legend("topright", legend = unique(Catch.summary$Gear), bty="n", cex=1, pch=eff.pch, col="black", pt.bg = gear.cols, pt.cex=eff.cex)
      title(xlab = "Day", ylab = "Total Daily Catch (fish)", outer=T, line=1)

```


```{r}
  # Plot #4 - Plot  Daily CPUE (fish/hr)
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~
      #windows(height=11, width=8.5)
      par(mfcol=c(length(unique(CPUE.summary$Section)),1), family='sans', xaxs="i", yaxs="i",cex.axis=1, cex=1, mgp=c(2,0.75, mpg_axis),mar=c(2,2,1,1), oma=c(3,3,1.5,1)) 
  

      for(section in 1:length(unique(CPUE.summary$Section))){
          sub.section<-CPUE.summary[CPUE.summary$Section == unique(CPUE.summary$Section)[section],]
          
          plot(NA, bty="n", ylim=c(0, round_any(max(CPUE.summary$u95, na.rm=T),0.05,f=ceiling)), xlim=c(0, max(CPUE.summary$Day, na.rm=T)+1)) #, main=paste("Section - ", unique(CPUE.summary$Section)[section], sep=""))
          
          for(day in 1:length(unique(sub.section$Day))){
              sub.day<-sub.section[sub.section$Day == unique(sub.section$Day)[day],]
              if(sub.day$DayType[1]==2){
                  abline(v=unique(sub.section$Day)[day], col="gray", lty=2, lwd=2)
          }}
              for(gear in 1:length(unique(sub.section$Gear))){
                sub.gear<-sub.section[sub.section$Gear == unique(sub.section$Gear)[gear],]
                x.axis<-as.numeric(as.character(sub.gear$Day))+jitter.seq[gear]
                
                arrows(x0=(as.numeric(as.character(sub.gear$Day))+jitter.seq[gear]), y0=as.numeric(sub.gear$l95)
                     , x1=(as.numeric(as.character(sub.gear$Day))+jitter.seq[gear]), y1=as.numeric(sub.gear$u95)
                     , length=0.05, angle=90, code=3, lwd=2)
                points(sub.gear$Median ~ x.axis, bg=gear.cols[gear], pch=eff.pch, cex=eff.cex, col="black")
    
              }
          for(day in 1:length(unique(sub.section$Day))){
              sub.day<-sub.section[sub.section$Day == unique(sub.section$Day)[day],]
              if(sub.day$Mean[1] == 0){
                x.line<-sub.day$Day+jitter.seq[1:length(unique(sub.section$Gear))]
                lines(x=c(min(x.line), max(x.line)), y=c(0,0), col='gray', lend=2, lwd=30)
          }
          legend("topleft", legend = unique(CPUE.summary$Section)[section], bty="n", bg="white", box.col = "white")  
          }
      }
      legend("topright", legend = unique(CPUE.summary$Gear), bty="n", cex=1, pch=eff.pch, col="black", pt.bg = gear.cols, pt.cex=eff.cex)
      title(xlab = "Day", ylab = "Daily CPUE (fish/hr)", outer=T, line=1)

  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~
  # Plot #5 - Total catch and effort by day
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~


dev.off()

```












