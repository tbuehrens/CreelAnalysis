---
title: CreelAnalysis from DWG
params:
  proj_name: "Skagit"
  water_body: "Skagit River,Sauk River"
  date_start: "2021-02-01"   
  date_end: "2021-04-13"
  year_group: "2020-2021"
  species: "Steelhead"
  fin_mark: "UM"
  fate: "Released"
  Model_number: 4
output:
  html_document:
    fig_caption: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
    code_folding: hide
---

# setup 

```{r setup, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(message = FALSE)

#library(DBI); library(odbc); library(dbplyr)
library(tidyverse)
library(rstan)

#base endpoints
dwg_base <- list(
  event = "https://data.wa.gov/resource/ui95-axtn.csv",
  effort = "https://data.wa.gov/resource/h9a6-g38s.csv",
  interview = "https://data.wa.gov/resource/rpax-ahqm.csv",
  catch = "https://data.wa.gov/resource/6y4e-8ftk.csv",
  gear = "https://data.wa.gov/resource/d2ks-afhz.csv"
)

dwg_sent <- list() #will hold full API strings built on above endpoints with params
creel <- list() #will hold resulting data objects

dates_holidays_2015_2030 <- read_lines("input_files/dates_holidays_2015_2030.txt") |> 
  as.Date(format="%Y-%m-%d")

lut <- map(
  list(
    river_loc = "input_files/lut_River.Locations_2019-01-07.csv",
    creel_models = "input_files/lut_Creel_Models_2021-01-20.csv",
    sections = "input_files/lut_water_body_location.csv",
    census_expansion = "input_files/lut_Proportional_Expansions_for_Tie_In_Sections_Skagit_Steelhead_2021_Example.csv" # EB Added proportional TI expansion to list of LUTs
  ),
  ~readr::read_csv(file.path(.x))
)


#tie_in_indicator: 0 is index/creel, 1 is tie-in/census

# Proportional tie in expansion table 
# value of 1 for p_TI means that the entire river section is surveyed during census counts?
# What is "Indirect_TI_Expan" and how it is calculated?

```

```{r build_mostly_static_inputs, eval=FALSE}
#### can/should continue to rethink the best way to get these in
#
## holiday dates
# write_lines(
#   2015:2030 %>%
#     map(~c(
#       timeDate::USNewYearsDay(.x), 
#       timeDate::USMLKingsBirthday(.x),
#       timeDate::USPresidentsDay(.x),
#       timeDate::USMemorialDay(.x), 
#       timeDate::USIndependenceDay(.x), 
#       timeDate::USLaborDay(.x),
#       timeDate::USVeteransDay(.x), 
#       timeDate::USThanksgivingDay(.x), 
#       timeDate::timeDate(as.character(timeDate::.nth.of.nday(.x, 11, 5, 4))), #Black Friday
#       timeDate::USChristmasDay(.x)
#     ) |> as.character()
#     ) |>  unlist(),
#   "input_files/dates_holidays_2015_2030.txt")
#
##starting point for section numbering, tie-in designation, prop expansion
##requires behind-VPN direct connection to DB
# lut_water_body_location <- full_join(
#   tbl(con_creel, dbplyr::in_schema("creel", "water_body_lut")) |> collect() |> select(water_body_id:water_body_desc),
#   tbl(con_creel, dbplyr::in_schema("creel", "location_lut")) |> collect() |> select(location_id:lower_rm),
#   by = c("water_body_id")
#   ) |>
#   collect()
# 
# write_csv(lut_water_body_location, "input_files/lut_water_body_location.csv")
```

# data

The data used are from the `r params$proj_name` project on the `r params$location` between `r params$date_start` and `r params$date_end`.

Further development may include interactive control parameter specification via the GUI: [https://bookdown.org/yihui/rmarkdown/params-knit.html#the-interactive-user-interface]

There is also the option to step through multiple pre-defined control parameters:
[https://bookdown.org/yihui/rmarkdown-cookbook/parameterized-reports.html]

## creel events

First, get the creel events of interest by building the Socrata API url string and grabbing the data

```{r get_event}
dwg_sent$event <- URLencode(
  paste0(dwg_base$event,
         "?$where=project_name in('", params$proj_name, "')",
         " AND water_body in('", str_replace(params$water_body, ",|\\|", "','"), "')",
         " AND event_date between '", params$date_start,
         "T00:00:00' and '", params$date_end,
         "T00:00:00'&$limit=100000"
  )
)

creel$event <- read_csv(dwg_sent$event) |> 
  dplyr::select(creel_event_id, water_body, event_date, tie_in_indicator)
```

Then, get the associated effort and interview data.

```{r pending_vw_changes_for_water_body}
# #if water_body dropped from event filter...
# #but regardless can/should build creel_event_id condition once and apply twice?
# eff_int_filter <- paste0(
#     "?$where=creel_event_id in('",
#     paste(creel$event$creel_event_id, collapse = "','"), "')",
#     " AND water_body in('", str_replace(params$water_body, ",|\\|", "','"), "')",
#     "&$limit=100000"
#   )

```


## effort counts

```{r get_effort}
dwg_sent$effort <- URLencode(
  paste0(dwg_base$effort,
         "?$where=creel_event_id in('",
         paste(creel$event$creel_event_id, collapse = "','"),
         "')&$limit=100000"
  )
)

creel$effort <- read_csv(dwg_sent$effort)
```

## interviews

```{r get_interview}
dwg_sent$interview <- URLencode(
  paste0(dwg_base$interview,
         "?$where=creel_event_id in('",
         paste(creel$event$creel_event_id, collapse = "','"),
         "')&$limit=100000"
  )
)

creel$interview <- read_csv(dwg_sent$interview) |> 
  rename(location = interview_location)
```

## catch data

And finally, the catch data associated with the interviews.

```{r get_catch}
dwg_sent$catch <- URLencode(
  paste0(dwg_base$catch,
         "?$where=creel_event_id in('",
         paste(creel$event$creel_event_id, collapse = "','"),
         "')&$limit=100000"
  )
)

# catch with redundant post-join columns removed 
# filtering catch to specific catch group parameter
creel$catch <- read_csv(dwg_sent$catch) |> 
  dplyr::select(interview_id, catch_id, species, run, life_stage, fin_mark, fate, fish_count) 

```

## sections

Aggregations of `location` units that may or may not correspond to `water_body`/`water_body_desc`...

```{r add_sections}
creel$effort <- creel$effort |> 
  select(-created_datetime, -modified_datetime) |>
  left_join(
    lut$sections |> select(water_body_desc, location = location_code, section),
    by = c("location")
    ) # |> count(section)

creel$interview <- creel$interview |> 
  select(-created_datetime, -modified_datetime,
         -state_residence, -zip_code) |> 
  left_join(
    lut$sections |> select(water_body_desc, location = location_code, section),
    by = c("location")
  )

### working out options below, can delete soon...

#x <- read_csv(URLencode(paste0(dwg_base$effort, "?$limit=1000000")))
#x |> count(project_name, water_body) |> print(n=50)

# #all the units in the database associated with the YAML water_body controls
# lut$sections |> 
#   #count(water_body_code, water_body_desc) |> print(n=100)
#   filter(water_body_desc %in% unlist(str_split(params$water_body, pattern = ","))) |> print(n=100)

# #all the units in the pulled data
# creel$event |> count(water_body)
# creel$effort |> count(water_body, location) |> print(n=50)
# creel$interview |> count(water_body, location) |> print(n=50)


# #join the full set of places to those that have been queried
# #then user assign section numbering according to analysis need
# sections <- full_join(
#   distinct(creel$effort, water_body, location),
#   distinct(creel$interview, water_body, location),
#   by = c("water_body", "location")
# ) |> 
#   left_join(
#     lut$sections |> select(water_body_desc, location = location_code, upper_rm, lower_rm)
#     ,
#     by = c("location")
#   ) |> 
#   #count(water_body, water_body_desc)
# ## user input to assign "section" numbering to the set of water_bodies
# ## in the queried data
#   left_join(
#     # tibble(
#     #   water_body_desc = c("Sauk River", "Skagit River"),
#     #   section = c(2,1) #equivalent to "Section.Summ" in older "Crosswalk_Table"s
#     # )
#     #,
#     #by = "water_body_desc"
# 
#     ##could also go against "location"
#     tibble(
#       location = c("101 hole (Car Body hole access point)", "Baker River Boat Ramp", "Big Timber log sign to Ovenells Heritage Inn driveway", "Chapel Hole", "Bryson Road access parking area", "Bryson Road to USFS Lower Sauk Ramp along Hwy 530", "Darrington Sauk-Prairie Road bridge (aka Darrington Mill Bridge)"),
#       section = c(1,1,1,1,2,2,2)
#     )
#     , 
#     by = "location"
#     )
# 
# 
# # #alternatives for example..
# #     tibble(
# #       water_body_desc = c("Stillaguamish River - North Fork", "Stillaguamish River - South Fork", "Stillaguamish River"),
# #       section = c(1,1,2) #or whatever...
# #     )

# #confirming that this codes locations to identical levels as prior csv approach 
# left_join(
#   sections,
#   read_csv("input_files/lut_02_Crosswalk_Table_for_Index_TieIn_Sections_2021-02-04_MatchedDB.csv"),
#   by = c("location" = "Section.Field")
#   ) |> 
#   select(-c(upper_rm:lower_rm, StreamName:Survey.Type)) |> 
#   mutate(d = section - Section.Summ) |> 
#   print(n=40)


# #add to the primary objects
# creel$effort <- creel$effort |> 
#   select(-created_datetime, -modified_datetime) |> 
#   left_join(
#     sections |> select(water_body, location, water_body_desc, section),
#     by = c("water_body", "location")
#     )
# 
# creel$interview <- creel$interview |> 
#   select(-created_datetime, -modified_datetime,
#          -state_residence, -zip_code) |> 
#   left_join(
#     sections |> select(water_body, location, water_body_desc, section),
#     by = c("water_body", "location")
#   )

```

## dates

First, build an "expanded dates lattice" to which any/all observations are attached, ensuring complete cases.

Then join per-section closures by date.

```{r d_days}
#creel$event |> distinct(event_date)
(
  d_days <- tibble(
    event_date = seq(
      as.Date(params$date_start, "%Y-%m-%d"),
      as.Date(params$date_end, "%Y-%m-%d"),
      by = "day")
    ) |> 
    mutate(
      Day = weekdays(event_date),
      DayType = if_else(Day == "Saturday" | Day == "Sunday" | Day %in% dates_holidays_2015_2030, "Weekend", "Weekday"),
      DayType_num = if_else(str_detect(DayType, "end"),1,0),
      DayL = suncalc::getSunlightTimes(
        date = event_date,
        tz = "America/Los_Angeles",
#need to add flexibility for other rivers/multiple lines in River.Locations lut
        lat = lut$river_loc$Lat,
        lon = lut$river_loc$Long,
        keep=c("dawn", "dusk")
      ) |> 
        mutate(DayL = as.numeric(dusk - dawn)) |>
        pluck("DayL"),
      Week = as.numeric(format(event_date, "%V"))
    ) |> 
    rowid_to_column(var = "day_index") |> 
    #now add closures...
    left_join(
      bind_rows(
        tibble(section = "1,2", closure_begin = "2021-02-03", closure_end = "2021-02-05"),
        tibble(section = "1,2", closure_begin = "2021-02-10", closure_end = "2021-02-12"),
        tibble(section = "1,2", closure_begin = "2021-02-17", closure_end = "2021-02-19"),
        tibble(section = "1,2", closure_begin = "2021-02-24", closure_end = "2021-02-26"),
        tibble(section = "1,2", closure_begin = "2021-03-03", closure_end = "2021-03-05"),
        tibble(section = "1,2", closure_begin = "2021-03-10", closure_end = "2021-03-12"),
        tibble(section = "1,2", closure_begin = "2021-03-17", closure_end = "2021-03-19"),
        tibble(section = "1,2", closure_begin = "2021-03-24", closure_end = "2021-03-26"),
        tibble(section = "1,2", closure_begin = "2021-03-31", closure_end = "2021-04-02"),
        tibble(section = "1,2", closure_begin = "2021-04-07", closure_end = "2021-04-09"),
        ) |> 
        rowwise() |>
        mutate(closure_date = paste(seq.Date(as.Date(closure_begin), as.Date(closure_end), by = "day"), collapse = ",")) |> 
        separate_rows(closure_date, sep = ",") |> 
        select(event_date = closure_date, section) |> 
        mutate(
          event_date = as.Date(event_date),
          closure_code = FALSE # TB - The 1e-06 is needed to keep the model from crashing ¦log-normal parameters cant be 0
        ) |> 
        separate_rows(section, sep = ",") |> 
        pivot_wider(names_from = section, names_prefix = "open_section_", values_from = closure_code)
      ,
      by ="event_date"
      ) |> 
    mutate(across(starts_with("open_section_"), ~replace_na(., TRUE)))
)

```

# Some sort of pre-analysis QAQC?

```{r, eval=FALSE}
identical(nrow(event), nrow(distinct(event, creel_event_id))) #example, likely unnecessary given database structure

#effort$creel_event_id matches event$creel_event_id
distinct(effort, creel_event_id)
setdiff(event$creel_event_id, effort$creel_event_id)
setdiff(effort$creel_event_id, event$creel_event_id)

#but interview$creel_event_id is nested subset of event$creel_event_id
distinct(interview, creel_event_id)
setdiff(event$creel_event_id, interview$creel_event_id)
setdiff(interview$creel_event_id, event$creel_event_id)

#and catch$creel_event_id is nested subset of interview$creel_event_id
setdiff(interview$creel_event_id, catch$creel_event_id)
setdiff(catch$creel_event_id, interview$creel_event_id)

#so catch$creel_event_id is even smaller subset of event$creel_event_id
distinct(catch, creel_event_id)
setdiff(event$creel_event_id, catch$creel_event_id)
setdiff(catch$creel_event_id, event$creel_event_id)

#similarly catch$interview_id is a nested subset of interview$interview_id
setdiff(interview$interview_id, catch$interview_id)
setdiff(catch$interview_id, interview$interview_id)

#event |> head() |> gt::gt()

```



# wrangle for `stan()`

```{r init_stan_data}
#declare an intermediary list
#can see later about functionalizing or removing and getting inline
#these are about to be overwritten by aggregated versions...

stan_data <- list(
  effort_census = creel$effort |> filter(tie_in_indicator == 1),
  effort_index = creel$effort |> filter(tie_in_indicator == 0)
)

```

## effort

### census

Aggregate census (tie in) effort counts, associating to closest-in-time index count. 

```{r build_effort_census}
stan_data$effort_census <- stan_data$effort_census |> 
  select(event_date, water_body, water_body_desc, location, section, tie_in_indicator, count_type, count_quantity) |> 
  #add/overwrite count_sequence (all census count_sequence == 1) with closest temporal match of count_sequence from paired counts object 
  left_join(
    #inline paired closest index counts to census
    left_join(
      stan_data$effort_census |> distinct(event_date, section, location, tie_in_indicator, effort_start_time, count_sequence),
      stan_data$effort_index |> distinct(event_date, section, location, tie_in_indicator, effort_start_time, count_sequence),
      by = c("event_date", "section"),
      suffix = c("_cen", "_ind")
    ) |> 
      group_by(event_date, section, location_cen) |> 
      slice_min(abs(effort_start_time_cen - effort_start_time_ind), n = 1) |> 
      ungroup() |> 
      #count(event_date, section, location_cen, count_sequence_cen, count_sequence_ind)
      distinct(event_date, section, location = location_cen, count_sequence = count_sequence_ind)
    ,
    by = c("event_date", "section", "location")
  ) |> 
  left_join(d_days, by = "event_date") |>
  mutate(
    angler_type = word(count_type, 1),
  ) |>
  group_by(event_date, day_index, section, count_sequence, angler_type) |>
  summarize(count_quantity = sum(count_quantity), .groups = "drop") |> 
  arrange(event_date, section, count_sequence) 

```

### index

Aggregate index counts of vehicles, trailers, anglers, and boats.

```{r build_effort_index}
# DW - 23 effort count stops return a different Count Number / Count Sequence between the static original values and the current dynamic values returned by the database. Most of these are due to the first count not being done, and the raw csv staring at CountNum = 2. I retained the original value in the comment field of the effort_event table. 

# EB - added in 'no count' events so that Database derived count sequence matches that of the original data. Only found these on 2/8 for 14 rows (effort counts of cars or trailers)

##DA added filter(!is.na(count_quantity)) to address above comments...
## need to think about whether !is.na(no_count_reason) should be interpolated or inferred or zeroed or...
## for example no_count_reason == "Conditions", what to do with count_quantity == NA
stan_data$effort_index <- stan_data$effort_index |> 
  filter(!is.na(count_quantity)) |> 
  select(event_date, water_body, water_body_desc, location, section, tie_in_indicator, count_sequence, count_type, count_quantity) |>
  left_join(d_days, by = "event_date") |> 
  group_by(section, event_date, day_index, Week, count_sequence, count_type) |> 
  summarise(count_quantity = sum(count_quantity), .groups = "drop") |> 
  pivot_wider(names_from = count_type, values_from = count_quantity) |> 
  arrange(event_date, section, count_sequence)
```


## interview

Only "index" for interview, by definition.

```{r build_stan_data_interview}
stan_data$interview <- creel$interview |> 
  left_join(d_days, by = "event_date") |> #summary()
  mutate(
    across(c(vehicle_count, trailer_count), ~replace_na(., 0)),
    trip_status = replace_na(trip_status, "Unknown"),
    angler_type = if_else(boat_used == "No", "Shore", "Boat"), #problems outside Skagit test case if census levels differ?
    fishing_end_time = if_else(is.na(fishing_end_time), interview_time, fishing_end_time),
    angler_hours = round(as.numeric(fishing_end_time - fishing_start_time) / 3600, 5),
    angler_hours_total = angler_count * angler_hours
  ) |> 
  left_join(
    creel$catch |> 
      filter(
        str_detect(species, params$species |> str_squish() |> str_replace("[:space:]", "") |> str_replace(",|\\|", "|")),
        str_detect(fin_mark, params$fin_mark |> str_squish() |> str_replace("[:space:]", "") |> str_replace(",|\\|", "|")),
        str_detect(fate, params$fate |> str_squish() |> str_replace("[:space:]", "") |> str_replace(",|\\|", "|"))
        # ,
        # str_detect(life_stage, params$life_stage |> str_squish() |> str_replace("[:space:]", "") |> str_replace(",|\\|", "|")),
        # str_detect(run, params$run |> str_squish() |> str_replace("[:space:]", "") |> str_replace(",|\\|", "|")),
      ) |> 
    group_by(interview_id) |> 
    summarise(fish_count = sum(fish_count), .groups = "drop")
    ,
    by = "interview_id"
  ) |> 
  select(section, event_date, day_index, Week, interview_id, interview_time, contains("angler"), contains("count")) |> 
  mutate(across(fish_count, ~replace_na(., 0))) |> 
  filter(angler_hours > 0) |>  
  arrange(event_date)


# 
# # EB: need to make current code covers these filters from original code 
#   #Drop "final_group.dat" data if "Start.Time" is <NA>, "Interview.Time" is <NA> for groups with Trip.Status == I, or "End.Time" is <NA> for groups with Trip.Status == C
# 
#   #Drop rows if hours fished by a Group is Negative 
# 
# # summarizing catch by unique interview (interview_id) in this chunk 
# interview_index_TF <- interview_index |> 
#   group_by(interview_id, event_date, interview_time, day_index, section, angler_count, angler_type, angler_hours, angler_hours_total,vehicle_count, trailer_count) |>
#   summarize(fish_count = sum(fish_count), .groups = "drop") |> 
#   arrange(event_date, section, interview_time)
# 
# # EB: equivalent to gear.xwalk table in original code. Keep going back on forth on what terminology makes most sense between "gear_type" or "angler_type"
# lut_angler_type <- interview_index_TF |>
#   select(angler_type) |> 
#   distinct() |> 
#   rowid_to_column(var = "angler_type_num")

```


### Daily interview summaries

```{r}
#total hours creel and total catch sample a per day/angler type/section to compare with effort
stan_data$interview_daily_totals <- stan_data$interview |>
  group_by(event_date, day_index, section, angler_type) |>
  summarise(
    angler_hours_total_dailysum = sum(angler_hours_total),
    catch_dailysum = sum(fish_count), .groups = "drop"
  ) |> 
#not sure whether this should stay?
  filter(angler_hours_total_dailysum > 0)

```

### Exclude interviews with NA vehicle and trailer counts

This would form the basis for IntA below if there were cause in the data...?

For now, leaving undeclared and simply defining IntA as == IntC, pending further revision...

```{r}
#stan_data$interview_with_trailer_vehicle_count <-

# stan_data$interview_no_zero_counts <- stan_data$interview |>
# 	#filter(across(c(vehicle_count, trailer_count), ~ !is.na(.)))
#   filter(vehicle_count > 0)# | trailer_count > 0 | angler_count > 0)

```


## stan_dat

```{r stan_dat}
stan_dat <- list(
  
  # Day attributes
  # int; number of fishing days; KB: pull from "master_date" DF
  D = nrow(d_days),
  # int; final number of unique gear/angler types  KB: pull from subsetted angler x-walk?
  G = as.integer(length(unique(stan_data$interview$angler_type))),
	# int; final number of river sections    KB: pull from subsetted section x-walk?
  S =  as.integer(length(unique(stan_data$effort_index$section))),
	# int; max number of angler effort counts within a sample day KB: max(z$effort$count_sequence)
	H = max(stan_data$effort_index$count_sequence),
  # int; number of days/periods KB: right now, model is set up to running as daily (P_n = D) or weekly (P_n = effectively D/7)
##NEED TO UPDATE? input param and if_else?
	P_n = nrow(d_days),
  #or for weekly? length(unique(d_days$Week))
	# vec; index denoting daytype (day-type is offset in model)   KB: pull from "master_date" DF (1= weekend, 0 = weekday)
  w = d_days$DayType_num,
	# int; index denoting fishing day/period      KB: calculate as 1:P_n (not sure why this isn't a vector)
	period = 1:nrow(d_days), 
	# vec; daylength (model offset; assumption)       KB: pull from "master_date" DF
	L = d_days$DayL,
	# mat; index denoting fishery status                          KB: user defined (1=open, 0 = closed; by period/date and section; 0 defined as 1E-6 for model)
	O = d_days |> 
	  select(contains("section_")) |>
    mutate(across(everything(), ~if_else(., 1, 0.00001))) |> 
	  as.matrix(),  

  # Vehicle index effort counts 
  # int; total number of individual vehicle index effort counts 
	V_n = nrow(stan_data$effort_index),
  # int; observed # of vehicles 
	V_I = stan_data$effort_index$`Vehicle Only`,
	# int; index for day/period 
	day_V = stan_data$effort_index$day_index,
  gear_V = as.integer(rep(1, nrow(stan_data$effort_index))),
	# int; index for section    
  section_V = stan_data$effort_index$section,
	# int; index for count_num  
	countnum_V = stan_data$effort_index$count_sequence,
	
	# Trailer index effort counts
  # int; total number of boat trailer index effort counts 
	T_n = nrow(stan_data$effort_index),
	# int; observed # of boat trailers 
	T_I = stan_data$effort_index$`Trailers Only`,
  # int; index for day/period
## INTENTIONALLY LEFT "WRONG" TO MARK LATER REVISION IN ABOVE WRANGLING
## TO ALLOW RAGGED VEHICLE/TRAILER VECTORS 
	day_T = stan_data$effort_index$day_index, #day_V,
  gear_T = as.integer(rep(2, nrow(stan_data$effort_index))), #SAME - TEMP OPTION PENDING OTHER CHANGES
  # int; index for section
	section_T = stan_data$effort_index$section,
  # int; index for count_num  
	countnum_T = stan_data$effort_index$count_sequence,

  # Angler index effort counts
	# int; total number of angler index effort counts
	# Need to circle back on how to deal with this when applicable, set all values to 0 as a placeholder, matching example standat_2021-05-28.txt
	A_n = 0,
	# int; observed # of anglers
	A_I = numeric(0), #EB placeholder
	# int; index for day/period
	day_A = numeric(0),
	# int; index denoting "gear/angler type"  
	gear_A = numeric(0), #EB placeholder 
	# int; index for section 
	section_A = numeric(0),
	# int; index for count_num
	countnum_A = numeric(0),

  
  # Census (tie-in) effort counts   
	# int; total number of angler tie-in effort counts
	E_n = nrow(stan_data$effort_census),
  # int; index denoting day/period
	day_E = stan_data$effort_census$day_index,
  # int; index denoting "gear/angler type"  
	# EB: depending on the variety of angler types (eg multiple types of boats) this may not be flexible enough for other datasets
## where vehicle==1 & trailer==2, then bank==1 & boat==2
  gear_E = if_else(stan_data$effort_census$angler_type == "Boat", 2, 1),
  # int; index for section
  section_E = stan_data$effort_census$section,
	# EB count_sequence here needs to match that of the closest effort_index count (if my understanding is correct)
  # int; index for count_num
  countnum_E = stan_data$effort_census$count_sequence, 
  # int; observed # of anglers
  E_s = stan_data$effort_census$count_quantity,

	# Proportion tie-in expansion mat; proportion of section covered by tie in counts KB: user defined (see "Proportional_Expansions_for_Tie_In_Sections_Kalama_Example"; need to format)

	p_TI = lut$census_expansion |> 
    select(gear_num, angler_type, section, p_TI) |> 
    pivot_wider(names_from = section, values_from = p_TI) |> 
    select(-gear_num, -angler_type) |> 
    as.matrix(),

	# interview data - CPUE 
	# int; total number of angler interviews with c & h data 
	IntC = nrow(stan_data$interview),
	# int; index denoting day/period   
	day_IntC = stan_data$interview$day_index,
	# int; index denoting "gear/angler type" 
	# EB: depending on the variety of angler types (eg multiple types of boats) this may not be flexible enough for other datasets 
	gear_IntC = if_else(stan_data$interview$angler_type == "Boat", 2, 1),
	# int; index for section
	section_IntC = stan_data$interview$section, 
	# int; total catch
	c = stan_data$interview$fish_count,
	# vec; total hours fished
  # EB Does Total_Hours refer to fishing time (angler hours) multiplied by the total number of anglers in an                    interviewed party (group_angler_hours)? 
	h = stan_data$interview$angler_hours_total,

	# interview data - Total Effort & Catch Creeled
	# int; total interviews by sub-groups	
	IntCreel = nrow(stan_data$interview_daily_totals),
	# int; index denoting day/period     
	day_Creel = stan_data$interview_daily_totals$day_index,
	# int; index denoting "gear/angler type"  
# EB: depending on the variety of angler types (eg multiple types of boats) this may not be flexible enough for other datasets 
	gear_Creel = if_else(stan_data$interview_daily_totals$angler_type == "Boat", 2, 1),
	# int; index for section 
	section_Creel = stan_data$interview_daily_totals$section,
  # int; total catch  
	C_Creel = stan_data$interview_daily_totals$catch_dailysum,	  
  # vec; total hours fished 
	E_Creel = stan_data$interview_daily_totals$angler_hours_total_dailysum,
	  
	# interview data - angler expansion 
	
	# int; total number of angler interviews where V_A, T_A, A_A were collected 
	IntA = nrow(stan_data$interview),
	# int; index denoting day/period
	day_IntA = stan_data$interview$day_index,
	# int; index denoting gear/angler  
  # EB: depending on the variety of angler types (eg multiple types of boats) this may not be flexible enough for other datasets 
	gear_IntA = if_else(stan_data$interview$angler_type == "Boat", 2, 1),
	# int; index denoting day/period
	section_IntA = stan_data$interview$section, 
	# int; total number of vehicles an angler group brought
	V_A = stan_data$interview$vehicle_count,
	# int; total number of trailers an angler group brought
	T_A = stan_data$interview$trailer_count, 
	# int; total number of anglers in the groups interviewed
  A_A = stan_data$interview$angler_count

)

#capture.output(stan_dat, file = "results/stan_dat_2021-12-22.txt")

```


# fit

```{r}
creel_models <- lut$lut_creel_models
model_period <- "day"
wd_models  <-"models"           # Location of model files 
wd_outputs <-"results"          # Location of saved output (summary figures/tables and model results)

# Denote whether you want to run a new model or load "saved" results from a previous model run
  model_source<-c("run_new")  #enter either "run_new" or "load_saved"

# Assign a "Model_Run" number (if model_source == run_new, results will be saved to a new sub-folder; if model_source == load_saved, previous model results will be loaded)
  Model_Run<-4 #Enter numeric number (NOTE: be careful not to over-write previous models runs by entering a number that's already been used)

# Denote which creel model you want to run
  #creel_models[,1:3] #model summary table
  model_number<-c(4)
  
# Specify time period to stratify data by - day vs. week 
  model_period<-c("day") #enter "day" or "week"
  
# Specify parameter values for model priors
  value_cauchyDF_sigma_eps_C = 0.5#1 # the hyperhyper scale (degrees of freedom) parameter in the hyperprior distribution sigma_eps_C; default = 1  
  value_cauchyDF_sigma_eps_E = 0.5#1 # the hyperhyper scale (degrees of freedom) parameter in the hyperprior distribution sigma_eps_E; default = 1  
  value_cauchyDF_sigma_r_E = 0.5#1 # the hyperhyper scale (degrees of freedom) parameter in the hyperprior distribution sigma_r_E; default = 1  
  value_cauchyDF_sigma_r_C = 0.5 #1 # the hyperhyper scale (degrees of freedom) parameter in the hyperprior distribution sigma_r_C; default = 1 
  
  value_normal_sigma_omega_C_0 = 1  #the SD hyperparameter in the prior distribution omega_C_0; normal sd (log-space); default = 1   
  value_normal_sigma_omega_E_0 =  3 # the SD hyperparameter in the prior distribution omega_E_0; normal sd (log-space);; default = 3  
  value_lognormal_sigma_b = 1 # the SD hyperparameter in the prior distribution b; default = 1  
  value_normal_sigma_B1 = 5 # the SD hyperparameter in the prior distribution B1; default = 5  
  value_normal_mu_mu_C = log(0.02) # the mean hyperparameter in the prior distribution mu_C; median (log-space); default = 0.02 (was originally  0.05) 
  value_normal_sigma_mu_C = 1.5 # the SD hyperparameter in the prior distribution mu_C; normal sd (log-space); default = 1.5 (was originally 5)
  value_normal_mu_mu_E = log(5) # the mean hyperparameter in the prior distribution mu_E; median effort (log-space); default = 15 
  value_normal_sigma_mu_E = 2  # the SD hyperparameter in the prior distribution mu_E; normal sd (log-space); default = 2 (was originally 5) 
  value_betashape_phi_E_scaled = 1 # the rate (alpha) and shape (beta) hyperparameters in phi_E_scaled; default = 1 (i.e., beta(1,1) which is uniform), alternative beta(2,2) 
  value_betashape_phi_C_scaled = 1 # the rate (alpha) and shape (beta) hyperparameters in phi_C_scaled; default = 1 (i.e., beta(1,1) which is uniform), alternative beta(2,2)
  value_cauchyDF_sigma_mu_C = 0.5#1      # the hyperhyper SD parameter in the hyperprior distribution sigma_mu_C
  value_cauchyDF_sigma_mu_E = 0.5#1      # the hyperhyper SD parameter in the hyperprior distribution sigma_mu_E

# Specific Stan model arguments
  n_chain<-2        # set the number of Markov chains. The default is 4.
  n_iter<-200        # set the number of iterations for each chain (including warmup). The default is 2000.
  n_cores<-4         # set the number of cores to use when executing the chains in parallel. The defaults is 1. NOTE: Stan manual recommends setting the mc.cores option to be as many processors as the hardware and RAM allow (up to the number of chains).
  n_warmup<-n_iter/2   # set the length of warm-up (aka burn-in) iterations per chain.  The default is n_iter/2.  
  n_thin<-1          # set the thinning rate (aka, the period for saving samples). The default is 1, which is usually the recommended value.
  adapt_delta<-0.8 #0.999  # set adapt delta, which is the target average proposal acceptance probability during Stan's adaptation period. Value between 0 and 1. The default is 0.8. Increasing it will force stan to take smaller steps (and thus run time will be longer). 
  max_treedepth<-10 # set the max tree depth; default is 8; NOTE: this sets the max depth of tree used by NUTS during each iteration; warnings about hitting the maximum treedepth is an efficiency concern
  
```  
  
```{r}
  #---------------------------------------------------------------------------------------- -    
# This file creates sub-folders to save model information, outputs, and summarized results
#---------------------------------------------------------------------------------------- - 
# outputs
  ifelse(!dir.exists(wd_outputs), {dir.create(wd_outputs); "Output sub-folder created"},"Output sub-folder exists already")
    
# catch group  
  filepath_catchgroup<-paste(wd_outputs, params$catch_group, sep="/")
  ifelse(!dir.exists(filepath_catchgroup), {dir.create(filepath_catchgroup); "Catch group sub-folder created"},"Catch group sub-folder exists already")

# model run  
  filepath_Run<-paste(filepath_catchgroup, paste0("Run_", Model_Run), sep="/")
  ifelse(!dir.exists(filepath_Run), {dir.create(filepath_Run); "Model run sub-folder created"},"Model run sub-folder exists already")

# model information and outputs    
  filepath_modeloutputs<-paste(filepath_Run, "model_outputs", sep="/")
  ifelse(!dir.exists(filepath_modeloutputs), {dir.create(filepath_modeloutputs); "Model info sub-folder created"},"Model info sub-folder exists already")

# summarized estimates  
  filepath_modelestimates<-paste(filepath_Run, "summarized_estimates", sep="/")
  ifelse(!dir.exists(filepath_modelestimates), {dir.create(filepath_modelestimates); "Model estimates sub-folder created"},"Model info sub-folder exists already")

  
```  
  
```{r}  
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# This file prepares final dataset for creel (stan) model
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Last minute data fixes
  stan_dat$V_A[stan_dat$V_A>stan_dat$A_A]<-stan_dat$A_A[stan_dat$V_A>stan_dat$A_A] #Number of vehicles for a group can't be greater than number of anglers in the group
  
# Create two new "stan_dat" variables depending on specified time (strata) period (e.g., day vs. week) 
  if(model_period=="day"){
    stan_dat$P_n<-nrow(d_days) # For now: enter "D" for day or length(unique(week)) for week
    stan_dat$period<-c(1:nrow(d_days)) #week          # For now: enter "1:D" for day or "week" for week
  }else{
    if(model_period=="week"){
      stan_dat$P_n<-length(unique(d_days$Week)) 
      stan_dat$period<-d_days$Week
    }
  }
  
# proportion tie-in expansion
  # p_TI<-matrix(rep(1, stan_dat$G * stan_dat$S), nrow=stan_dat$G, ncol=stan_dat$S)
  # stan_dat$p_TI<-p_TI

# Add priors to "stan_dat"
    stan_dat$value_cauchyDF_sigma_eps_C<-value_cauchyDF_sigma_eps_C   
    stan_dat$value_cauchyDF_sigma_eps_E<-value_cauchyDF_sigma_eps_E       
    stan_dat$value_cauchyDF_sigma_r_E<-value_cauchyDF_sigma_r_E     
    stan_dat$value_cauchyDF_sigma_r_C<-value_cauchyDF_sigma_r_C    
    
    stan_dat$value_cauchyDF_sigma_mu_C <- value_cauchyDF_sigma_mu_C
    stan_dat$value_cauchyDF_sigma_mu_E <- value_cauchyDF_sigma_mu_E
    
    stan_dat$value_normal_sigma_omega_C_0<-value_normal_sigma_omega_C_0         
    stan_dat$value_normal_sigma_omega_E_0<-value_normal_sigma_omega_E_0
    stan_dat$value_normal_sigma_B1<-value_normal_sigma_B1      
    stan_dat$value_lognormal_sigma_b<-value_lognormal_sigma_b                    
    stan_dat$value_normal_mu_mu_C<-value_normal_mu_mu_C   
    stan_dat$value_normal_sigma_mu_C<-value_normal_sigma_mu_C   
    stan_dat$value_normal_mu_mu_E<-value_normal_mu_mu_E   
    stan_dat$value_normal_sigma_mu_E<-value_normal_sigma_mu_E   
    stan_dat$value_betashape_phi_E_scaled<-value_betashape_phi_E_scaled 
    stan_dat$value_betashape_phi_C_scaled<-value_betashape_phi_C_scaled 
    
```

```{r}
  
# Based on "model_number" chosen, create object of the "model.name"
    model.name<-as.character(creel_models$Model_Name[creel_models$Model_number == model_number])

# Load saved model results or...
if(model_source== "load_saved"){
  res_Stan <- readRDS(paste(filepath_modeloutputs, "results_res_Stan.rds", sep="/")) #Load RDS (stan model output)
#...Run new model
}else{

#Compile Model
  start.time<-Sys.time(); print(start.time)
  message(paste("Compiling stan model"))
  model.file.name<-as.character(creel_models$Model_file_name[creel_models$Model_number == model_number])
  model<-stan_model(paste(wd_models, model.file.name, sep="/")) 

  
##12/22/21 just doing it
foo <- stan(
  file = paste0("models/", lut$creel_models |> filter(Model_number == params$Model_number) |> pluck("Model_file_name")),
  data=stan_dat,
  chains = n_chain, cores=n_cores, iter=n_iter,
  thin=n_thin, init="0", warmup=n_warmup, include=T,
  control=list(adapt_delta=adapt_delta , max_treedepth=max_treedepth)
)


#Run model in stan using NUTS/HMC
  message(paste("Running stan model - track progress via Viewer Pane"))
  res_Stan<-
    sampling(
      object=model
      ,data=stan_dat
      , chains = n_chain
      , cores=n_cores
      , iter=n_iter
      , thin=n_thin
      , init="0"
      , warmup=n_warmup
      , include=T
      , control=list(adapt_delta=adapt_delta , max_treedepth=max_treedepth)
    )
  end.time<-Sys.time()
  model_duration<-print(paste("Elapsed Time = ",end.time-start.time,sep=""))
  print(elasped_time_by_chain<-get_elapsed_time(res_Stan))
  approx.model.runtime.minutes<-max(elasped_time_by_chain[,2])/60+max(elasped_time_by_chain[,1])/60
  
#Save Model warnings
  print(warnings())
  model_warnings<-warnings() #KB note: this doesn't appear to be working (not sure why)
}
#Extract posterior draws    
  res<-extract(res_Stan) 

```


```{r}
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# This code generates summaries files regarding the inputs and outputs for a new model run 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# # Model warnings #KB note: this doesn't appear to be working (not sure why)
#     if(length(model_warnings)>0){saved_model_warning<-model_warnings}else{saved_model_warning<-c("there were no model warnings")}
#     writeLines(capture.output(saved_model_warning), paste0(filepath_modeloutputs, "/info_Model_warnings_", Sys.Date(), ".txt"))

# R session Info
  writeLines(capture.output(sessionInfo()), paste0(filepath_modeloutputs, "/info_sessionInfo_", Sys.Date(), ".txt"))
    
# Model summary
  mod.sum.elements<-
      c("Model Name"                   , as.character(creel_models$Model_Name[creel_models$Model_number == model_number])
      , "Model File"                   , model.file.name
      , "Model Period"                 , model_period
      , "Date Begin"                   , as.character(Date_Begin)
      , "Date End"                     , as.character(Date_End)
      , "Catch Group"                  , catch.group.of.interest
      , ""                             , c("")
      , "Chains"                       , n_chain
      , "Iterations"                   , n_iter
      , "Warmup"                       , n_warmup
      , "Thin Rate"                    , n_thin
      , "Adapt_Delta"                  , adapt_delta
      , "Max Tree Depth"               , max_treedepth
      , "Cores"                        , n_cores
      , ""                             , c("")
      , "value_cauchyDF_sigma_eps_C"   , value_cauchyDF_sigma_eps_C
      , "value_cauchyDF_sigma_eps_E"   , value_cauchyDF_sigma_eps_E
      , "value_cauchyDF_sigma_r_E"     , value_cauchyDF_sigma_r_E
      , "value_cauchyDF_sigma_r_C"     , value_cauchyDF_sigma_r_C
      , "value_normal_sigma_omega_C_0" , value_normal_sigma_omega_C_0 
      , "value_normal_sigma_omega_E_0" , value_normal_sigma_omega_E_0
      , "value_lognormal_sigma_b"      , value_lognormal_sigma_b
      , "value_normal_sigma_B1"        , value_normal_sigma_B1
      , "value_normal_mu_mu_C"         , round(value_normal_mu_mu_C, 3)
      , "value_normal_sigma_mu_C"      , round(value_normal_sigma_mu_C, 3) 
      , "value_normal_mu_mu_E"         , round(value_normal_mu_mu_E, 3) 
      , "value_normal_sigma_mu_E"      , round(value_normal_sigma_mu_E, 3)
      , "value_betashape_phi_E_scaled" , value_betashape_phi_E_scaled 
      , "value_betashape_phi_C_scaled" , value_betashape_phi_C_scaled 
  )
  mod.sum<-setNames(as.data.frame(matrix(mod.sum.elements, nrow=length(mod.sum.elements)/2, ncol=2, byrow=TRUE)), c("Argument", "Sim_Input"))
  writeLines(capture.output(mod.sum), paste0(filepath_modeloutputs, "/info_Model_setup_", Sys.Date(), ".txt"))

# Model run-time
  run.time<-c("Approx. Run Time", round(approx.model.runtime.minutes,2))
  mod_run.time<-setNames(as.data.frame(matrix(run.time, nrow=length(run.time)/2, ncol=2, byrow=TRUE)), c("Argument", "Time_minutes"))
  writeLines(capture.output(mod_run.time), paste0(filepath_modeloutputs, "/info_Model_Run_Time_", Sys.Date(), ".txt"))

# Save a file of the summary model results
  s.stan<-summary(res_Stan)
  write.csv(s.stan$summary, paste0(filepath_modeloutputs, "/results_res_Stan_summary_", Sys.Date(), ".csv"))
  
# Save "res_stan" object as .rds file   
  saveRDS(res_Stan, file=paste0(filepath_modeloutputs, "/results_res_Stan.rds"))

# Calculate loo-IC
  loo_IC<-loo(res$log_lik) # see: https://rdrr.io/cran/loo/man/loo.html and https://cran.r-project.org/web/packages/loo/loo.pdf 
  writeLines(capture.output(loo_IC), paste0(filepath_modeloutputs, "/results_model_looIC_", Sys.Date(), ".txt"))  

```


```{r summarize_results,message=FALSE, warning=FALSE,results = "hide",echo=FALSE}
### Summarize and Save Results
#Here we will summarize and save results of the in-season analysis of creel data to estimate CPUE, effort, and catch using the chunk of code with this heading in the RMD file.
#=============================================
#convergence diagnostics
  launch_diagnostics<-c("No") #Enter "Yes" to launch ShinyShin diagnostics
  if(launch_diagnostics=="Yes"){launch_shinystan(output$res_stan)} 

# generate plots and tables of creel estimates 
  source(file.path(wd_source_files,"Generate_Summaries_of_Creel_Estimates.R")) 
    
# KB note: update so table/plots of results are shown in PDF document
```


```{r}

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# This file summarizes creel model estimates by creating summary plots and tables
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#--------------------------------------------------------------------------------------------------------------------- -
# SUMMARIZE SEASON TOTAL EFFORT AND CATCH                                                                ----
#--------------------------------------------------------------------------------------------------------------------- - 
  #calculate season total effort
    #Total_season_catch<-c(setNames(mean(res$C_sum), "Mean"), quantile(res$C_sum,c(0.025,0.25,0.5,0.75,0.975)), setNames(sd(res$C_sum)/mean(res$C_sum), "CV"))
    Total_season_catch<-c(setNames(round(mean(c(apply(res$C_sum, c(1), sum))),0), "Mean"), round(quantile(apply(res$C_sum, c(1), sum),c(0.025,0.25,0.5,0.75,0.975)),0), setNames(round(sd(c(apply(res$C_sum, c(1), sum)))/mean(c(apply(res$C_sum, c(1), sum))),3), "CV"))
  
  #calculate season total catch
    Total_season_effort<-c(setNames(round(mean(res$E_sum),0), "Mean"), round(quantile(res$E_sum,c(0.025,0.25,0.5,0.75,0.975)),0), setNames(round(sd(res$E_sum)/mean(res$E_sum),3), "CV"))
    
  #Export .csv of total catch and effort
    catch.effort.totals<-rbind(Total_season_catch, Total_season_effort)
    write.csv(catch.effort.totals, paste(filepath_modelestimates, paste("Summary_Total_Catch_and_Effort", catch.group.of.interest, paste0("Run_", Model_Run), ".csv", sep="_"), sep="/"), row.names = T)

#--------------------------------------------------------------------------------------------------------------------- -
# SUMMARIZE TOTAL EFFORT DATA - by date, section, and gear-type (all Skagit models estimate effort by section and gear)                                                                                 ----
#--------------------------------------------------------------------------------------------------------------------- -   
  #Extract effort data and name dimensions (so that they are meaningful)
      effort.results<-res$E; dim(effort.results)
      dimnames(effort.results)<-list(seq(1:nrow(effort.results)), section.xwalk$Section_Name, paste("day",seq(1:standat$D), sep="_"), gear.xwalk$Gear_Name )
      # effort.results[1:2, , 1:2,]
      # E.mean<-setNames(as.data.frame(matrix(matrix(apply(effort.results,c(2:4),mean),nrow=dim(effort.results)[2]*dim(effort.results)[3],byrow=T),nrow=dim(effort.results)[3]))
      #                 , CJ(dimnames(effort.results)[[2]], dimnames(effort.results)[[4]], sorted = FALSE)[, paste(V1, V2, sep =" - ")])
  
  #create data frame to fill with summarized effort data
      Effort.summary<-setNames(as.data.frame(matrix(NA, nrow=0, ncol=7)), c("Day", "Section", "Gear", "Mean", "Median", "l95", "u95"))
    
  #loop through array and summarize by day, section, and gear type 
      for(period in 1:dim(effort.results)[3]){
          sub.period<-effort.results[1:nrow(effort.results), 1:dim(effort.results)[2], period, 1:dim(effort.results)[4]]
          sub.period[1:5,,]
          
          for(gear in 1:dim(sub.period)[2]){
            gear.xwalk
            sub.gear<-sub.period[1:nrow(effort.results), 1:dim(sub.period)[2], gear]
            sub.gear[1:5, ]
            
            sub.Effort.summary<-setNames(as.data.frame(matrix(NA, nrow=dim(sub.period)[2], ncol=7)), c("Day", "Section", "Gear", "Mean", "Median", "l95", "u95"))
            
            for(section in 1:dim(sub.period)[2]){
              sub.section<-sub.gear[, section]
              section.xwalk
              
              sub.Effort.summary[section, "Day"]<-as.numeric(period)
              sub.Effort.summary[section, "Section"]<-as.character(section.xwalk$Section_Name[section.xwalk$Section_Num == section])
              sub.Effort.summary[section, "Gear"]<-as.character(gear.xwalk$Gear_Name[gear.xwalk$Gear_Num == gear])
              sub.Effort.summary[section, "Mean"]<-mean(sub.section)
              sub.Effort.summary[section, "Median"]<-quantile(sub.section, 0.5)
              sub.Effort.summary[section, "l95"]<-quantile(sub.section, 0.025)
              sub.Effort.summary[section, "u95"]<-quantile(sub.section, 0.975)
            }
           Effort.summary<-rbind(Effort.summary, sub.Effort.summary) 
          }
      }
      all.Dates$Day<-as.numeric(as.character(all.Dates$Day)) #make sure "all.Dates$Day" is a numeric variable 
      
  #Join "Effort.summary" with "all.Dates" date and daytype information   
     Effort.summary<-left_join(Effort.summary, all.Dates[, c("Day", "Date", "DayType")], by="Day") 

  # write "Effort.summary" to a .csv file   
      write.csv(Effort.summary, paste(filepath_modelestimates, paste("Summary_Effort (total hours per Period) by Section and Gear", catch.group.of.interest, paste0("Run_", Model_Run), ".csv", sep="_"), sep="/"), row.names = F)

#--------------------------------------------------------------------------------------------------------------------- -
# SUMMARIZE CATCH AND CPUE DATA                                                                                 ----
#--------------------------------------------------------------------------------------------------------------------- -   
  #=============================================================================================================== =
  # CATCH SUMMARY
  #=============================================================================================================== =
    #Extract catch data and name dimensions (so that they are meaningful)
        catch.results<-res$C; dim(catch.results)
        dimnames(catch.results)<-list(seq(1:nrow(catch.results)), section.xwalk$Section_Name, paste("day",seq(1:standat$D), sep="_"), gear.xwalk$Gear_Name )
        # C.mean<-setNames(as.data.frame(matrix(matrix(apply(catch.results,c(2:4),mean),nrow=dim(catch.results)[2]*dim(catch.results)[3],byrow=T),nrow=dim(catch.results)[3]))
        #             , CJ(dimnames(catch.results)[[2]], dimnames(catch.results)[[4]], sorted = FALSE)[, paste(V1, V2, sep =" - ")])
    
    #create data frame to fill with summarized catch data
        Catch.summary<-setNames(as.data.frame(matrix(NA, nrow=0, ncol=7)), c("Day", "Section", "Gear", "Mean", "Median", "l95", "u95"))
      
    #loop through array and summarize by day, gear type, and section
        for(period in 1:dim(catch.results)[3]){
            sub.period<-catch.results[1:nrow(catch.results), 1:dim(catch.results)[2], period, 1:dim(catch.results)[4]]
            sub.period[1:5,,]
            
            for(gear in 1:dim(sub.period)[2]){
              
              gear.xwalk
              sub.gear<-sub.period[1:nrow(catch.results), 1:dim(sub.period)[2], gear]
              sub.gear[1:5, ]
              
              sub.Catch.summary<-setNames(as.data.frame(matrix(NA, nrow=dim(sub.period)[2], ncol=7)), c("Day", "Section", "Gear", "Mean", "Median", "l95", "u95"))
              
              for(section in 1:dim(sub.period)[2]){
                sub.section<-sub.gear[, section]
                section.xwalk
                
                sub.Catch.summary[section, "Day"]<-as.numeric(period)
                sub.Catch.summary[section, "Section"]<-as.character(section.xwalk$Section_Name[section.xwalk$Section_Num == section])
                sub.Catch.summary[section, "Gear"]<-as.character(gear.xwalk$Gear_Name[gear.xwalk$Gear_Num == gear])
                sub.Catch.summary[section, "Mean"]<-mean(sub.section)
                sub.Catch.summary[section, "Median"]<-quantile(sub.section, 0.5)
                sub.Catch.summary[section, "l95"]<-quantile(sub.section, 0.025)
                sub.Catch.summary[section, "u95"]<-quantile(sub.section, 0.975)
              }
             Catch.summary<-rbind(Catch.summary, sub.Catch.summary) 
            }
        }
        
    # Join "Catch.summary" with "all.Dates" date and daytype information   
       Catch.summary<-left_join(Catch.summary, all.Dates[, c("Day", "Date", "DayType")], by="Day") 
       
    # write "Catch.summary" to a .csv file   
       write.csv(Catch.summary, paste(filepath_modelestimates, paste("Summary_Catch (total fish per Period) by Gear and Section", catch.group.of.interest, paste0("Run_", Model_Run), ".csv", sep="_"), sep="/"), row.names = F)
       
  #=============================================================================================================== =
  # CPUE SUMMARY
  #=============================================================================================================== =
    #Extract catch data and name dimensions (so that they are meaningful)
        cpue.results<-res$lambda_C_S; dim(cpue.results)
        dimnames(cpue.results)<-list(seq(1:nrow(cpue.results)), section.xwalk$Section_Name, paste("day",seq(1:standat$D), sep="_"), gear.xwalk$Gear_Name )
        # C.mean<-setNames(as.data.frame(matrix(matrix(apply(cpue.results,c(2:4),mean),nrow=dim(cpue.results)[2]*dim(cpue.results)[3],byrow=T),nrow=dim(cpue.results)[3]))
        #             , CJ(dimnames(cpue.results)[[2]], dimnames(cpue.results)[[4]], sorted = FALSE)[, paste(V1, V2, sep =" - ")])
    
    #create data frame to fill with summarized catch data
        CPUE.summary<-setNames(as.data.frame(matrix(NA, nrow=0, ncol=7)), c("Day", "Section", "Gear", "Mean", "Median", "l95", "u95"))
      
    #loop through array and summarize by day, gear type, and section
        for(period in 1:dim(cpue.results)[3]){
            sub.period<-cpue.results[1:nrow(cpue.results), 1:dim(cpue.results)[2], period, 1:dim(cpue.results)[4]]
            sub.period[1:5,,]
            
            for(gear in 1:dim(sub.period)[2]){
              
              gear.xwalk
              sub.gear<-sub.period[1:nrow(cpue.results), 1:dim(sub.period)[2], gear]
              sub.gear[1:5, ]
              
              sub.CPUE.summary<-setNames(as.data.frame(matrix(NA, nrow=dim(sub.period)[2], ncol=7)), c("Day", "Section", "Gear", "Mean", "Median", "l95", "u95"))
              
              for(section in 1:dim(sub.period)[2]){
                sub.section<-sub.gear[, section]
                section.xwalk
                
                sub.CPUE.summary[section, "Day"]<-as.numeric(period)
                sub.CPUE.summary[section, "Section"]<-as.character(section.xwalk$Section_Name[section.xwalk$Section_Num == section])
                sub.CPUE.summary[section, "Gear"]<-as.character(gear.xwalk$Gear_Name[gear.xwalk$Gear_Num == gear])
                sub.CPUE.summary[section, "Mean"]<-round(mean(sub.section),3)
                sub.CPUE.summary[section, "Median"]<-round(quantile(sub.section, 0.5),3)
                sub.CPUE.summary[section, "l95"]<-round(quantile(sub.section, 0.025),3)
                sub.CPUE.summary[section, "u95"]<-round(quantile(sub.section, 0.975),3)
              }
             CPUE.summary<-rbind(CPUE.summary, sub.CPUE.summary) 
            }
        }
        
    # Join "CPUE.summary" with "all.Dates" date and daytype information   
       CPUE.summary<-left_join(CPUE.summary, all.Dates[, c("Day", "Date", "DayType")], by="Day") 
       
    # write "CPUE.summary" to a .csv file   
       write.csv(CPUE.summary, paste(filepath_modelestimates, paste("Summary_CPUE (fish per hr per Period) by Gear and Section", catch.group.of.interest, paste0("Run_", Model_Run), ".csv", sep="_"), sep="/"), row.names = F)   


```

# results

```{r}

# GENERATE PLOTS OF EFFORT AND CATCH SUMMARIES                                                                      ----
#--------------------------------------------------------------------------------------------------------------------- -        
      
  #plot timeseries and total effort and catch
    plot.width<-8.5
    plot.height<-11
    
  #Set up plotting arguments
    eff.pch=21
    eff.cex=2
    gear.cols<-c(colorRampPalette(brewer.pal(max(3,as.numeric(length(unique(Effort.summary$Gear)))),"Blues"))(length(unique(Effort.summary$Gear))))
    section.cols<-c(colorRampPalette(brewer.pal(max(3,as.numeric(length(unique(Effort.summary$Section)))),"Greens"))(length(unique(Effort.summary$Section))))

  #Self-made jitter sequence for x-axis
    jitter.right<-seq(0,1,0.25)
    temp.jitter<-c()
    for(value in 1:length(jitter.right)){
      temp.jitter<-c(temp.jitter, c(jitter.right[value], jitter.right[value]*-1))
    }
    jitter.seq<-as.numeric(unique(temp.jitter))

#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++    
#START OF PDF FUNCTION!!!!! 
#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++    
  pdf(paste(filepath_modelestimates, paste("BSS creel model summary plots", catch.group.of.interest, paste0("Run_", Model_Run),".pdf", sep="_"), sep="/"), width=plot.width, height=plot.height)
  mpg_axis<-0

```


```{r}
# Plot #1 - PDF of total seasonal catch and total seasonal effort
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~
      # set par arguments  
        #windows(width=8.5, height=11)
        par(mfcol=c(2,1), family='sans',  xaxs="i", yaxs="i", cex.axis=1, cex=1, mgp=c(2, 0.75, mpg_axis), mar=c(3,3,1,1), oma=c(4,3,1.5,1)) 
    
      #total seasonal catch    
        plot(density(apply(res$C_sum, c(1), sum)), col="blue", xlab="Total Catch (fish)", ylab="Probability Density", main="", bty="n", xaxs="i",yaxs="i")
        #abline(v=mean(c(apply(res$C_sum, c(1), sum))),lwd=2, lty=2, col="red") #add abline for mean
        abline(v=median(c(apply(res$C_sum, c(1), sum))),lwd=2, lty=2,  col="red") #add abline for median
        abline(v=quantile(apply(res$C_sum, c(1), sum), 0.025),lwd=2, lty=2,  col="black"); abline(v=quantile(apply(res$C_sum, c(1), sum), 0.975),lwd=2, lty=2,  col="black")  #add ablines for 95%
        legend("topright", c( "median", "95%CI"), lty=2, col=c("red", "black"), bty="n", lwd=2)
      
      #total seasonal effort  
        plot(density(res$E_sum),col="blue",xlab="Total Effort (hrs)", ylab="Probability Density", main="", bty="n", xaxs="i",yaxs="i")
        #abline(v=mean(res$E_sum),lwd=2, lty=2, col="red") #add abline for mean
        abline(v=quantile(res$E_sum, 0.5),lwd=2, lty=2,  col="red") #add abline for median
        abline(v=quantile(res$E_sum, 0.025),lwd=2, lty=2,  col="black"); abline(v=quantile(res$E_sum, 0.975),lwd=2, lty=2,  col="black")  #add ablines for 95%
        

```


```{r}
# Plot #2 - Plot Total Daily Effort (hrs) by Section and Gear Type
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~
      # windows(height=11, width=8.5)
        
      # set par arguments    
          par(mfcol=c(length(unique(Effort.summary$Section)),1), family='sans', xaxs="i", yaxs="i",cex.axis=1, cex=1, mgp=c(2,0.75, mpg_axis),mar=c(2,2,1,1), oma=c(3,3,1.5,1)) 
      
      # Loop through "Effort.summary" and plot
          for(section in 1:length(unique(Effort.summary$Section))){
              sub.section<-Effort.summary[Effort.summary$Section == unique(Effort.summary$Section)[section],]
              
              plot(NA, bty="n", ylim=c(-10, round_any(max(Effort.summary$u95, na.rm=T),200,f=ceiling)), xlim=c(0, max(Effort.summary$Day, na.rm=T)+1)) #, main=paste("Section - ", unique(Effort.summary$Section)[section], sep=""))
              
              for(day in 1:length(unique(sub.section$Day))){
                  sub.day<-sub.section[sub.section$Day == unique(sub.section$Day)[day],]
                  if(sub.day$DayType[1]==2){
                      abline(v=unique(sub.section$Day)[day], col="gray", lty=2, lwd=2)
              }}
                  for(gear in 1:length(unique(sub.section$Gear))){
                    sub.gear<-sub.section[sub.section$Gear == unique(sub.section$Gear)[gear],]
                    x.axis<-as.numeric(as.character(sub.gear$Day))+jitter.seq[gear]
                    
                    arrows(x0=(as.numeric(as.character(sub.gear$Day))+jitter.seq[gear]), y0=as.numeric(sub.gear$l95)
                         , x1=(as.numeric(as.character(sub.gear$Day))+jitter.seq[gear]), y1=as.numeric(sub.gear$u95)
                         , length=0.05, angle=90, code=3, lwd=2)
                    points(sub.gear$Median ~ x.axis, bg=gear.cols[gear], pch=eff.pch, cex=eff.cex, col="black")
        
                  }
              for(day in 1:length(unique(sub.section$Day))){
                  sub.day<-sub.section[sub.section$Day == unique(sub.section$Day)[day],]
                  if(sub.day$Mean[1] == 0){
                    x.line<-sub.day$Day+jitter.seq[1:length(unique(sub.section$Gear))]
                    lines(x=c(min(x.line), max(x.line)), y=c(0,0), col='gray', lend=2, lwd=30)
              }
              legend("topleft", legend = unique(Effort.summary$Section)[section], bty="n", bg="white", box.col = "white")  
              }
          }
          legend("topright", legend = unique(Effort.summary$Gear), bty="n", cex=1, pch=eff.pch, col="black", pt.bg = gear.cols, pt.cex=eff.cex)
          title(xlab = "Day", ylab = "Total Daily Effort (hrs)", outer=T, line=1)

```


```{r}
 # Plot #3 - Plot Total Daily Catch (fish) by Section and Gear Type
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~
      #windows(height=11, width=8.5)
      par(mfcol=c(length(unique(Catch.summary$Section)),1), family='sans', xaxs="i", yaxs="i",cex.axis=1, cex=1, mgp=c(2,0.75, mpg_axis),mar=c(2,2,1,1), oma=c(3,3,1.5,1)) 
  

      for(section in 1:length(unique(Catch.summary$Section))){
          sub.section<-Catch.summary[Catch.summary$Section == unique(Catch.summary$Section)[section],]
          
          plot(NA, bty="n", ylim=c(-1, round_any(max(Catch.summary$u95, na.rm=T),10,f=ceiling)), xlim=c(0, max(Catch.summary$Day, na.rm=T)+1)) #, main=paste("Section - ", unique(Catch.summary$Section)[section], sep=""))
          
          for(day in 1:length(unique(sub.section$Day))){
              sub.day<-sub.section[sub.section$Day == unique(sub.section$Day)[day],]
              if(sub.day$DayType[1]==2){
                  abline(v=unique(sub.section$Day)[day], col="gray", lty=2, lwd=2)
          }}
              for(gear in 1:length(unique(sub.section$Gear))){
                sub.gear<-sub.section[sub.section$Gear == unique(sub.section$Gear)[gear],]
                x.axis<-as.numeric(as.character(sub.gear$Day))+jitter.seq[gear]
                
                arrows(x0=(as.numeric(as.character(sub.gear$Day))+jitter.seq[gear]), y0=as.numeric(sub.gear$l95)
                     , x1=(as.numeric(as.character(sub.gear$Day))+jitter.seq[gear]), y1=as.numeric(sub.gear$u95)
                     , length=0.05, angle=90, code=3, lwd=2)
                points(sub.gear$Median ~ x.axis, bg=gear.cols[gear], pch=eff.pch, cex=eff.cex, col="black")
    
              }
          for(day in 1:length(unique(sub.section$Day))){
              sub.day<-sub.section[sub.section$Day == unique(sub.section$Day)[day],]
              if(sub.day$Mean[1] == 0){
                x.line<-sub.day$Day+jitter.seq[1:length(unique(sub.section$Gear))]
                lines(x=c(min(x.line), max(x.line)), y=c(0,0), col='gray', lend=2, lwd=30)
          }
          legend("topleft", legend = unique(Catch.summary$Section)[section], bty="n", bg="white", box.col = "white")  
          }
      }
      legend("topright", legend = unique(Catch.summary$Gear), bty="n", cex=1, pch=eff.pch, col="black", pt.bg = gear.cols, pt.cex=eff.cex)
      title(xlab = "Day", ylab = "Total Daily Catch (fish)", outer=T, line=1)

```


```{r}
  # Plot #4 - Plot  Daily CPUE (fish/hr)
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~
      #windows(height=11, width=8.5)
      par(mfcol=c(length(unique(CPUE.summary$Section)),1), family='sans', xaxs="i", yaxs="i",cex.axis=1, cex=1, mgp=c(2,0.75, mpg_axis),mar=c(2,2,1,1), oma=c(3,3,1.5,1)) 
  

      for(section in 1:length(unique(CPUE.summary$Section))){
          sub.section<-CPUE.summary[CPUE.summary$Section == unique(CPUE.summary$Section)[section],]
          
          plot(NA, bty="n", ylim=c(0, round_any(max(CPUE.summary$u95, na.rm=T),0.05,f=ceiling)), xlim=c(0, max(CPUE.summary$Day, na.rm=T)+1)) #, main=paste("Section - ", unique(CPUE.summary$Section)[section], sep=""))
          
          for(day in 1:length(unique(sub.section$Day))){
              sub.day<-sub.section[sub.section$Day == unique(sub.section$Day)[day],]
              if(sub.day$DayType[1]==2){
                  abline(v=unique(sub.section$Day)[day], col="gray", lty=2, lwd=2)
          }}
              for(gear in 1:length(unique(sub.section$Gear))){
                sub.gear<-sub.section[sub.section$Gear == unique(sub.section$Gear)[gear],]
                x.axis<-as.numeric(as.character(sub.gear$Day))+jitter.seq[gear]
                
                arrows(x0=(as.numeric(as.character(sub.gear$Day))+jitter.seq[gear]), y0=as.numeric(sub.gear$l95)
                     , x1=(as.numeric(as.character(sub.gear$Day))+jitter.seq[gear]), y1=as.numeric(sub.gear$u95)
                     , length=0.05, angle=90, code=3, lwd=2)
                points(sub.gear$Median ~ x.axis, bg=gear.cols[gear], pch=eff.pch, cex=eff.cex, col="black")
    
              }
          for(day in 1:length(unique(sub.section$Day))){
              sub.day<-sub.section[sub.section$Day == unique(sub.section$Day)[day],]
              if(sub.day$Mean[1] == 0){
                x.line<-sub.day$Day+jitter.seq[1:length(unique(sub.section$Gear))]
                lines(x=c(min(x.line), max(x.line)), y=c(0,0), col='gray', lend=2, lwd=30)
          }
          legend("topleft", legend = unique(CPUE.summary$Section)[section], bty="n", bg="white", box.col = "white")  
          }
      }
      legend("topright", legend = unique(CPUE.summary$Gear), bty="n", cex=1, pch=eff.pch, col="black", pt.bg = gear.cols, pt.cex=eff.cex)
      title(xlab = "Day", ylab = "Daily CPUE (fish/hr)", outer=T, line=1)

  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~
  # Plot #5 - Total catch and effort by day
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~


dev.off()

```












