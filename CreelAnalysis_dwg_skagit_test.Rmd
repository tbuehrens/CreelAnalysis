---
title: CreelAnalysis from DWG
params:
  proj_name: "District 14"
  water_body: "Skagit River"
  date_start: "2021-09-01"   
  date_end: "2022-11-30"
  year_group: "2020-2021"
  sections: "lut_water_body_location_d14_skagit_fall_salmon_PE.csv"
  species: "Chinook"
  fin_mark: "UM"
  fate: "Released"
  model_name: "BSS_04_corr"
  model_number: 4
output:
  html_document:
    fig_caption: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
    code_folding: hide
---

# setup 

```{r setup, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(message = FALSE)

library(tidyverse)
library(gt)
library(rstan)
rstan_options(auto_write = TRUE)

#base endpoints
dwg_base <- list(
  event = "https://data.wa.gov/resource/ui95-axtn.csv",
  effort = "https://data.wa.gov/resource/h9a6-g38s.csv",
  interview = "https://data.wa.gov/resource/rpax-ahqm.csv",
  catch = "https://data.wa.gov/resource/6y4e-8ftk.csv",
  gear = "https://data.wa.gov/resource/d2ks-afhz.csv"
)

dwg_sent <- list() #will hold full API strings built on above endpoints with params
creel <- list() #will hold resulting data objects

dates_holidays_2015_2030 <- read_lines("input_files/dates_holidays_2015_2030.txt") |> 
  as.Date(format="%Y-%m-%d")

lut <- map(
  list(
    river_loc = "input_files/lut_River.Locations_2019-01-07.csv",
    creel_models = "input_files/lut_Creel_Models_2021-01-20.csv",
    sections = file.path("input_files", params$sections),
    census_expansion = "input_files/lut_Proportional_Expansions_for_Tie_In_Sections_Skagit_Fall_Salmon_2021_PE.csv"
      #"input_files/lut_Proportional_Expansions_for_Tie_In_Sections_Skagit_Steelhead_2021_Example.csv" 
  ),
  ~readr::read_csv(file.path(.x))
)


#tie_in_indicator: 0 is index/creel, 1 is tie-in/census

# Proportional tie in expansion table 
# value of 1 for p_TI means that the entire river section is surveyed during census counts?
# What is "Indirect_TI_Expan" and how it is calculated?

```

```{r build_mostly_static_inputs, eval=FALSE}
#### can/should continue to rethink the best way to get these in
#
## holiday dates
# write_lines(
#   2015:2030 %>%
#     map(~c(
#       timeDate::USNewYearsDay(.x), 
#       timeDate::USMLKingsBirthday(.x),
#       timeDate::USPresidentsDay(.x),
#       timeDate::USMemorialDay(.x), 
#       timeDate::USIndependenceDay(.x), 
#       timeDate::USLaborDay(.x),
#       timeDate::USVeteransDay(.x), 
#       timeDate::USThanksgivingDay(.x), 
#       timeDate::timeDate(as.character(timeDate::.nth.of.nday(.x, 11, 5, 4))), #Black Friday
#       timeDate::USChristmasDay(.x)
#     ) |> as.character()
#     ) |>  unlist(),
#   "input_files/dates_holidays_2015_2030.txt")
#

## 1/28/22 - retreating from single master section LU
## and/but users should build analysis-specific section LUs from database location_code
## (assuming that location_id is not currently available in views)

##starting point for section numbering, tie-in designation, prop expansion
##requires behind-VPN direct connection to DB
# lut_water_body_location <- full_join(
#   tbl(con_creel, dbplyr::in_schema("creel", "water_body_lut")) |> collect() |> select(water_body_id:water_body_desc),
#   tbl(con_creel, dbplyr::in_schema("creel", "location_lut")) |> collect() |> select(location_id:lower_rm),
#   by = c("water_body_id")
#   ) |>
#   collect()
# 
# write_csv(lut_water_body_location, "input_files/lut_water_body_location.csv")
#
# ## has been modified by hand with section numbering per location_code
# ## 1/28/22 reran lines above to regen lut_water_body_location
# lut_water_body_location <- rows_upsert(
#   lut$sections, 
#   lut_water_body_location,
#   by = c("water_body_id", "location_id")
#   ) 
# #and then overwrote...

```

# get raw data

The data used are from the `r params$proj_name` project on the `r params$location` between `r params$date_start` and `r params$date_end`.

Further development may include interactive control parameter specification via the GUI: [https://bookdown.org/yihui/rmarkdown/params-knit.html#the-interactive-user-interface]

There is also the option to step through multiple pre-defined control parameters:
[https://bookdown.org/yihui/rmarkdown-cookbook/parameterized-reports.html]

## creel events

First, get the creel events of interest by building the Socrata API url string and grabbing the data

```{r get_event}
dwg_sent$event <- URLencode(
  paste0(dwg_base$event,
         "?$where=project_name in('", params$proj_name, "')",
         " AND water_body in('", str_replace(params$water_body, ",|\\|", "','"), "')",
         " AND event_date between '", params$date_start,
         "T00:00:00' and '", params$date_end,
         "T00:00:00'&$limit=100000"
  )
)

creel$event <- read_csv(dwg_sent$event) |> 
  dplyr::select(creel_event_id, water_body, event_date, tie_in_indicator)
```

Then, get the associated effort and interview data.

```{r pending_vw_changes_for_water_body}
# #if water_body dropped from event filter...
# #but regardless can/should build creel_event_id condition once and apply twice?
# eff_int_filter <- paste0(
#     "?$where=creel_event_id in('",
#     paste(creel$event$creel_event_id, collapse = "','"), "')",
#     " AND water_body in('", str_replace(params$water_body, ",|\\|", "','"), "')",
#     "&$limit=100000"
#   )

```

## effort counts

```{r get_effort}
dwg_sent$effort <- URLencode(
  paste0(dwg_base$effort,
         "?$where=creel_event_id in('",
         paste(creel$event$creel_event_id, collapse = "','"),
         "')&$limit=100000"
  )
)

creel$effort <- read_csv(dwg_sent$effort)
# EB are there important reasons why a new variable like count_id, used to perform a qa/qc filter for index counts with missing locations in code chunk 'r data clean up', should be mutated here instead of further down?
```

## interviews

```{r get_interview}
dwg_sent$interview <- URLencode(
  paste0(dwg_base$interview,
         "?$where=creel_event_id in('",
         paste(creel$event$creel_event_id, collapse = "','"),
         "')&$limit=100000"
  )
)

creel$interview <- read_csv(dwg_sent$interview) |> 
  rename(location = interview_location)
```

## catch data

And finally, the catch data associated with the interviews.

```{r get_catch}
dwg_sent$catch <- URLencode(
  paste0(dwg_base$catch,
         "?$where=creel_event_id in('",
         paste(creel$event$creel_event_id, collapse = "','"),
         "')&$limit=100000"
  )
)

# catch with redundant post-join columns removed 
# filtering catch to specific catch group parameter
creel$catch <- read_csv(dwg_sent$catch) |> 
  dplyr::select(interview_id, catch_id, species, run, life_stage, fin_mark, fate, fish_count) 

```

## sections

Aggregations of `location` units that depend on `r params$section` lookup table.

**NOTE ANY DATA THAT ARE NOT ASSIGNED TO A SECTION WILL BE EXCLUDED**

```{r add_sections}
creel$effort <- creel$effort |> 
  select(-created_datetime, -modified_datetime) |>
  left_join(
    lut$sections |> select(water_body_desc, location = location_code, section),
    by = c("location")
    ) |> 
  filter(!is.na(section))

creel$interview <- creel$interview |> 
  select(-created_datetime, -modified_datetime,
         -state_residence, -zip_code) |> 
  left_join(
    lut$sections |> select(location = location_code, section),
    by = c("location")
  ) |> 
  filter(!is.na(section))

```

# pre-analysis QAQC

## evaluate matches in table join keys 
```{r, eval=FALSE}
identical(nrow(event), nrow(distinct(event, creel_event_id))) #example, likely unnecessary given database structure

#effort$creel_event_id matches event$creel_event_id
distinct(effort, creel_event_id)
setdiff(event$creel_event_id, effort$creel_event_id)
setdiff(effort$creel_event_id, event$creel_event_id)

#but interview$creel_event_id is nested subset of event$creel_event_id
distinct(interview, creel_event_id)
setdiff(event$creel_event_id, interview$creel_event_id)
setdiff(interview$creel_event_id, event$creel_event_id)

#and catch$creel_event_id is nested subset of interview$creel_event_id
setdiff(interview$creel_event_id, catch$creel_event_id)
setdiff(catch$creel_event_id, interview$creel_event_id)

#so catch$creel_event_id is even smaller subset of event$creel_event_id
distinct(catch, creel_event_id)
setdiff(event$creel_event_id, catch$creel_event_id)
setdiff(catch$creel_event_id, event$creel_event_id)

#similarly catch$interview_id is a nested subset of interview$interview_id
setdiff(interview$interview_id, catch$interview_id)
setdiff(catch$interview_id, interview$interview_id)

```


## evaluate completeness of index counts / interviews 

```{r count and interview evaluation}

# Number of unique sites visits per drive around effort count
data_eval <- list()

data_eval$index_counts

# Visualize raw counts of vehicles and trailers during monitoring period
# are periods with virtually 0 effort worth running through the model?
data_eval$index_counts <-  creel$effort |>
  filter(tie_in_indicator == 0) |> 
  group_by(section, event_date, count_sequence, count_type) |> 
  summarise(count_quantity = sum(count_quantity), .groups = "drop") |> 
  pivot_wider(names_from = count_type, values_from = count_quantity) |>
  select(-6) |> 
  mutate(
    `Trailers Only` = replace_na(`Trailers Only`, 0)
  ) |>
  pivot_longer(names_to = "count_type", values_to = "count_quantity", cols = c(`Trailers Only`, `Vehicle Only`)) |> 
  arrange(event_date, section)
 
data_eval$index_counts |>
ggplot(aes(x = event_date, y = count_quantity)) +
  geom_point(aes(fill=count_type), colour="black",pch=21, size=2) +
  scale_fill_manual(values=c("orange", "blue")) +
  theme_bw() +
  facet_wrap(.~ section)


# Assess data completeness by looking for deviations from the expected count of distinct effort count locations per section per day. Note - days can look complete if "no count" has been entered for a site, so this is just one view into potential data completeness issues 
data_eval$location_check <- creel$effort |>
  filter(tie_in_indicator == 0) |> 
  select(event_date, section, location, count_sequence) |> 
  group_by(event_date, section, count_sequence) |> 
  summarise(
    n_distinct_locations =  n_distinct(location),
    distinct_locations = paste(unique(location), collapse = ","))

data_eval$location_check |>  ggplot(aes(x = event_date, y = n_distinct_locations)) +
  geom_point() +
  theme_bw() +
  facet_wrap(.~ section)

# Look at records where "no_count_reason" was used 
data_eval$no_count_records <- creel$effort |>
  filter(tie_in_indicator == 0, !is.na(no_count_reason)) |> 
  arrange(event_date)

# Evaluate on a daily basis the number of boat and bank interviews obtained 
# data_eval$interview <- creel$interview |>
#   mutate(
#     interview_type = if_else(boat_used == "Yes", "Boat", "Bank")
#   ) |> 
#   group_by(event_date, section, location, interview_type) |> 
#   summarize(
#     n_interviews = n()
#   ) |> 
#   pivot_wider(names_from = interview_type, values_from = n_interviews, values_fill = 0) |> 
#   pivot_longer(names_to = "interview_type", values_to = "n_interviews", cols = starts_with("B")) |> 
#   arrange(event_date, section)
# 
# data_eval$interview |>
# ggplot(aes(x = event_date, y = n_interviews)) +
#   geom_point(aes(fill=interview_type), colour="black",pch=21, size=2) +
#   scale_fill_manual(values=c("orange", "blue")) +
#   theme_bw() +
#   facet_wrap(.~ section)
  
```


## examine raw catch data 
```{r raw catch data}
left_join(
  creel$interview, 
  creel$catch, 
  by = "interview_id") |> 
  filter(species %in% c(params$species)) |> 
  group_by(event_date, section, species, fin_mark, fate) |> 
  summarise(across(fish_count, sum), .groups = "drop") |> 
  ggplot(aes(event_date, fish_count, fill = fate, color = fate)) + 
  geom_col(position = "dodge") + 
  facet_wrap(~section+species, ncol = 2) #, scales = "free_y"

# left_join(
#   creel$interview, 
#   creel$catch, 
#   by = "interview_id") |> 
#   filter(species %in% c(params$species)) |> 
#   group_by(event_date, section, species, fin_mark, fate) |> 
#   summarise(across(fish_count, sum), .groups = "drop") |> 
#   ggplot(aes(event_date, fish_count, fill = fin_mark)) + 
#   geom_col(position = "dodge") + 
#   facet_wrap(~section+species, ncol = 2) #, scales = "free_y"

```
  
## fishery specific data issues 
```{r data clean up}

# chunk for fishery specific data QA/QC issues

# Skagit issues which may or may not occur elsewhere include missing locations from drive around index effort counts and a location that was supposed to be monitored, but wasn't, for over half of the season (Skagit City Access). Would be good see data issues from other creels to think about what's general vs. project specific. 

# effort count location that was left out of counts for large chunk of season
# data_eval$remove_location <- bind_rows(
#   tibble(location = "Skagit City Access (SF Skagit, N. end of Fir Island)"),
#   tibble(location = "Rasar State Park"))

# mistakes in data collection led to instances of incomplete effort counts, this chunk provides the user an option to censor those data by filtering them out of the final dataset 
# count_id is a paste of event_date, section, count_sequence 

creel$effort <- creel$effort |>
   mutate(
    count_id = paste(event_date, section, count_sequence, sep = "_")
  )

# declare individual count_sequences which contain missing counts and should be filtered from final dataset

# data_eval$remove_count <- bind_rows(
#     tibble(count_id = "2021-08-19_2_1"),
#     tibble(count_id = "2021-08-20_1_3"),
#     tibble(count_id = "2021-09-17_3_4"),
#     tibble(count_id = "2021-09-19_3_2"),
#     tibble(count_id = "2021-09-23_4_1"),
#     tibble(count_id = "2021-09-23_3_1"),
#     tibble(count_id = "2021-09-23_1_3"),
#     tibble(count_id = "2021-09-26_2_3"),
#     tibble(count_id = "2021-09-26_3_2"),
#     tibble(count_id = "2021-10-21_4_3"),
#     tibble(count_id = "2021-10-31_4_1")
#     )

# n rows in effort data pre filter 
data_eval$counts_pre_filter <- nrow(creel$effort)

creel$effort <- creel$effort |>
  filter(!location %in% data_eval$remove_location$location, !count_id %in% data_eval$remove_count$count_id) |>
  arrange(event_date, section, count_sequence)

# n rows in effort data post filter 
data_eval$counts_post_filter <- nrow(creel$effort)

# number of counts removed from dataset 
print(data_eval$counts_pre_filter - data_eval$counts_post_filter)

# Placeholder as option to replace NA's with 0's if/when deemed appropriate
# creel$effort <- creel$effort |> 
#   mutate(count_quantity = if_else(!is.na(no_count_reason), 0, count_quantity))

```




# prepare data for model fitting

```{r init_stan_data_prelim}
#declare an intermediary list

stan_data_prelim <- list(
  effort_census = creel$effort |> filter(tie_in_indicator == 1),
  effort_index = creel$effort |> filter(tie_in_indicator == 0),
  interview = creel$interview
)

```



## dates

First, build an "expanded dates lattice" to which any/all observations are attached, ensuring complete cases.

Start with presumption that all dates are open, then join per-section closures by date.

```{r d_days}
#creel$event |> distinct(event_date)
d_days <- tibble(
  event_date = seq(
    as.Date(params$date_start, "%Y-%m-%d"),
    as.Date(params$date_end, "%Y-%m-%d"),
    by = "day")
) |> 
  mutate(
    Day = weekdays(event_date),
    DayType = if_else(Day == "Saturday" | Day == "Sunday" | Day %in% dates_holidays_2015_2030, "Weekend", "Weekday"),
    DayType_num = if_else(str_detect(DayType, "end"),1,0),
    DayL = suncalc::getSunlightTimes(
      date = event_date,
      tz = "America/Los_Angeles",
      #need to add flexibility for other rivers/multiple lines in River.Locations lut
      lat = lut$river_loc$Lat,
      lon = lut$river_loc$Long,
      keep=c("dawn", "dusk")
    ) |> 
      mutate(DayL = as.numeric(dusk - dawn)) |>
      pluck("DayL"),
    Week = as.numeric(format(event_date, "%V"))
  ) |> 
  rowid_to_column(var = "day_index") |> 
  #make open section cols (only those actually used, not all in LU)
  left_join(
    expand_grid(
      event_date = seq(
        as.Date(params$date_start, "%Y-%m-%d"),
        as.Date(params$date_end, "%Y-%m-%d"),
        by = "day")
      ,
      s = paste0("open_section_", sort(unique(stan_data_prelim$effort_index$section)))
    ) |> 
      mutate(closure_code = TRUE) |> 
      pivot_wider(names_from = s, values_from = closure_code)
    ,
    by = "event_date")


# EB: better working example now for subset of Skagit fall salmon with closures 
# NOTE: all declared closures must fall within start_date and end_date params. 
# #now add closures if any...
# #and because may move to params file path to closures csv or similar
d_days <- rows_upsert(d_days,
    bind_rows(
      # tibble(section = "3", closure_begin = "2021-08-14", closure_end = "2021-08-31"),
      tibble(section = "2", closure_begin = "2021-09-01", closure_end = "2021-09-01"), # treaty fishery closure
      tibble(section = "2", closure_begin = "2021-09-07", closure_end = "2021-09-09"), # treaty fishery closure
      tibble(section = "2", closure_begin = "2021-09-14", closure_end = "2021-09-16"), # treaty fishery closure
      tibble(section = "1,2,3", closure_begin = "2021-09-18", closure_end = "2021-09-18"), # river out due to flows
      tibble(section = "2", closure_begin = "2021-10-05", closure_end = "2021-10-06"), # treaty fishery closure
      tibble(section = "2", closure_begin = "2021-10-12", closure_end = "2021-10-13"), # treaty fishery closure
      tibble(section = "2", closure_begin = "2021-10-19", closure_end = "2021-10-20"), # treaty fishery closure
      tibble(section = "1,2,3", closure_begin = "2021-11-13", closure_end = "2021-11-14"), # river out due to flows
      tibble(section = "1,2,3", closure_begin = "2021-11-17", closure_end = "2021-11-18") # river out due to flows
    )
    |>
      rowwise() |>
      mutate(closure_date = paste(seq.Date(as.Date(closure_begin), as.Date(closure_end), by = "day"), collapse = ",")) |>
      separate_rows(closure_date, sep = ",") |>
      select(event_date = closure_date, section) |>
      mutate(
        event_date = as.Date(event_date),
        closure_code = FALSE # TB - The 1e-06 is needed to keep the model from crashing ¦log-normal parameters cant be 0
      ) |>
      separate_rows(section, sep = ",") |>
      pivot_wider(names_from = section, names_prefix = "open_section_", values_from = closure_code) |>
      mutate(across(starts_with("open_section_"), ~replace_na(., TRUE)))
    ,
    by ="event_date"
  )

```


## effort

### effort census

Aggregate census (tie in) effort counts, associating to closest-in-time index count. 

```{r stan_data_prelim_effort_census}
#to the initial effort_census, with all count_sequence == 1,
#add/overwrite the count_sequence val with that from closest temporal match from inline/anonymous paired counts object 

stan_data_prelim$effort_census <- stan_data_prelim$effort_census |> 
  select(event_date, water_body, water_body_desc, location, section, tie_in_indicator, count_type, count_quantity) |> 
  left_join(
    left_join(
      stan_data_prelim$effort_census |> distinct(event_date, section, location, tie_in_indicator, effort_start_time, count_sequence),
      stan_data_prelim$effort_index |> distinct(event_date, section, location, tie_in_indicator, effort_start_time, count_sequence),
      by = c("event_date", "section"),
      suffix = c("_cen", "_ind")
      ) |> 
      group_by(event_date, section, location_cen) |> 
      slice_min(abs(effort_start_time_cen - effort_start_time_ind), n = 1) |> 
      ungroup() |> 
      #count(event_date, section, location_cen, count_sequence_cen, count_sequence_ind)
      distinct(event_date, section, location = location_cen, count_sequence = count_sequence_ind)
    ,
    by = c("event_date", "section", "location")
  ) |> 
  left_join(d_days, by = "event_date") |>
  mutate(
    #angler_type = word(count_type, 1),
    angler_type = case_when(
      word(count_type, 1) %in% c("Bank","Shore") ~ "bank",
      word(count_type, 1) %in% c("Boat") ~ "boat"
    ),
    angler_type_ind = as.integer(factor(angler_type))
  ) |>
  #exclude any count_type strings we didn't whitelist into an angler_type
  #e.g., "Boats" which are not a thing we use because reasons
  filter(!is.na(angler_type)) |> 
  group_by(event_date, day_index, section, count_sequence, angler_type, angler_type_ind) |>
  summarize(count_quantity = sum(count_quantity), .groups = "drop") |> 
  arrange(event_date, section, count_sequence) 

```

### effort index

Aggregate index counts of vehicles, trailers, anglers, and boats.

```{r stan_data_prelim_effort_index}

##DA added filter(!is.na(count_quantity)) to address above comments...
## need to think about whether !is.na(no_count_reason) should be interpolated or inferred or zeroed or...
## for example no_count_reason == "Conditions", what to do with count_quantity == NA
## Skagit winter gamefish example also shows valid realworld situation
## section 1 has 'Vehicle Only' counts but not 'Trailers Only' due to low angler effort
## so excluding section 1 is legit, but needs to be clearly indicated...

stan_data_prelim$effort_index <- stan_data_prelim$effort_index |> 
  filter(
    # !is.na(count_quantity),
    is.na(no_count_reason),
    !is.na(count_type)
    ) |> 
  select(event_date, water_body, water_body_desc, location, section, tie_in_indicator, count_sequence, count_type, count_quantity) |>
  left_join(d_days, by = "event_date") |> 
  group_by(section, event_date, day_index, Week, count_sequence, count_type) |> 
  summarise(count_quantity = sum(count_quantity), .groups = "drop") |> 
  pivot_wider(names_from = count_type, values_from = count_quantity) |> 
  arrange(event_date, section, count_sequence) |>
  mutate(
    `Trailers Only` = replace_na(`Trailers Only`, 0)
  ) # EB placeholder for trailer counts with NA's that should be 0's 
  
```

## interview

Only "index" for interview, by definition. Combines the angler fields with the catch fields (from the subset of interviews in which catches were reported and recorded).

```{r stan_data_prelim_interview}
stan_data_prelim$interview <- stan_data_prelim$interview |> 
  left_join(d_days, by = "event_date") |> #summary()
  mutate(
    across(c(vehicle_count, trailer_count), ~replace_na(., 0)),
    trip_status = replace_na(trip_status, "Unknown"),
    angler_type = case_when(
      is.na(fish_from_boat) ~ "bank",
      fish_from_boat == "Bank" ~ "bank",
      fish_from_boat == "Boat" ~ "boat"
      ),
    angler_type_ind = as.integer(factor(angler_type)),
    fishing_end_time = if_else(is.na(fishing_end_time), interview_time, fishing_end_time),
    angler_hours = round(as.numeric(fishing_end_time - fishing_start_time) / 3600, 5),
    angler_hours_total = angler_count * angler_hours
  ) |> 
  left_join(
    creel$catch |> 
      filter(
        str_detect(species, params$species |> str_squish() |> str_replace("[:space:]", "") |> str_replace(",|\\|", "|")),
        str_detect(fin_mark, params$fin_mark |> str_squish() |> str_replace("[:space:]", "") |> str_replace(",|\\|", "|")),
        str_detect(fate, params$fate |> str_squish() |> str_replace("[:space:]", "") |> str_replace(",|\\|", "|"))
        # ,
        # str_detect(life_stage, params$life_stage |> str_squish() |> str_replace("[:space:]", "") |> str_replace(",|\\|", "|")),
        # str_detect(run, params$run |> str_squish() |> str_replace("[:space:]", "") |> str_replace(",|\\|", "|")),
      ) |> 
    group_by(interview_id) |> 
    summarise(fish_count = sum(fish_count), .groups = "drop")
    ,
    by = "interview_id"
  ) |> 
  select(section, event_date, day_index, Week, interview_id, interview_time, contains("angler"), contains("count")) |> 
  mutate(across(fish_count, ~replace_na(., 0))) |> 
  filter(angler_hours > 0) |>  
  arrange(event_date)

```

### Daily interview summaries

```{r stan_data_prelim_interview_daily_totals}
#total hours creel and total catch sample a per day/angler type/section to compare with effort
stan_data_prelim$interview_daily_totals <- stan_data_prelim$interview |>
  group_by(event_date, day_index, section, angler_type, angler_type_ind) |>
  summarise(
    angler_hours_total_dailysum = sum(angler_hours_total),
    catch_dailysum = sum(fish_count), .groups = "drop"
  ) |> 
#not sure whether this should stay?
  filter(angler_hours_total_dailysum > 0)

```

### NOT RUN Exclude interviews with NA vehicle and trailer counts

This would form the basis for IntA below if there were cause in the data...? For now, leaving undeclared and simply defining IntA as == IntC, pending further revision...

```{r placeholder_IntA}
#stan_data_prelim$interview_with_trailer_vehicle_count <-

# stan_data_prelim$interview_no_zero_counts <- stan_data_prelim$interview |>
# 	#filter(across(c(vehicle_count, trailer_count), ~ !is.na(.)))
#   filter(vehicle_count > 0)# | trailer_count > 0 | angler_count > 0)

```

## declare stan_data

```{r stan_data}
stan_data <- list(
  
  # Day attributes
  # int; number of fishing days; KB: pull from "master_date" DF
  D = nrow(d_days),
  # int; final number of unique gear/angler types 
  G = as.integer(length(unique(stan_data_prelim$interview$angler_type))),
	# int; final number of river sections 
  S = as.integer(length(unique(stan_data_prelim$effort_index$section))),
	# int; max number of angler effort counts within a sample day KB: max(z$effort$count_sequence)
	H = max(stan_data_prelim$effort_index$count_sequence),
  # int; number of days/periods KB: right now, model is set up to running as daily (P_n = D) or weekly (P_n = effectively D/7)
##NEED TO UPDATE? input param and if_else?
	P_n = nrow(d_days),
  #or for weekly? length(unique(d_days$Week))
	# vec; index denoting daytype (day-type is offset in model)
  w = d_days$DayType_num,
	# int; index denoting fishing day/period      KB: calculate as 1:P_n (not sure why this isn't a vector)
	period = 1:nrow(d_days), 
	# vec; daylength (model offset; assumption)
	L = d_days$DayL,
	# mat; index denoting fishery status                          KB: user defined (1=open, 0 = closed; by period/date and section; 0 defined as 1E-6 for model)
	O = d_days |> 
	  select(contains("section_")) |>
    mutate(across(everything(), ~if_else(., 1, 0.00001))) |> 
	  as.matrix(),

  # Vehicle index effort counts 
  # int; total number of individual vehicle index effort counts 
	V_n = nrow(stan_data_prelim$effort_index),
  # int; observed # of vehicles 
	V_I = stan_data_prelim$effort_index$`Vehicle Only`,
	# int; index for day/period 
	day_V = stan_data_prelim$effort_index$day_index,
  gear_V = as.integer(rep(1, nrow(stan_data_prelim$effort_index))),
	# int; index for section    
  section_V = stan_data_prelim$effort_index$section,
	# int; index for count_num  
	countnum_V = stan_data_prelim$effort_index$count_sequence,
	
	# Trailer index effort counts
  # int; total number of boat trailer index effort counts 
	T_n = nrow(stan_data_prelim$effort_index),
	# int; observed # of boat trailers 
	T_I = stan_data_prelim$effort_index$`Trailers Only`,
  # int; index for day/period
## INTENTIONALLY LEFT "WRONG" TO MARK LATER REVISION IN ABOVE WRANGLING
## TO ALLOW RAGGED VEHICLE/TRAILER VECTORS 
	day_T = stan_data_prelim$effort_index$day_index, #day_V,
  gear_T = as.integer(rep(2, nrow(stan_data_prelim$effort_index))), #SAME - TEMP OPTION PENDING OTHER CHANGES
  # int; index for section
	section_T = stan_data_prelim$effort_index$section,
  # int; index for count_num  
	countnum_T = stan_data_prelim$effort_index$count_sequence,

  # Angler index effort counts
	# int; total number of angler index effort counts
	# Need to circle back on how to deal with this when applicable, set all values to 0 as a placeholder, matching example standat_2021-05-28.txt
	A_n = 0,
	# int; observed # of anglers
	A_I = numeric(0), #EB placeholder
	# int; index for day/period
	day_A = numeric(0),
	# int; index denoting "gear/angler type"  
	gear_A = numeric(0), #EB placeholder 
	# int; index for section 
	section_A = numeric(0),
	# int; index for count_num
	countnum_A = numeric(0),

  
  # Census (tie-in) effort counts   
	# int; total number of angler tie-in effort counts
	E_n = nrow(stan_data_prelim$effort_census),
  # int; index denoting day/period
	day_E = stan_data_prelim$effort_census$day_index,
  # int; index denoting "gear/angler type"  
  gear_E = stan_data_prelim$effort_census$angler_type_ind, #if_else(stan_data_prelim$effort_census$angler_type == "Boat", 2, 1),
  # int; index for section
  section_E = stan_data_prelim$effort_census$section,
	# EB count_sequence here needs to match that of the closest effort_index count (if my understanding is correct)
  # int; index for count_num
  countnum_E = stan_data_prelim$effort_census$count_sequence, 
  # int; observed # of anglers
  E_s = stan_data_prelim$effort_census$count_quantity,

	# Proportion tie-in expansion mat; proportion of section covered by tie in counts KB: user defined (see "Proportional_Expansions_for_Tie_In_Sections_Kalama_Example"; need to format)

	p_TI = lut$census_expansion |> 
    select(gear_num, angler_type, section, p_TI) |> 
    pivot_wider(names_from = section, values_from = p_TI) |> 
    select(-gear_num, -angler_type) |> 
    as.matrix(),

	# interview data - CPUE 
	# int; total number of angler interviews with c & h data 
	IntC = nrow(stan_data_prelim$interview),
	# int; index denoting day/period   
	day_IntC = stan_data_prelim$interview$day_index,
	# int; index denoting "gear/angler type" 
	gear_IntC = stan_data_prelim$interview$angler_type_ind, #if_else(stan_data_prelim$interview$angler_type == "Boat", 2, 1),
	# int; index for section
	section_IntC = stan_data_prelim$interview$section, 
	# int; total catch
	c = stan_data_prelim$interview$fish_count,
	# vec; total hours fished
  # EB Does Total_Hours refer to fishing time (angler hours) multiplied by the total number of anglers in an                    interviewed party (group_angler_hours)? 
	h = stan_data_prelim$interview$angler_hours_total,

	# interview data - Total Effort & Catch Creeled
	# int; total interviews by sub-groups	
	IntCreel = nrow(stan_data_prelim$interview_daily_totals),
	# int; index denoting day/period     
	day_Creel = stan_data_prelim$interview_daily_totals$day_index,
	# int; index denoting "gear/angler type"  
	gear_Creel = stan_data_prelim$interview_daily_totals$angler_type_ind, #if_else(stan_data_prelim$interview_daily_totals$angler_type == "Boat", 2, 1),
	# int; index for section 
	section_Creel = stan_data_prelim$interview_daily_totals$section,
  # int; total catch  
	C_Creel = stan_data_prelim$interview_daily_totals$catch_dailysum,	  
  # vec; total hours fished 
	E_Creel = stan_data_prelim$interview_daily_totals$angler_hours_total_dailysum,
	  
	# interview data - angler expansion 
	
	# int; total number of angler interviews where V_A, T_A, A_A were collected 
	IntA = nrow(stan_data_prelim$interview),
	# int; index denoting day/period
	day_IntA = stan_data_prelim$interview$day_index,
	# int; index denoting gear/angler  
	gear_IntA = stan_data_prelim$interview$angler_type_ind, #if_else(stan_data_prelim$interview$angler_type == "Boat", 2, 1),
	# int; index denoting day/period
	section_IntA = stan_data_prelim$interview$section, 
	# int; total number of vehicles an angler group brought
	V_A = stan_data_prelim$interview$vehicle_count,
	# int; total number of trailers an angler group brought
	T_A = stan_data_prelim$interview$trailer_count, 
	# int; total number of anglers in the groups interviewed
  A_A = stan_data_prelim$interview$angler_count,

  #priors
  #hyperhyper scale (degrees of freedom) parameters
  value_cauchyDF_sigma_eps_C = 0.5, #for the hyperprior distribution sigma_eps_C; default = 1  
  value_cauchyDF_sigma_eps_E = 0.5, #for the hyperprior distribution sigma_eps_E; default = 1  
  value_cauchyDF_sigma_r_E = 0.5,   #for the hyperprior distribution sigma_r_E; default = 1  
  value_cauchyDF_sigma_r_C = 0.5,   #for the hyperprior distribution sigma_r_C; default = 1 
  value_cauchyDF_sigma_mu_C = 0.5,  #the hyperhyper SD parameter in the hyperprior distribution sigma_mu_C
  value_cauchyDF_sigma_mu_E = 0.5,   #the hyperhyper SD parameter in the hyperprior distribution sigma_mu_E
  
  value_normal_sigma_omega_C_0 = 1, #the SD hyperparameter in the prior distribution omega_C_0; normal sd (log-space); default = 1   
  value_normal_sigma_omega_E_0 = 3, #the SD hyperparameter in the prior distribution omega_E_0; normal sd (log-space);; default = 3  
  value_lognormal_sigma_b = 1,      #the SD hyperparameter in the prior distribution b; default = 1  
  value_normal_sigma_B1 = 5,        #the SD hyperparameter in the prior distribution B1; default = 5  
  value_normal_mu_mu_C = log(0.02), #the mean hyperparameter in the prior distribution mu_C; median (log-space); default = 0.02 (was originally  0.05) 
  value_normal_sigma_mu_C = 1.5,    #the SD hyperparameter in the prior distribution mu_C; normal sd (log-space); default = 1.5 (was originally 5)
  value_normal_mu_mu_E = log(5),    #the mean hyperparameter in the prior distribution mu_E; median effort (log-space); default = 15 
  value_normal_sigma_mu_E = 2,      #the SD hyperparameter in the prior distribution mu_E; normal sd (log-space); default = 2 (was originally 5) 
  value_betashape_phi_E_scaled = 1, #the rate (alpha) and shape (beta) hyperparameters in phi_E_scaled; default = 1 (i.e., beta(1,1) which is uniform), alternative beta(2,2) 
  value_betashape_phi_C_scaled = 1 #the rate (alpha) and shape (beta) hyperparameters in phi_C_scaled; default = 1 (i.e., beta(1,1) which is uniform), alternative beta(2,2)

)

#capture.output(stan_dat, file = "results/stan_dat_2021-12-22.txt")

```

# fit

```{r quick_fit}
creel_models <- lut$lut_creel_models

# Specific Stan model arguments
n_chain <- 2  
n_cores <- 4    

n_iter <- 200 # DA uses 200 for quick see if it breaks fit; TB and DA suggest that 1,000 should be sufficient, maybe more for final end of season estimate 
n_warmup <- n_iter/2
n_thin <- 1  # set the thinning rate (aka, the period for saving samples). The default is 1, which is usually the recommended value.
adapt_delta <-0.8 #0.999  # set adapt delta, which is the target average proposal acceptance probability during Stan's adaptation period. Value between 0 and 1. The default is 0.8. Increasing it will force stan to take smaller steps (and thus run time will be longer). 
max_treedepth <- 10 # set the max tree depth; default is 8; NOTE: this sets the max depth of tree used by NUTS during each iteration; warnings about hitting the maximum treedepth is an efficiency concern
init = "0"
  
#Last minute data fixes
#Number of vehicles for a group can't be greater than number of anglers in the group
#DA: why is this happening?
stan_data$V_A[stan_data$V_A > stan_data$A_A] <- stan_data$A_A[stan_data$V_A > stan_data$A_A] 
  
# Create two new "stan_dat" variables depending on specified time (strata) period (e.g., day vs. week) 
model_period <- "day"# Specify time period to stratify data by - day vs. week 
if(model_period=="day"){
  stan_data$P_n<-nrow(d_days) # For now: enter "D" for day or length(unique(week)) for week
  stan_data$period<-c(1:nrow(d_days)) #week          # For now: enter "1:D" for day or "week" for week
}else{
  if(model_period=="week"){
    stan_data$P_n<-length(unique(d_days$Week)) 
    stan_data$period<-d_days$Week
  }
}

# # proportion tie-in expansion
# # p_TI<-matrix(rep(1, stan_dat$G * stan_dat$S), nrow=stan_dat$G, ncol=stan_dat$S)
# # stan_dat$p_TI<-p_TI


stan_fit <- stan(
  file = paste0("models/", lut$creel_models |> filter(Model_number == params$model_number) |> pluck("Model_file_name")),
  data = stan_data,
  chains = n_chain,
  cores = n_cores,
  iter = n_iter,
  warmup = n_warmup,
  thin = n_thin, init = init, include = T,
  control = list(
    adapt_delta = adapt_delta,
    max_treedepth = max_treedepth
    )
)
```


# results

## save model object and summary 
```{r}
filepath_modeloutput <- "model_output/"

# Save a file of the summary model results
s_stan <- summary(stan_fit)
  
write.csv(s_stan$summary, paste0(filepath_modeloutput, "/results_res_Stan_summary", Sys.Date(), ".csv"))
  
# Save "res_stan" object as .rds file   
saveRDS(stan_fit, file=paste0(filepath_modeloutput, "/results_res_Stan.rds"))

# extract posterior draws
res <- extract(stan_fit)
  
# Calculate loo-IC
loo_IC<-loo(res$log_lik) # see: https://rdrr.io/cran/loo/man/loo.html and https://cran.r-project.org/web/packages/loo/loo.pdf 
writeLines(capture.output(loo_IC), paste0(filepath_modeloutput, "/results_model_looIC_", Sys.Date(), ".txt"))  

  
```


## option to load existing model 
```{r}
# file path to export model results in project folder in Rstudio remote server instance
# filepath_modeloutput <- "model_output/"
# 
# stan_fit <- read_rds(paste0(filepath_modeloutput, "results_res_Stan.rds"))

```


# diagnostics

Add more...

```{r}
stan_fit |> get_sampler_params(inc_warmup = FALSE) |> map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
```

# results

```{r}
results <- list()
```

## total cach and effort summary 
```{r res_summaries}
results$e_sum_c_sum <- summary(stan_fit, pars = c("E_sum", "C_sum"))$summary |> 
  as.data.frame() |> rownames_to_column("Estimate") |> as_tibble() |> 
  mutate(CV = sd / mean)

results$e_sum_c_sum |> 
  select(Estimate, mean, `2.5%`:`97.5%`,CV) |> 
  gt() |> 
  fmt_number(-Estimate, decimals = 0)

# write effort summary to file 
write_csv(results$e_sum_c_sum, paste(filepath_modeloutput, paste("Summary_Total_Catch_Effort", paste(params$species, params$fin_mark, params$fate, sep = "_"), paste0("Run_", params$model_number), ".csv", sep="_"), sep="/"))

```

## posterior plots of summary estimates

```{r}
#various other diplay options: ggdist::stat_cdfinterval(), ggdist::stat_slab(), ggdist::stat_pointinterval()

tidybayes::spread_draws(stan_fit, C_sum) |> 
  ggplot(aes(C_sum)) +
  ggdist::stat_halfeye()

tidybayes::spread_draws(stan_fit, E_sum) |> 
  ggplot(aes(E_sum)) +
  ggdist::stat_halfeye()

```

## catch by day, section, angler type

```{r res_c_by_strata}
# #could use ggdist::geom/stat_lineribbon() if keeping full fit objects?
# tidybayes::spread_draws(stan_fit, C[s][d,g] | s)

#the E & C array dims are E[section, day, gear]?
#summary(stan_fit, pars = c("E"))$summary |> 
results$C <- summary(stan_fit, pars = c("C"))$summary |> 
  as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
  mutate(
    indices = str_sub(stan_out, 3, 20) |> str_remove("\\]")
  ) |> 
  separate(col = indices, into = c("section", "day_ind", "angler_type_ind")) |> 
  mutate(
    day_ind = as.integer(day_ind),
    angler_type_ind = as.integer(angler_type_ind)
  ) |> 
  #need "section names" in LUT if going to display more than section number
  left_join(
    distinct(lut$sections, section, section_name) |> mutate(section = as.character(section)),
    by = "section"
  ) |> 
  left_join(
    distinct(stan_data_prelim$interview, angler_type_ind, angler_type),
    by = "angler_type_ind"
  ) |> 
  left_join(
    d_days |> select(day_ind = day_index, event_date:DayType, Week) |> mutate(m = lubridate::month(event_date), mon = month.abb[m]),
    by = "day_ind"
  )

# write effort summary to file 
write_csv(results$C, paste(filepath_modeloutput, paste("Summary_Catch", paste(params$species, params$fin_mark, params$fate, sep = "_"), paste0("Run_", params$model_number), ".csv", sep="_"), sep="/"))

results$C |> 
  ggplot(aes(x = event_date, y = `50%`, fill = angler_type, color = angler_type)) +
  geom_ribbon(aes(ymin = `2.5%`, ymax = `97.5%`), alpha = 0.5) +
  geom_line() + geom_point() +
  scale_x_date() +
  facet_wrap(~section_name + angler_type, ncol = 2)
  


#unclear how sensible/meaningful this is...but we can do it!
results$C |> 
  group_by(section_name, angler_type, mon, DayType) |> 
  summarise(across(`2.5%`:`97.5%`, sum), .groups = "drop") |> 
  arrange(desc(`50%`))

#the sort of cross-ref to raw that would be helpful in QAQC, laugh testing
# copying this into pre-analysis QAQC section 
left_join(
  creel$interview, 
  creel$catch, 
  by = "interview_id") |> 
  filter(species %in% c(params$species)) |> 
  group_by(event_date, section, species, fin_mark, fate) |> 
  summarise(across(fish_count, sum), .groups = "drop") |> 
  ggplot(aes(event_date, fish_count, fill = fate, color = fate)) + 
  geom_col(position = "dodge") + 
  facet_wrap(~section+species, ncol = 2) #, scales = "free_y"



```


## effort by day, section, angler type

```{r res_e_by_strata}
# #could use ggdist::geom/stat_lineribbon() if keeping full fit objects?
# tidybayes::spread_draws(stan_fit, C[s][d,g] | s)

#the E & C array dims are E[section, day, gear]?
#summary(stan_fit, pars = c("E"))$summary |> 
results$E <- summary(stan_fit, pars = c("E"))$summary |> 
  as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
  mutate(
    indices = str_sub(stan_out, 3, 20) |> str_remove("\\]")
  ) |> 
  separate(col = indices, into = c("section", "day_ind", "angler_type_ind")) |> 
  mutate(
    day_ind = as.integer(day_ind),
    angler_type_ind = as.integer(angler_type_ind)
  ) |> 
  #need "section names" in LUT if going to display more than section number
  left_join(
    distinct(lut$sections, section, section_name) |> mutate(section = as.character(section)),
    by = "section"
  ) |> 
  left_join(
    distinct(stan_data_prelim$interview, angler_type_ind, angler_type),
    by = "angler_type_ind"
  ) |> 
  left_join(
    d_days |> select(day_ind = day_index, event_date:DayType, Week) |> mutate(m = lubridate::month(event_date), mon = month.abb[m]),
    by = "day_ind"
  )

# write effort summary to file 
write_csv(results$E, paste(filepath_modeloutput, paste("Summary_Effort", paste(params$species, params$fin_mark, params$fate, sep = "_"), paste0("Run_", params$model_number), ".csv", sep="_"), sep="/"))


results$E |> 
  ggplot(aes(x = event_date, y = `50%`, fill = angler_type, color = angler_type)) +
  geom_ribbon(aes(ymin = `2.5%`, ymax = `97.5%`), alpha = 0.5) +
  geom_line() + geom_point() +
  scale_x_date() +
  facet_wrap(~section_name + angler_type, ncol = 2)
```



## CPUE by day, section, angler type

```{r res_cpue_by_strata}
# #could use ggdist::geom/stat_lineribbon() if keeping full fit objects?
# tidybayes::spread_draws(stan_fit, C[s][d,g] | s)

#the E & C array dims are E[section, day, gear]?
#summary(stan_fit, pars = c("E"))$summary |> 
results$CPUE <- summary(stan_fit, pars = c("lambda_C_S"))$summary |> 
  as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
  mutate(
    indices = str_sub(stan_out, 12, 20) |> str_remove("\\]")
  ) |> 
  separate(col = indices, into = c("section", "day_ind", "angler_type_ind")) |> 
  mutate(
    day_ind = as.integer(day_ind),
    angler_type_ind = as.integer(angler_type_ind)
  ) |> 
  #need "section names" in LUT if going to display more than section number
  left_join(
    distinct(lut$sections, section, section_name) |> mutate(section = as.character(section)),
    by = "section"
  ) |> 
  left_join(
    distinct(stan_data_prelim$interview, angler_type_ind, angler_type),
    by = "angler_type_ind"
  ) |> 
  left_join(
    d_days |> select(day_ind = day_index, event_date:DayType, Week) |> mutate(m = lubridate::month(event_date), mon = month.abb[m]),
    by = "day_ind"
  )

# write effort summary to file 
write_csv(results$CPUE, paste(filepath_modeloutput, paste("Summary_CPUE", paste(params$species, params$fin_mark, params$fate, sep = "_"), paste0("Run_", params$model_number), ".csv", sep="_"), sep="/"))

results$CPUE |> 
  ggplot(aes(x = event_date, y = `50%`, fill = angler_type, color = angler_type)) +
  geom_ribbon(aes(ymin = `2.5%`, ymax = `97.5%`), alpha = 0.5) +
  geom_line() + geom_point() +
  scale_x_date() +
  facet_wrap(~section_name + angler_type, ncol = 2)
```


# density plots of total catch and effort from SkagitAnalysis.Rmd
```{r, include=TRUE, fig.align="center", fig.cap=c("Figure 5. Season total catch (fish) and effort (angler hours). The middle dashed line shows the posterior median or 'best' estimate. Outer dashed lines show 95 percent credible intervals."),echo=FALSE,warning = FALSE}
season_results<-data.frame(res$C_sum,res$E_sum)%>%
  mutate(iter=row_number())%>%
  rename(`Season Total Catch`=res.C_sum,`Season Total Effort`=res.E_sum)%>%
  pivot_longer(names_to = "Parameter",values_to="value",cols=c(`Season Total Catch`,`Season Total Effort`))

#elimiminate very very extreme quantiles to pretty up plots
lims<-season_results%>%group_by(Parameter)%>%summarise(value = quantile(value, c(0, 0.99)), q = c(0,0.99))%>%pivot_wider(names_from = q,values_from=value)

season_results_trunc=season_results%>%left_join(lims,by="Parameter")%>%filter(value>as.numeric(`0`) & value < as.numeric(`0.99`))

ggplot(season_results_trunc,aes(x=value,fill=Parameter))+
  scale_fill_manual(values=c("orange", "blue")) +
  facet_wrap(~Parameter,ncol=1,  scales = 'free')+
  theme_bw()+
  geom_density()+
  ylab(NULL)+
  geom_vline(season_results%>%group_by(Parameter)%>%summarise(value = quantile(value, c(0.025, 0.5, 0.975)), q = c(0.025, 0.5, 0.975)),mapping=aes(xintercept=value,group=Parameter),linetype="dashed")+
  theme(legend.title = element_blank())

```


# older 
```{r}
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# This code generates summaries files regarding the inputs and outputs for a new model run 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# # Model warnings #KB note: this doesn't appear to be working (not sure why)
#     if(length(model_warnings)>0){saved_model_warning<-model_warnings}else{saved_model_warning<-c("there were no model warnings")}
#     writeLines(capture.output(saved_model_warning), paste0(filepath_modeloutput, "/info_Model_warnings_", Sys.Date(), ".txt"))

# R session Info
#   writeLines(capture.output(sessionInfo()), paste0(filepath_modeloutput, "/info_sessionInfo_", Sys.Date(), ".txt"))
#     
# # Model summary
  model_summary_elements<-
      c("Model Name"                   , as.character(params$model_name)
      , "Model Number"                 , params$model_number
      , "Model Period"                 , params$model_period
      , "Date Begin"                   , as.character(params$date_start)
      , "Date End"                     , as.character(params$date_end)
      , "Catch Group"                  , paste(params$species, params$fin_mark, params$fate, sep = "_") #
      , ""                             , c("")
      , "Chains"                       , n_chain #
      , "Iterations"                   , n_iter #
      , "Warmup"                       , n_warmup #
      , "Thin Rate"                    , n_thin #
      , "Adapt_Delta"                  , adapt_delta #
      , "Max Tree Depth"               , max_treedepth #
      , "Cores"                        , n_cores #
      , ""                             , c("")
      , "value_cauchyDF_sigma_eps_C"   , stan_data$value_cauchyDF_sigma_eps_C
      , "value_cauchyDF_sigma_eps_E"   , stan_data$value_cauchyDF_sigma_eps_E
      , "value_cauchyDF_sigma_r_E"     , stan_data$value_cauchyDF_sigma_r_E
      , "value_cauchyDF_sigma_r_C"     , stan_data$value_cauchyDF_sigma_r_C
      , "value_normal_sigma_omega_C_0" , stan_data$value_normal_sigma_omega_C_0
      , "value_normal_sigma_omega_E_0" , stan_data$value_normal_sigma_omega_E_0
      , "value_lognormal_sigma_b"      , stan_data$value_lognormal_sigma_b
      , "value_normal_sigma_B1"        , stan_data$value_normal_sigma_B1
      , "value_normal_mu_mu_C"         , round(stan_data$value_normal_mu_mu_C, 3)
      , "value_normal_sigma_mu_C"      , round(stan_data$value_normal_sigma_mu_C, 3)
      , "value_normal_mu_mu_E"         , round(stan_data$value_normal_mu_mu_E, 3)
      , "value_normal_sigma_mu_E"      , round(stan_data$value_normal_sigma_mu_E, 3)
      , "value_betashape_phi_E_scaled" , stan_data$value_betashape_phi_E_scaled
      , "value_betashape_phi_C_scaled" , stan_data$value_betashape_phi_C_scaled
  )
  model_summary <-setNames(as.data.frame(matrix(model_summary_elements, nrow=length(model_summary_elements)/2, ncol=2, byrow=TRUE)), c("Argument", "Sim_Input"))
  # writeLines(capture.output(mod.sum), paste0(filepath_modeloutput, "/info_Model_setup_", Sys.Date(), ".txt"))
write_delim(model_summary, paste0(filepath_modeloutput, "/info_Model_setup_", Sys.Date(), ".txt"))
# Model run-time
  
print(elasped_time_by_chain<-get_elapsed_time(stan_fit))
model_runtime_minutes <- max(elasped_time_by_chain[,2])/60+max(elasped_time_by_chain[,1])/60
  
  run_time<-c("Approx. Run Time", round(model_runtime_minutes,2))
  mod_run_time<-setNames(as.data.frame(matrix(run_time, nrow=length(run_time)/2, ncol=2, byrow=TRUE)), c("Argument", "Time_minutes"))
  writeLines(capture.output(mod_run_time), paste0(filepath_modeloutput, "/info_Model_Run_Time_", Sys.Date(), ".txt"))

```
